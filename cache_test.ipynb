{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d55160d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.4.7: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.642 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e57f4db8dd74ebb92fe0e3f06b6e79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "# from transformers.models.gemma3.modeling_gemma3 import Gemma3TextScaledWordEmbedding\n",
    "from functions import *\n",
    "# model, tokenizer = FastModel.from_pretrained(\n",
    "#     model_name = \"unsloth/gemma-3-4b-pt-unsloth-bnb-4bit\",\n",
    "#     max_seq_length = 8192, # Choose any for long context!\n",
    "#     load_in_4bit = True,\n",
    "#     resize_model_vocab=24,\n",
    "# )\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    # model_name=\"unsloth/gemma-3-12b-pt\",\n",
    "    # model_name=\"unsloth/gemma-3-4b-pt\",\n",
    "    max_seq_length = 8192, # Choose any for long context!\n",
    "    load_in_4bit = True,\n",
    "    resize_model_vocab=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a787dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gemma3\n",
    "# model.config.image_token_index = 16 # new image token\n",
    "# model.language_model.lm_head.weight.requires_grad_(True);\n",
    "# # # model.language_model.lm_head.load_state_dict(torch.load(\"/home/zhenlan/Desktop/Projects/ARC2/Model/gemma24.pth\"))\n",
    "# model.language_model.lm_head.load_state_dict(torch.load(\"../../Model/lm_heads_weights_VLM2.pth\"))\n",
    "# model = PeftModel.from_pretrained(model, \"../../Model/merged_model_VLM2\", is_trainable=False)\n",
    "# model.eval()\n",
    "# model.language_model.model.gradient_checkpointing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afbfc8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e425388",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head.load_state_dict(torch.load(\"../../Model/qwen16.pth\"))\n",
    "model.model.embed_tokens.load_state_dict(torch.load(\"../../Model/qwen16.pth\"))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "007f3632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "output_path = '/home/zhenlan/Desktop/Projects/ARC2/Data/ARC-AGI-2-main/combined_data.json'\n",
    "with open(output_path, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02368fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_idx 1986 end_idx 2856\n"
     ]
    }
   ],
   "source": [
    "inputs_f = next(iter(data_gen(data, IsTrain=False, max_length=8192, autoregressive=True, NeedPosition=False, tokenize_func=tokenize_causal, IsDecode=False)))\n",
    "inputs_t = next(iter(data_gen(data, IsTrain=False, max_length=8192, autoregressive=True, NeedPosition=False, tokenize_func=tokenize_causal, IsDecode=True)))\n",
    "start_idx = len(inputs_t['input_tokens'][0]) - 1\n",
    "end_idx = len(inputs_f['input_tokens'][0]) - 2\n",
    "print(\"start_idx\", start_idx, \"end_idx\", end_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d630705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs_f, y_f = next(iter(data_gen_VLM(data, False, tokenizer, 3, False)))\n",
    "# inputs_t, y_t = next(iter(data_gen_VLM(data, False, tokenizer, 3, True)))\n",
    "# start_idx = len(inputs_t['input_ids'][0]) - 1\n",
    "# end_idx = len(inputs_f['input_ids'][0]) - 2\n",
    "# start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33a45526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logit1 and logit2 give different results, suggesting something is sequence length dependent.\n",
    "# tested mannual casual mask, still see the difference. So not caused by mask.\n",
    "# model.eval();\n",
    "# with torch.no_grad():\n",
    "#     with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "#         outputs_full = model(**inputs_f)\n",
    "#         logits1 = outputs_full.logits[0, :start_idx+1]\n",
    "\n",
    "# # shorten inputs\n",
    "# inputs_f['input_ids'] = inputs_f['input_ids'][:, :start_idx+1]\n",
    "# # inputs_f['attention_mask'] = inputs_f['attention_mask'][:, :, :start_idx+1, :start_idx+1]\n",
    "# inputs_f['attention_mask'] = inputs_f['attention_mask'][:, :start_idx+1]\n",
    "# inputs_f['token_type_ids'] = inputs_f['token_type_ids'][:, :start_idx+1]\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "#         outputs_full = model(**inputs_f)\n",
    "#         logits2 = outputs_full.logits\n",
    "# torch.sum(logits1.argmax(-1) == logits2.argmax(-1))/logits1.shape[0]\n",
    "# torch.mean(torch.abs(logits1 - logits2)) # original mask, no peft\n",
    "# torch.mean(torch.abs(logits1 - logits2)) # casual mask\n",
    "# torch.mean(torch.abs(logits1 - logits2)) # original mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8c735e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model);\n",
    "with torch.no_grad():\n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        outputs_full = model(inputs_f['input_tokens'])\n",
    "        logits1 = outputs_full.logits[0, start_idx: end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2375a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3888]), torch.Size([1, 1, 3888, 3888]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_t['input_ids'][0].shape, inputs_t['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5123de7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import HybridCache\n",
    "# past_key_values = HybridCache(model.config.text_config, 1, 5600)\n",
    "# logits2 = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     # Initial pass with the input prompt\n",
    "#     with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "#         outputs_kv_init = model(**inputs_t, use_cache=True, past_key_values=past_key_values)\n",
    "#         # outputs_kv_init = model(**inputs_t)\n",
    "#         logits2.append(outputs_kv_init.logits[0, -1, :])\n",
    "#         # Store the KV cache from initial pass\n",
    "#         past_key_values = outputs_kv_init.past_key_values\n",
    "\n",
    "#     # Iterate through the continuation tokens\n",
    "#     for i in range(end_idx - start_idx - 1):\n",
    "#         # Prepare input for the next iteration: only the current continuation token\n",
    "#         current_token_id_tensor = inputs_f['input_ids'][:, start_idx + i + 1].unsqueeze(1)\n",
    "\n",
    "#         # Model call with the single next token and past_key_values\n",
    "#         with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "#             outputs_kv_step = model(input_ids=current_token_id_tensor, \n",
    "#                                    past_key_values=past_key_values, \n",
    "#                                    use_cache=True,\n",
    "#                                 #    attention_mask=inputs_f['attention_mask'][:, :start_idx + i + 2], # .unsqueeze(0).unsqueeze(0)\n",
    "#                                 #    token_type_ids=inputs_f['token_type_ids'][:, start_idx + i + 1:start_idx + i + 2],\n",
    "#                                    ) # \n",
    "#             # Store the logits from this step - CORRECTION HERE\n",
    "#             logits2.append(outputs_kv_step.logits[0, -1, :])\n",
    "#             # Update KV cache with new values\n",
    "#             past_key_values = outputs_kv_step.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b903e064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mStaticCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_cache_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlayer_device_map\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m        \n",
      "\u001b[0;32mclass\u001b[0m \u001b[0mStaticCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m    Static Cache class to be used with `torch.compile(model)` and `torch.export()`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Parameters:\u001b[0m\n",
      "\u001b[0;34m        config (`PretrainedConfig`):\u001b[0m\n",
      "\u001b[0;34m            The configuration file defining the shape-related attributes required to initialize the static cache.\u001b[0m\n",
      "\u001b[0;34m        max_batch_size (`int`):\u001b[0m\n",
      "\u001b[0;34m            The maximum batch size with which the model will be used. Note that a new instance must be instantiated if a\u001b[0m\n",
      "\u001b[0;34m            smaller batch size is used. If you are manually setting the batch size, make sure to take into account the\u001b[0m\n",
      "\u001b[0;34m            number of beams if you are running beam search\u001b[0m\n",
      "\u001b[0;34m        max_cache_len (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m            The maximum sequence length with which the model will be used.\u001b[0m\n",
      "\u001b[0;34m        device (`torch.device` or `str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m            The device on which the cache should be initialized. If you're using more than 1 computation device, you\u001b[0m\n",
      "\u001b[0;34m            should pass the `layer_device_map` argument instead.\u001b[0m\n",
      "\u001b[0;34m        dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):\u001b[0m\n",
      "\u001b[0;34m            The default `dtype` to use when initializing the layer.\u001b[0m\n",
      "\u001b[0;34m        layer_device_map (`Optional[Dict[int, Union[str, torch.device, int]]]]`, *optional*):\u001b[0m\n",
      "\u001b[0;34m            Mapping between the layers and its device. This is required when you are manually initializing the cache\u001b[0m\n",
      "\u001b[0;34m            and the model is split between different gpus. You can know which layers mapped to which device by\u001b[0m\n",
      "\u001b[0;34m            checking the associated device_map: `model.hf_device_map`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Example:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        ```python\u001b[0m\n",
      "\u001b[0;34m        >>> from transformers import AutoTokenizer, AutoModelForCausalLM, StaticCache\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        >>> model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\u001b[0m\n",
      "\u001b[0;34m        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        >>> inputs = tokenizer(text=\"My name is Llama\", return_tensors=\"pt\")\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        >>> # Prepare a cache class and pass it to model's forward\u001b[0m\n",
      "\u001b[0;34m        >>> # Leave empty space for 10 new tokens, which can be used when calling forward iteratively 10 times to generate\u001b[0m\n",
      "\u001b[0;34m        >>> max_generated_length = inputs.input_ids.shape[1] + 10\u001b[0m\n",
      "\u001b[0;34m        >>> past_key_values = StaticCache(config=model.config, max_batch_size=1, max_cache_len=max_generated_length, device=model.device, dtype=model.dtype)\u001b[0m\n",
      "\u001b[0;34m        >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\u001b[0m\n",
      "\u001b[0;34m        >>> outputs.past_key_values # access cache filled with key/values from generation\u001b[0m\n",
      "\u001b[0;34m        StaticCache()\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mis_compileable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_cache_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mlayer_device_map\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_cache_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_position_embeddings\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmax_cache_len\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmax_cache_len\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Some model define a custom `head_dim` != config.hidden_size // config.num_attention_heads\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"head_dim\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_key_value_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"num_key_value_heads\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_key_value_heads\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_cache\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_cache\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Note: There will be significant perf decrease if switching to use 5D tensors instead.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcache_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_key_value_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_cache_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mlayer_device_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mlayer_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_device_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mlayer_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mnew_layer_key_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mnew_layer_value_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# Note: `mark_static_address` is used to tag the cache as a fixed data pointer,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# preventing compiled graph breaks when updating the cache.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_static_address\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_layer_key_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_static_address\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_layer_value_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_layer_key_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_layer_value_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mkey_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mvalue_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mlayer_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcache_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\u001b[0m\n",
      "\u001b[0;34m        It is VERY important to index using a tensor, otherwise you introduce a copy to the device.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters:\u001b[0m\n",
      "\u001b[0;34m            key_states (`torch.Tensor`):\u001b[0m\n",
      "\u001b[0;34m                The new key states to cache.\u001b[0m\n",
      "\u001b[0;34m            value_states (`torch.Tensor`):\u001b[0m\n",
      "\u001b[0;34m                The new value states to cache.\u001b[0m\n",
      "\u001b[0;34m            layer_idx (`int`):\u001b[0m\n",
      "\u001b[0;34m                The index of the layer to cache the states for.\u001b[0m\n",
      "\u001b[0;34m            cache_kwargs (`Dict[str, Any]`, `optional`):\u001b[0m\n",
      "\u001b[0;34m                Additional arguments for the cache subclass. The `StaticCache` needs the `cache_position` input\u001b[0m\n",
      "\u001b[0;34m                to know how where to write in the cache.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Return:\u001b[0m\n",
      "\u001b[0;34m            A tuple containing the updated key and value states.\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mcache_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mcache_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcache_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cache_position\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mk_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mv_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mcache_position\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mk_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mv_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# Note: here we use `tensor.index_copy_(dim, index, tensor)` that is equivalent to\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# `tensor[:, :, index] = tensor`, but the first one is compile-friendly and it does explicitly an in-place\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# operation, that avoids copies and uses less memory.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mk_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_copy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_position\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mv_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_copy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_position\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# The operator 'aten::index_copy.out' is not currently implemented for the MPS device.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mk_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_position\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mv_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_position\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mk_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_out\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mget_seq_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Returns the sequence length of the cached states that were seen by the model.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Occupied cache == any slot in the 3rd dim (sequence length) holds a non-zero value. To save on compute, let's\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# limit the check to the first batch member and head dimension.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# TODO: deprecate this function in favor of `cache_position`\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mget_max_cache_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_cache_len\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Resets the cache values while preserving the objects\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mlayer_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# In-place ops prevent breaking the static address\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/lib/python3.12/site-packages/transformers/cache_utils.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     SlidingWindowCache, OffloadedStaticCache"
     ]
    }
   ],
   "source": [
    "from transformers import StaticCache\n",
    "StaticCache??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a7e113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e863900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import StaticCache\n",
    "# model.config.text_config.sliding_window_pattern = 1 # disable sliding window. all layers use static cache now\n",
    "# past_key_values = StaticCache(model.config, 1, 2888, device='cuda')\n",
    "# Assume 'model', 'inputs_t', 'inputs_f', 'start_idx', 'end_idx', 'past_key_values'\n",
    "# are already defined and loaded appropriately.\n",
    "# 'model' is a Hugging Face transformer model supporting 'cache_position'.\n",
    "# 'inputs_t' contains the initial prompt tensors (e.g., {'input_ids': tensor, 'attention_mask': tensor})\n",
    "# 'inputs_f' contains the full sequence including continuation tokens.\n",
    "# 'past_key_values' might be None initially or contain a pre-filled cache.\n",
    "\n",
    "logits2 = []\n",
    "current_seq_len = 0 # Keep track of the sequence length processed so far\n",
    "\n",
    "with torch.no_grad():\n",
    "    # --- Initial pass with the input prompt ---\n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        outputs_kv_init = model(\n",
    "            inputs_t['input_tokens'], # Only the prompt\n",
    "            use_cache=True,\n",
    "            # past_key_values=past_key_values,\n",
    "        )\n",
    "        # Store the logit for the *last* token of the prompt\n",
    "        logits2.append(outputs_kv_init.logits[0, -1, :])\n",
    "        # Store the KV cache from initial pass\n",
    "        past_key_values = outputs_kv_init.past_key_values\n",
    "        # Update the sequence length tracker\n",
    "\n",
    "    # --- Iterate through the continuation tokens ---\n",
    "    num_continuation_tokens = end_idx - start_idx - 1\n",
    "    for i in range(num_continuation_tokens):\n",
    "        # Prepare input for the next iteration: only the current continuation token\n",
    "        # Note: Original code uses inputs_f. Ensure this aligns with your logic.\n",
    "        # Usually, in generation, the input is the *predicted* token from the previous step.\n",
    "        # Assuming inputs_f contains ground truth continuation here for analysis.\n",
    "        current_token_id_tensor = inputs_f['input_tokens'][:, start_idx + i + 1].unsqueeze(1) # Shape: (batch_size, 1)\n",
    "\n",
    "        # The position for this single token is the current total sequence length\n",
    "        # Needs to have shape (batch_size, 1)\n",
    "\n",
    "        # Model call with the single next token and past_key_values\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            outputs_kv_step = model(\n",
    "                input_ids=current_token_id_tensor,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True,\n",
    "            )\n",
    "            # Store the logits for the token we just processed\n",
    "            # Logits shape is (batch_size, 1, vocab_size), so index [0, 0, :] or [0, -1, :]\n",
    "            logits2.append(outputs_kv_step.logits[0, -1, :]) # Index 0 as seq_len is 1\n",
    "            # Update KV cache with new values\n",
    "            past_key_values = outputs_kv_step.past_key_values\n",
    "            # Increment the sequence length tracker\n",
    "            current_seq_len += 1\n",
    "\n",
    "logits2 = torch.stack(logits2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1191f393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0615, 0.0601, 0.1465, 0.0396, 0.0830, 0.1533, 0.0981, 0.0361, 0.1025,\n",
       "        0.0405, 0.0408, 0.2051, 0.0349, 0.0679, 0.0317, 0.1221, 0.2256, 0.0481,\n",
       "        0.0869, 0.0225, 0.1289, 0.0500, 0.0869, 0.0767, 0.0474, 0.0366, 0.0864,\n",
       "        0.0405, 0.0269, 0.1123, 0.0449, 0.2080, 0.0547, 0.0449, 0.0547, 0.0840,\n",
       "        0.0703, 0.0625, 0.4004, 0.0410, 0.0273, 0.0508, 0.0957, 0.0410, 0.0215,\n",
       "        0.1611, 0.0820, 0.0371, 0.0293, 0.0762, 0.0996, 0.0430, 0.0500, 0.0410,\n",
       "        0.0703, 0.0645, 0.0605, 0.0527, 0.0215, 0.0811, 0.0332, 0.0410, 0.0469,\n",
       "        0.0454, 0.0815, 0.0537, 0.1523, 0.0547, 0.1084, 0.0703, 0.1318, 0.0654,\n",
       "        0.0549, 0.2402, 0.0645, 0.1875, 0.0352, 0.0195, 0.1387, 0.0767, 0.0488,\n",
       "        0.0312, 0.0752, 0.0273, 0.1582, 0.0859, 0.0762, 0.1221, 0.0391, 0.2061,\n",
       "        0.1055, 0.0425, 0.0391, 0.0835, 0.0449, 0.0586, 0.0474, 0.0352, 0.0493,\n",
       "        0.0078, 0.1436, 0.0447, 0.1162, 0.0498, 0.0859, 0.0383, 0.0488, 0.0410,\n",
       "        0.0488, 0.0312, 0.0488, 0.0352, 0.0645, 0.0234, 0.0579, 0.0332, 0.0542,\n",
       "        0.0461, 0.0410, 0.0508, 0.0781, 0.0386, 0.0469, 0.1094, 0.0430, 0.0371,\n",
       "        0.0698, 0.0371, 0.1084, 0.0410, 0.0723, 0.0752, 0.0464, 0.0767, 0.0254,\n",
       "        0.0381, 0.0781, 0.0549, 0.2480, 0.0583, 0.0391, 0.0547, 0.1738, 0.0293,\n",
       "        0.0532, 0.0547, 0.0811, 0.0413, 0.0410, 0.1245, 0.0469, 0.0806, 0.0430,\n",
       "        0.1426, 0.1406, 0.1504, 0.0352, 0.0176, 0.2324, 0.0586, 0.2988, 0.1348,\n",
       "        0.1221, 0.0684, 0.0645, 0.0752, 0.0312, 0.0410, 0.0527, 0.0371, 0.0371,\n",
       "        0.0449, 0.2139, 0.0312, 0.0464, 0.0566, 0.0713, 0.1514, 0.0586, 0.0786,\n",
       "        0.0391, 0.1211, 0.0488, 0.0459, 0.0391, 0.0508, 0.0381, 0.0488, 0.0898,\n",
       "        0.0586, 0.1807, 0.0535, 0.0322, 0.0723, 0.0391, 0.0640, 0.0312, 0.0234,\n",
       "        0.0908, 0.0583, 0.2197, 0.0527, 0.0977, 0.0449, 0.2734, 0.2227, 0.1006,\n",
       "        0.1602, 0.0605, 0.0801, 0.0254, 0.1230, 0.0312, 0.0957, 0.0742, 0.0742,\n",
       "        0.0544, 0.0645, 0.0273, 0.0430, 0.3320, 0.0732, 0.0645, 0.0601, 0.0508,\n",
       "        0.0889, 0.0508, 0.0742, 0.0879, 0.0273, 0.0508, 0.0273, 0.1006, 0.0605,\n",
       "        0.1187, 0.0410, 0.0378, 0.0811, 0.0820, 0.0596, 0.0449, 0.7578, 0.0293,\n",
       "        0.0391, 0.0957, 0.1001, 0.0620, 0.0469, 0.1611, 0.0254, 0.1084, 0.0942,\n",
       "        0.0508, 0.1787, 0.0781, 0.0728, 0.0312, 0.0437, 0.0723, 0.0386, 0.0430,\n",
       "        0.0312, 0.2500, 0.0410, 0.0126, 0.0234, 0.0498, 0.2021, 0.0410, 0.0801,\n",
       "        0.0586, 0.0366, 0.0527, 0.1846, 0.0605, 0.0840, 0.0430, 0.0293, 0.0615,\n",
       "        0.0586, 0.1748, 0.0864, 0.1680, 0.0952, 0.0352, 0.0386, 0.0605, 0.0586,\n",
       "        0.0332, 0.0156, 0.0547, 0.0703, 0.1118, 0.0586, 0.1865, 0.0371, 0.0859,\n",
       "        0.2236, 0.0449, 0.1187, 0.0352, 0.2695, 0.0391, 0.0498, 0.1094, 0.0332,\n",
       "        0.0527, 0.0352, 0.0752, 0.0586, 0.1621, 0.0596, 0.0762, 0.0654, 0.0352,\n",
       "        0.0952, 0.0312, 0.0527, 0.0894, 0.0547, 0.0679, 0.1113, 0.0356, 0.0645,\n",
       "        0.1582, 0.0605, 0.1396, 0.0535, 0.0664, 0.0488, 0.0645, 0.2148, 0.0527,\n",
       "        0.0361, 0.2480, 0.1133, 0.1475, 0.0332, 0.1279, 0.0469, 0.0869, 0.0703,\n",
       "        0.0796, 0.0698, 0.0840, 0.0811, 0.0508, 0.0391, 0.0391, 0.0449, 0.0605,\n",
       "        0.0312, 0.0903, 0.0352, 0.0378, 0.0391, 0.1553, 0.0796, 0.0742, 0.0352,\n",
       "        0.0508, 0.1846, 0.0312, 0.2578, 0.0605, 0.0371, 0.0664, 0.0312, 0.1357,\n",
       "        0.0391, 0.0957, 0.1084, 0.1079, 0.1924, 0.0176, 0.0571, 0.0586, 0.0576,\n",
       "        0.1348, 0.0996, 0.0566, 0.0996, 0.0498, 0.0176, 0.0791, 0.0537, 0.0820,\n",
       "        0.1338, 0.0234, 0.1787, 0.0605, 0.1846, 0.0547, 0.1367, 0.0723, 0.0469,\n",
       "        0.0635, 0.0254, 0.0374, 0.0430, 0.0693, 0.0474, 0.0635, 0.0449, 0.0312,\n",
       "        0.0669, 0.0332, 0.0547, 0.0234, 0.0195, 0.0527, 0.0625, 0.1040, 0.0469,\n",
       "        0.0605, 0.0488, 0.1172, 0.0645, 0.0508, 0.0464, 0.1035, 0.8711, 0.0840,\n",
       "        0.0488, 0.2227, 0.1777, 0.1729, 0.0352, 0.0554, 0.0332, 0.0625, 0.1719,\n",
       "        0.0342, 0.2490, 0.0137, 0.2041, 0.0391, 0.0332, 0.0173, 0.1040, 0.0432,\n",
       "        0.0371, 0.0864, 0.0781, 0.0442, 0.1641, 0.0525, 0.1138, 0.0527, 0.1289,\n",
       "        0.0195, 0.0559, 0.0859, 0.2246, 0.0820, 0.0469, 0.0237, 0.0605, 0.1650,\n",
       "        0.0410, 0.0425, 0.0459, 0.0396, 0.1348, 0.0488, 0.1436, 0.0293, 0.0391,\n",
       "        0.0488, 0.1094, 0.0605, 0.0410, 0.1162, 0.0332, 0.1108, 0.0469, 0.1396,\n",
       "        0.1738, 0.0840, 0.0977, 0.0742, 0.6016, 0.0527, 0.0332, 0.0586, 0.0156,\n",
       "        0.0635, 0.0527, 0.2178, 0.0352, 0.1426, 0.0276, 0.0344, 0.1426, 0.0371,\n",
       "        0.0488, 0.0645, 0.0557, 0.1025, 0.0256, 0.0605, 0.0586, 0.0776, 0.0293,\n",
       "        0.1670, 0.0752, 0.0459, 0.0669, 0.0449, 0.0952, 0.0410, 0.0713, 0.0547,\n",
       "        0.0918, 0.0957, 0.0444, 0.1357, 0.0254, 0.0908, 0.0234, 0.1992, 0.0425,\n",
       "        0.1816, 0.0806, 0.0293, 0.1074, 0.0312, 0.0742, 0.0547, 0.0430, 0.0820,\n",
       "        0.0273, 0.0801, 0.0547, 0.1406, 0.0469, 0.1934, 0.0515, 0.0820, 0.1211,\n",
       "        0.0508, 0.2266, 0.0547, 0.0615, 0.0273, 0.0625, 0.0270, 0.0508, 0.2471,\n",
       "        0.0410, 0.1504, 0.3398, 0.0273, 0.0874, 0.0430, 0.2383, 0.0312, 0.0684,\n",
       "        0.0684, 0.0266, 0.1357, 0.0645, 0.0503, 0.0215, 0.0918, 0.1216, 0.1602,\n",
       "        0.1592, 0.0371, 0.2246, 0.0547, 0.0620, 0.0762, 0.0410, 0.0410, 0.0645,\n",
       "        0.0894, 0.0547, 0.1387, 0.0684, 0.1060, 0.0603, 0.0420, 0.0488, 0.0625,\n",
       "        0.0762, 0.0234, 0.0547, 0.0488, 0.0664, 0.0840, 0.0215, 0.1147, 0.0684,\n",
       "        0.1025, 0.0742, 0.1504, 0.0483, 0.0273, 0.0522, 0.0527, 1.3828, 0.1270,\n",
       "        0.1094, 0.0588, 0.0820, 0.0549, 0.0215, 0.0479, 0.0156, 0.0452, 0.0947,\n",
       "        0.1582, 0.0791, 0.0488, 0.0962, 0.0586, 0.1777, 0.4160, 0.1426, 0.0410,\n",
       "        0.0332, 0.0496, 0.0488, 0.0157, 0.0479, 0.0459, 0.0742, 0.0352, 0.1377,\n",
       "        0.0430, 0.1533, 0.0488, 0.0527, 0.0449, 0.0508, 0.1865, 0.0430, 0.2197,\n",
       "        0.0488, 0.1475, 0.1289, 0.0391, 0.1128, 0.0391, 0.0459, 0.0371, 0.0312,\n",
       "        0.0527, 0.0449, 0.0703, 0.0391, 0.1553, 0.0879, 0.0298, 0.0811, 0.1226,\n",
       "        0.0723, 0.0391, 0.0771, 0.0352, 0.0830, 0.0566, 0.2471, 0.0195, 0.0391,\n",
       "        0.1123, 0.0332, 0.0811, 0.0645, 0.1118, 0.2910, 0.0908, 0.0811, 0.0352,\n",
       "        0.0374, 0.0312, 0.1113, 0.0439, 0.0747, 0.1016, 0.0488, 0.0488, 0.0352,\n",
       "        0.0415, 0.0688, 0.0635, 0.2773, 0.0527, 0.0415, 0.0156, 0.1836, 0.0508,\n",
       "        0.1719, 0.2002, 0.0347, 0.0474, 0.0410, 0.0757, 0.0430, 0.2773, 0.2266,\n",
       "        0.2852, 0.1099, 0.0586, 0.1729, 0.0703, 0.0605, 0.0664, 0.1074, 0.1367,\n",
       "        0.0352, 0.0659, 0.0293, 0.0383, 0.1055, 0.0879, 0.0645, 0.0508, 0.0356,\n",
       "        0.0312, 1.8516, 0.1426, 0.3867, 0.0566, 0.0508, 0.2480, 0.0742, 0.0415,\n",
       "        0.0469, 0.2715, 0.0425, 0.0469, 0.0474, 0.0449, 0.2578, 0.0449, 0.0535,\n",
       "        0.1050, 0.0352, 0.0293, 0.0605, 0.1250, 0.0996, 0.1021, 0.0312, 0.0684,\n",
       "        0.0933, 0.0879, 0.1523, 0.0566, 0.2217, 0.0488, 0.0713, 0.0508, 0.0312,\n",
       "        0.0471, 0.0371, 0.1309, 0.0234, 0.1055, 0.0417, 0.0566, 0.0608, 0.0352,\n",
       "        0.0206, 0.0625, 0.0332, 0.0605, 0.0352, 0.0449, 0.0273, 0.1226, 0.0566,\n",
       "        0.1543, 0.0879, 0.2207, 0.1299, 0.0371, 0.0845, 0.0840, 0.5273, 0.0527,\n",
       "        0.0215, 0.0947, 0.0801, 0.1602, 0.0566, 0.1650, 0.0391, 0.0286, 0.0369,\n",
       "        0.0991, 0.1895, 0.0605, 0.1592, 0.0352, 0.1504, 0.0393, 0.0618, 0.0510,\n",
       "        0.0273, 0.0283, 0.0273, 0.2334, 0.0957, 0.1787, 0.1416, 0.0664, 0.1025,\n",
       "        0.0586, 0.0889, 0.0508, 0.0508, 0.0430, 0.0703, 0.0293, 0.0195, 0.0493,\n",
       "        0.0449, 0.0391, 0.0898, 0.0352, 0.0469, 0.0508, 0.1172, 0.0840, 0.0605,\n",
       "        0.0332, 0.0391, 0.0840, 0.0254, 0.1226, 0.0430, 0.0742, 0.0703, 0.1055,\n",
       "        0.1172, 0.1426, 0.1406, 0.1250, 0.0449, 0.0381, 0.0393, 0.0991, 0.0476,\n",
       "        0.0342, 0.0811, 0.0869, 0.1084, 0.0771, 0.0654, 0.0498, 0.0742, 0.1001,\n",
       "        0.0723, 0.0718, 0.1299, 0.1670, 0.0728, 0.0776, 0.1250, 0.1270, 0.0339,\n",
       "        0.1182, 0.1973, 0.1279, 0.0537, 0.1406, 0.0771], device='cuda:0',\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.abs(logits1 - logits2),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe67f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemma3\n",
    "# import torch\n",
    "# from transformers import HybridCache\n",
    "# # model.config.text_config.sliding_window_pattern = 1 # disable sliding window. all layers use static cache now\n",
    "# past_key_values = HybridCache(model.config.text_config, 1, 5600)\n",
    "# # Assume 'model', 'inputs_t', 'inputs_f', 'start_idx', 'end_idx', 'past_key_values'\n",
    "# # are already defined and loaded appropriately.\n",
    "# # 'model' is a Hugging Face transformer model supporting 'cache_position'.\n",
    "# # 'inputs_t' contains the initial prompt tensors (e.g., {'input_ids': tensor, 'attention_mask': tensor})\n",
    "# # 'inputs_f' contains the full sequence including continuation tokens.\n",
    "# # 'past_key_values' might be None initially or contain a pre-filled cache.\n",
    "\n",
    "# logits2 = []\n",
    "# current_seq_len = 0 # Keep track of the sequence length processed so far\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     # --- Initial pass with the input prompt ---\n",
    "#     with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "#         outputs_kv_init = model(\n",
    "#             **inputs_t,\n",
    "#             use_cache=True,\n",
    "#             past_key_values=past_key_values,\n",
    "#         )\n",
    "#         # Store the logit for the *last* token of the prompt\n",
    "#         logits2.append(outputs_kv_init.logits[0, -1, :])\n",
    "#         # Store the KV cache from initial pass\n",
    "#         past_key_values = outputs_kv_init.past_key_values\n",
    "#         # Update the sequence length tracker\n",
    "#         current_seq_len = len(inputs_t['input_ids'][0]) # Assuming batch size of 1\n",
    "\n",
    "#     # --- Iterate through the continuation tokens ---\n",
    "#     num_continuation_tokens = end_idx - start_idx - 1\n",
    "#     for i in range(num_continuation_tokens):\n",
    "#         # Prepare input for the next iteration: only the current continuation token\n",
    "#         # Note: Original code uses inputs_f. Ensure this aligns with your logic.\n",
    "#         # Usually, in generation, the input is the *predicted* token from the previous step.\n",
    "#         # Assuming inputs_f contains ground truth continuation here for analysis.\n",
    "#         current_token_id_tensor = inputs_f['input_ids'][:, start_idx + i + 1].unsqueeze(1) # Shape: (batch_size, 1)\n",
    "\n",
    "#         # The position for this single token is the current total sequence length\n",
    "#         # Needs to have shape (batch_size, 1)\n",
    "#         current_cache_position = torch.tensor([current_seq_len], device=current_token_id_tensor.device)\n",
    "\n",
    "#         # Model call with the single next token and past_key_values\n",
    "#         with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "#             outputs_kv_step = model(\n",
    "#                 input_ids=current_token_id_tensor,\n",
    "#                 past_key_values=past_key_values,\n",
    "#                 use_cache=True,\n",
    "#                 cache_position=current_cache_position # ADDED\n",
    "#             )\n",
    "#             # Store the logits for the token we just processed\n",
    "#             # Logits shape is (batch_size, 1, vocab_size), so index [0, 0, :] or [0, -1, :]\n",
    "#             logits2.append(outputs_kv_step.logits[0, -1, :]) # Index 0 as seq_len is 1\n",
    "#             # Update KV cache with new values\n",
    "#             past_key_values = outputs_kv_step.past_key_values\n",
    "#             # Increment the sequence length tracker\n",
    "#             current_seq_len += 1\n",
    "\n",
    "# logits2 = torch.stack(logits2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd94d21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1f5f0d0",
   "metadata": {},
   "source": [
    "Text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52cbb74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma3ForConditionalGeneration(\n",
       "      (vision_tower): SiglipVisionModel(\n",
       "        (vision_model): SiglipVisionTransformer(\n",
       "          (embeddings): SiglipVisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "            (position_embedding): Embedding(4096, 1152)\n",
       "          )\n",
       "          (encoder): SiglipEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-26): 27 x SiglipEncoderLayer(\n",
       "                (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "                (self_attn): SiglipAttention(\n",
       "                  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): SiglipMLP(\n",
       "                  (activation_fn): PytorchGELUTanh()\n",
       "                  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (multi_modal_projector): Gemma3MultiModalProjector(\n",
       "        (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "      )\n",
       "      (language_model): Gemma3ForCausalLM(\n",
       "        (model): Gemma3TextModel(\n",
       "          (embed_tokens): Gemma3TextScaledWordEmbedding(24, 2560, padding_idx=0)\n",
       "          (layers): ModuleList(\n",
       "            (0): Gemma3DecoderLayer(\n",
       "              (self_attn): Gemma3Attention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2048, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              )\n",
       "              (mlp): Gemma3MLP(\n",
       "                (gate_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=10240, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            )\n",
       "            (1): Gemma3DecoderLayer(\n",
       "              (self_attn): Gemma3Attention(\n",
       "                (q_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              )\n",
       "              (mlp): Gemma3MLP(\n",
       "                (gate_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=10240, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            )\n",
       "            (2): Gemma3DecoderLayer(\n",
       "              (self_attn): Gemma3Attention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2048, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              )\n",
       "              (mlp): Gemma3MLP(\n",
       "                (gate_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=10240, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            )\n",
       "            (3): Gemma3DecoderLayer(\n",
       "              (self_attn): Gemma3Attention(\n",
       "                (q_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              )\n",
       "              (mlp): Gemma3MLP(\n",
       "                (gate_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=10240, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            )\n",
       "            (4): Gemma3DecoderLayer(\n",
       "              (self_attn): Gemma3Attention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2048, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              )\n",
       "              (mlp): Gemma3MLP(\n",
       "                (gate_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=10240, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            )\n",
       "            (5): Gemma3DecoderLayer(\n",
       "              (self_attn): Gemma3Attention(\n",
       "                (q_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              )\n",
       "              (mlp): Gemma3MLP(\n",
       "                (gate_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=10240, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            )\n",
       "            (6-33): 28 x Gemma3DecoderLayer(\n",
       "              (self_attn): Gemma3Attention(\n",
       "                (q_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              )\n",
       "              (mlp): Gemma3MLP(\n",
       "                (gate_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=10240, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            )\n",
       "          )\n",
       "          (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (rotary_emb): Gemma3RotaryEmbedding()\n",
       "          (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "        )\n",
       "        (lm_head): Linear(in_features=2560, out_features=24, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e05381d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.base_model.language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51b50b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "with torch.no_grad():\n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        outputs_full = model(inputs_f['input_ids'])\n",
    "        logits1 = outputs_full.logits[0, start_idx: end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf28be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits2 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Initial pass with the input prompt\n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        outputs_kv_init = model(inputs_t['input_ids'], use_cache=True)\n",
    "        logits2.append(outputs_kv_init.logits[0, -1, :])\n",
    "        # Store the KV cache from initial pass\n",
    "        past_key_values = outputs_kv_init.past_key_values\n",
    "\n",
    "    # Iterate through the continuation tokens\n",
    "    for i in range(end_idx - start_idx - 1):\n",
    "        # Prepare input for the next iteration: only the current continuation token\n",
    "        current_token_id_tensor = inputs_f['input_ids'][:, start_idx + i + 1].unsqueeze(1)\n",
    "\n",
    "        # Model call with the single next token and past_key_values\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            outputs_kv_step = model(input_ids=current_token_id_tensor, \n",
    "                                   past_key_values=past_key_values, \n",
    "                                   use_cache=True)\n",
    "            # Store the logits from this step - CORRECTION HERE\n",
    "            logits2.append(outputs_kv_step.logits[0, -1, :])\n",
    "            # Update KV cache with new values\n",
    "            past_key_values = outputs_kv_step.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d73ed24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits2 = torch.stack(logits2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "412df5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([869, 24]), torch.Size([869, 24]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits2.shape, logits1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75b79b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0625, 2.7812, 3.1406, 3.1094, 3.0781, 3.0625, 3.1250, 3.1719, 3.2812,\n",
       "        3.0625, 2.7969, 2.6406, 2.5625, 2.5312, 2.2812, 2.1250, 2.0625, 2.1875,\n",
       "        2.3906, 2.4062, 2.4062, 2.4062, 2.2969, 2.1406, 2.1094, 2.0625, 1.8359,\n",
       "        1.6562, 1.6641, 1.6641, 1.9219, 3.0625, 3.6094, 4.5312, 4.8438, 4.9375,\n",
       "        4.7188, 4.3438, 2.4688, 4.1562, 3.9062, 3.6406, 2.7656, 2.5312, 2.3125,\n",
       "        2.2344, 1.9766, 1.8984, 1.8594, 1.9219, 2.0938, 1.8828, 1.7422, 1.8906,\n",
       "        1.9766, 1.9531, 1.9688, 1.8828, 1.5859, 1.2109, 1.4844, 1.9062, 2.3906,\n",
       "        2.2812, 2.2656, 2.2656, 2.2812, 2.1562, 2.1094, 2.4531, 2.0000, 1.8438,\n",
       "        1.8594, 1.9922, 1.8984, 1.9531, 1.9531, 1.8047, 1.6797, 1.6562, 1.6016,\n",
       "        1.5547, 1.7578, 1.6641, 1.7500, 1.6094, 1.5156, 1.5625, 1.4844, 1.5078,\n",
       "        1.7031, 2.1250, 2.2656, 2.5625, 2.2188, 1.7969, 1.8125, 1.8750, 2.2031,\n",
       "        2.4062, 2.3125, 2.2031, 2.5781, 2.3125, 2.1094, 2.3438, 2.1094, 2.0781,\n",
       "        1.8438, 1.7109, 1.9453, 1.6953, 1.7891, 1.5859, 1.7656, 1.6406, 1.7031,\n",
       "        1.5391, 1.6484, 1.6406, 1.6094, 2.1250, 2.0000, 2.1250, 2.0781, 2.1094,\n",
       "        2.1562, 2.1562, 2.2812, 2.5469, 2.3125, 2.5000, 2.0000, 2.1250, 2.0625,\n",
       "        2.3438, 2.0156, 2.1875, 2.0469, 2.2812, 1.8750, 1.8906, 1.7188, 1.7734,\n",
       "        1.6172, 1.6719, 1.5391, 1.5078, 1.7578, 1.7812, 1.7656, 2.1719, 1.8438,\n",
       "        2.0625, 2.0625, 2.0469, 1.9688, 1.8516, 2.0938, 1.9766, 1.9219, 1.9062,\n",
       "        2.0156, 2.0000, 1.8906, 1.9531, 2.0000, 2.0781, 1.9141, 1.9219, 1.5781,\n",
       "        1.6016, 1.7578, 1.8984, 1.5391, 1.5859, 1.8984, 1.7734, 1.5000, 1.6172,\n",
       "        1.6641, 1.9531, 2.0781, 2.4688, 2.7031, 2.2344, 2.4062, 2.5156, 2.5781,\n",
       "        2.4844, 2.4219, 2.3750, 1.9766, 2.3594, 2.3125, 2.6250, 2.0938, 2.3281,\n",
       "        2.4062, 2.2500, 2.3438, 1.6875, 1.8672, 1.7500, 1.7734, 1.5938, 1.7812,\n",
       "        1.7266, 1.5703, 1.8281, 1.6406, 2.1719, 2.1719, 2.6250, 2.2812, 2.2344,\n",
       "        2.4531, 2.5312, 2.4375, 2.3594, 2.3906, 2.3594, 2.4531, 2.3750, 2.2500,\n",
       "        2.5625, 2.0000, 2.0312, 2.2969, 2.2656, 1.9219, 1.7422, 2.0312, 1.9141,\n",
       "        1.6797, 1.9844, 2.2969, 1.8359, 1.4922, 1.6328, 1.7500, 2.1562, 2.1406,\n",
       "        2.4062, 2.2969, 2.2969, 2.3438, 2.4219, 2.5781, 2.2188, 2.0938, 2.3750,\n",
       "        2.1875, 2.4375, 2.2188, 2.3594, 1.9766, 2.0781, 2.2031, 2.3438, 2.0781,\n",
       "        1.8438, 2.0312, 1.8750, 1.6250, 2.1094, 2.3125, 2.1406, 1.7188, 1.8984,\n",
       "        1.7266, 2.1562, 2.1250, 2.5781, 2.6719, 2.3281, 2.4531, 2.7031, 2.6250,\n",
       "        2.5156, 2.5938, 2.5312, 3.1094, 3.0469, 2.6875, 2.8594, 2.1875, 2.5000,\n",
       "        2.6094, 2.5938, 2.1875, 2.1094, 2.2031, 2.1719, 1.7734, 2.1562, 2.6719,\n",
       "        2.4375, 1.9219, 1.6797, 1.8438, 2.1406, 2.2188, 2.7344, 2.3281, 2.4688,\n",
       "        2.5000, 2.8594, 2.6562, 2.4844, 2.5312, 2.9688, 2.2656, 2.7812, 2.5625,\n",
       "        2.6875, 2.1406, 2.1562, 2.7500, 2.5312, 2.5000, 1.9375, 2.1094, 1.9609,\n",
       "        2.0469, 1.9297, 2.0625, 2.0156, 1.4922, 2.1875, 1.6094, 2.0469, 2.0938,\n",
       "        2.3594, 2.1562, 2.1250, 2.2188, 2.4219, 2.4844, 2.2969, 2.6250, 2.6406,\n",
       "        2.4688, 2.2188, 2.5781, 2.4844, 2.1406, 2.2344, 2.5781, 2.6719, 2.2031,\n",
       "        2.2031, 2.1406, 2.2188, 1.8984, 2.5156, 2.8594, 2.4844, 1.8047, 1.7344,\n",
       "        2.0781, 2.2812, 2.4375, 2.4844, 2.5625, 2.4531, 2.5938, 2.7812, 2.6875,\n",
       "        2.4062, 2.6094, 2.8438, 2.1875, 2.7344, 2.6094, 2.6875, 2.0469, 2.4688,\n",
       "        2.4219, 2.7344, 2.3438, 2.0625, 2.1406, 2.0781, 1.8125, 2.5781, 2.5312,\n",
       "        2.4531, 2.0312, 2.1719, 1.6875, 2.0781, 2.2656, 2.4844, 2.2500, 2.4062,\n",
       "        2.5156, 2.7344, 2.6719, 2.3281, 2.5781, 2.4062, 2.7969, 2.8906, 2.5000,\n",
       "        2.7344, 2.1250, 2.3125, 2.6562, 2.6562, 2.2031, 2.0156, 2.1094, 2.1719,\n",
       "        1.7344, 2.2188, 2.6406, 2.4375, 1.9375, 1.8516, 1.7266, 1.8828, 2.1250,\n",
       "        2.2344, 2.0469, 1.9766, 2.1875, 2.4219, 2.4219, 2.1719, 2.1875, 2.5625,\n",
       "        2.1875, 2.4062, 2.1562, 2.3438, 1.9141, 1.8516, 2.2812, 2.2812, 2.1250,\n",
       "        1.7656, 1.9375, 1.7812, 1.7969, 1.7578, 1.8984, 1.8281, 1.4609, 2.5156,\n",
       "        1.5391, 1.8438, 2.0781, 2.2188, 2.1875, 2.0781, 2.2656, 2.4375, 2.3594,\n",
       "        2.1562, 2.3438, 2.1562, 2.8281, 2.9844, 2.4219, 2.5000, 1.9453, 2.0312,\n",
       "        2.3906, 2.5156, 2.2344, 1.9609, 2.1406, 2.0938, 1.9062, 2.4062, 2.6719,\n",
       "        2.4062, 1.7656, 1.9375, 1.6719, 1.8672, 2.0625, 2.2969, 2.0469, 2.2188,\n",
       "        2.4219, 2.5938, 2.4531, 2.1406, 2.3281, 2.7344, 2.1250, 2.8594, 2.4531,\n",
       "        2.5156, 1.9531, 2.3438, 2.3438, 2.5938, 2.1250, 1.9375, 2.1562, 1.9219,\n",
       "        1.8203, 2.4062, 2.3750, 2.3594, 1.9219, 2.3906, 1.5391, 1.8359, 1.9609,\n",
       "        2.2344, 1.9688, 1.9062, 1.9922, 2.2656, 2.3750, 2.1719, 2.4531, 2.4688,\n",
       "        2.2812, 2.2188, 2.5000, 2.5000, 2.0469, 2.4844, 2.5312, 2.4531, 1.9453,\n",
       "        1.9531, 2.2188, 2.1875, 1.7812, 2.3125, 2.7188, 2.3906, 1.6484, 2.1719,\n",
       "        1.5625, 1.9453, 2.0625, 2.2344, 2.4062, 2.1719, 2.3906, 2.4375, 2.4688,\n",
       "        2.0938, 2.3594, 2.7656, 2.0781, 2.6875, 2.4219, 2.5000, 1.8672, 1.8906,\n",
       "        2.4375, 2.3594, 2.4531, 1.8672, 2.0938, 1.8906, 1.8906, 1.8828, 2.0312,\n",
       "        2.0781, 1.5625, 2.5156, 1.4375, 1.6641, 1.9766, 2.2969, 1.9531, 2.0312,\n",
       "        2.2344, 2.3906, 2.3750, 2.1875, 2.4219, 2.2969, 2.7188, 2.8594, 2.3750,\n",
       "        2.4219, 1.9062, 1.9453, 2.3750, 2.4219, 2.2500, 1.9062, 2.1094, 2.0469,\n",
       "        1.9375, 2.4375, 2.6406, 2.3906, 1.8594, 2.2188, 1.5156, 1.7656, 1.8750,\n",
       "        2.0156, 1.8516, 1.7812, 1.9062, 2.1250, 2.2812, 2.1094, 2.0469, 2.3281,\n",
       "        2.1562, 2.3125, 2.1562, 2.2656, 1.8672, 2.1250, 1.9766, 2.1875, 2.0469,\n",
       "        1.7891, 1.9766, 1.8359, 1.7266, 2.4375, 2.3906, 2.2500, 1.8516, 2.6875,\n",
       "        1.5625, 1.8125, 1.9922, 2.1250, 2.1406, 1.9453, 2.2344, 2.3906, 2.3750,\n",
       "        2.1250, 2.3125, 2.0938, 2.7500, 2.9844, 2.4688, 2.5781, 1.9375, 2.1719,\n",
       "        2.5000, 2.5469, 2.1719, 1.9609, 2.2812, 2.2031, 1.8750, 2.2812, 2.5625,\n",
       "        2.4531, 1.8828, 1.8438, 1.5312, 1.7812, 2.0312, 2.2031, 2.0625, 1.9844,\n",
       "        2.3438, 2.4844, 2.5156, 2.1719, 2.3750, 2.8594, 2.1562, 2.9375, 2.5312,\n",
       "        2.5312, 1.9219, 1.8281, 2.5625, 2.5312, 2.3750, 1.8594, 2.1875, 1.9141,\n",
       "        1.9688, 1.8984, 1.9609, 2.0469, 1.4219, 2.7656, 1.5078, 1.6562, 1.8516,\n",
       "        2.0469, 1.9062, 1.8281, 1.8672, 2.0625, 2.2812, 2.0781, 2.3906, 2.3906,\n",
       "        2.2031, 2.2031, 2.3906, 2.3906, 1.8672, 1.8359, 2.4844, 2.6094, 2.0781,\n",
       "        2.0312, 2.2656, 2.1719, 2.0469, 2.6094, 2.7188, 2.3906, 1.7266, 2.6094,\n",
       "        1.5469, 1.7422, 1.9375, 2.0469, 2.2500, 2.0938, 2.2812, 2.4844, 2.4844,\n",
       "        2.1250, 2.4062, 2.8125, 2.1094, 2.7969, 2.4688, 2.5000, 1.8750, 2.2344,\n",
       "        2.2188, 2.4375, 2.3281, 1.9062, 2.1562, 1.8984, 1.7891, 2.5625, 2.4062,\n",
       "        2.3906, 1.8125, 2.7969, 1.5391, 1.8203, 1.9844, 2.1406, 1.9453, 1.9844,\n",
       "        2.4688, 2.5312, 2.6406, 2.2500, 2.6094, 2.5469, 2.9375, 3.0312, 2.5625,\n",
       "        2.7656, 2.0625, 2.3125, 2.6406, 2.6719, 2.2344, 2.0000, 2.4062, 2.2031,\n",
       "        2.0312, 2.3750, 2.6875, 2.3906, 1.8281, 2.3438, 1.5312, 1.7500, 1.8828,\n",
       "        1.9375, 1.9141, 1.7734, 1.9531, 2.1875, 2.3906, 2.1406, 2.2031, 2.4219,\n",
       "        2.0625, 2.4062, 2.1094, 2.3438, 1.7578, 1.6719, 2.3750, 2.2500, 2.3125,\n",
       "        1.8047, 2.0000, 1.7891, 1.8125, 1.8203, 1.9062, 1.8047, 1.3281, 3.0469,\n",
       "        1.2031, 1.5312, 1.7031, 1.7422, 2.2188, 2.2500, 2.0781, 1.9766, 2.1875,\n",
       "        2.6250, 2.6406, 2.5625, 2.5781, 2.5781, 2.5156, 2.5156, 2.1094, 2.0000,\n",
       "        2.5000, 2.5156, 2.3750, 1.8359, 1.9766, 2.0625, 2.1875, 2.2500, 2.2656,\n",
       "        2.0469, 1.5938, 1.8047, 1.2031, 1.7266, 1.6406, 1.5234, 1.4844, 1.4375,\n",
       "        1.4688, 1.5547, 1.5312, 1.3203, 1.2109, 1.1875, 1.2578, 1.2266, 1.1172,\n",
       "        1.0469, 1.0781, 1.2031, 1.4219, 1.4375, 1.3047, 1.0859, 1.2422, 1.1172,\n",
       "        1.3125, 1.4844, 1.3281, 1.2422, 1.8984], device='cuda:0',\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.abs(logits1 - logits2),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5ad532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3451e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609d3c68c8084415b615279d7ddd0e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    \"google/gemma-3-4b-it\",\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    # device_map=\"auto\",\n",
    "    # attn_implementation=\"sdpa\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aebb28f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randint(0, 1000, (1, 256))#.to(\"cuda\")\n",
    "start_idx = 128\n",
    "end_idx = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35b69d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "with torch.no_grad():\n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        outputs_full = model(inputs)\n",
    "        logits1 = outputs_full.logits[0, start_idx: end_idx + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05f92f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import HybridCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9ab19ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_key_values = HybridCache(model.config.text_config, 1, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd8c695",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 256 is out of bounds for dimension 1 with size 256",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Iterate through the continuation tokens\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(end_idx \u001b[38;5;241m-\u001b[39m start_idx):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Prepare input for the next iteration: only the current continuation token\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     current_token_id_tensor \u001b[38;5;241m=\u001b[39m inputs[:, start_idx \u001b[38;5;241m+\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Model call with the single next token and past_key_values\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16):\n",
      "\u001b[0;31mIndexError\u001b[0m: index 256 is out of bounds for dimension 1 with size 256"
     ]
    }
   ],
   "source": [
    "logits2 = []\n",
    "with torch.no_grad():\n",
    "    # Initial pass with the input prompt\n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        outputs_kv_init = model(inputs[:,:start_idx + 1], use_cache=True, past_key_values=past_key_values)\n",
    "        logits2.append(outputs_kv_init.logits[0, -1, :])\n",
    "        # Store the KV cache from initial pass\n",
    "        past_key_values = outputs_kv_init.past_key_values\n",
    "\n",
    "    # Iterate through the continuation tokens\n",
    "    for i in range(end_idx - start_idx - 1):\n",
    "        # Prepare input for the next iteration: only the current continuation token\n",
    "        current_token_id_tensor = inputs[:, start_idx + i + 1].unsqueeze(1)\n",
    "\n",
    "        # Model call with the single next token and past_key_values\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            outputs_kv_step = model(input_ids=current_token_id_tensor, \n",
    "                                   past_key_values=past_key_values, \n",
    "                                   use_cache=True)\n",
    "            # Store the logits from this step - CORRECTION HERE\n",
    "            logits2.append(outputs_kv_step.logits[0, -1, :])\n",
    "            # Update KV cache with new values\n",
    "            past_key_values = outputs_kv_step.past_key_values\n",
    "logits2 = torch.stack(logits2, dim=0)\n",
    "logits2.shape, logits1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e0c2842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 262208]), torch.Size([128, 262208]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logits2 = torch.stack(logits2, dim=0)\n",
    "logits2.shape, logits1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ea6b216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.5761e-06, 7.2332e-06, 7.8308e-06, 8.9197e-06, 7.5235e-06, 7.1803e-06,\n",
       "        9.2514e-06, 1.0456e-05, 9.3818e-06, 1.2672e-05, 9.0390e-06, 8.1634e-06,\n",
       "        7.5472e-06, 1.1036e-05, 1.1434e-05, 1.0017e-05, 9.7087e-06, 1.2099e-05,\n",
       "        1.3566e-05, 1.0992e-05, 8.9328e-06, 8.0241e-06, 7.5371e-06, 8.4412e-06,\n",
       "        1.1368e-05, 7.4331e-06, 8.1296e-06, 8.6147e-06, 8.4934e-06, 1.0547e-05,\n",
       "        9.2572e-06, 7.8645e-06, 9.2196e-06, 1.1559e-05, 9.1299e-06, 1.1141e-05,\n",
       "        9.9925e-06, 1.1491e-05, 8.4543e-06, 9.1530e-06, 8.3192e-06, 8.1446e-06,\n",
       "        9.0655e-06, 1.0497e-05, 8.2049e-06, 1.0780e-05, 7.5738e-06, 8.0629e-06,\n",
       "        6.8507e-06, 1.0084e-05, 9.1416e-06, 8.0334e-06, 8.4851e-06, 1.0729e-05,\n",
       "        1.3607e-05, 7.8625e-06, 7.1217e-06, 7.0614e-06, 7.1365e-06, 9.5032e-06,\n",
       "        1.0092e-05, 7.0158e-06, 8.9976e-06, 9.3340e-06, 1.2156e-05, 8.5299e-06,\n",
       "        9.6471e-06, 1.3323e-05, 1.4118e-05, 1.1182e-05, 7.2470e-06, 9.4420e-06,\n",
       "        1.4857e-05, 1.0573e-05, 6.9856e-06, 6.7008e-06, 8.0400e-06, 6.8249e-06,\n",
       "        6.3196e-06, 6.5911e-06, 7.4116e-06, 6.7315e-06, 7.1983e-06, 6.5463e-06,\n",
       "        8.3482e-06, 8.0930e-06, 5.2618e-06, 9.3029e-06, 7.1126e-06, 8.6791e-06,\n",
       "        9.0481e-06, 8.4542e-06, 6.8355e-06, 8.4746e-06, 7.4499e-06, 8.2346e-06,\n",
       "        7.0119e-06, 6.8156e-06, 7.0467e-06, 8.4187e-06, 8.6727e-06, 9.1642e-06,\n",
       "        7.2884e-06, 7.7947e-06, 6.7549e-06, 8.4489e-06, 7.1558e-06, 7.1298e-06,\n",
       "        6.3564e-06, 4.9521e-06, 5.7325e-06, 6.7724e-06, 6.3755e-06, 5.5515e-06,\n",
       "        4.1408e-06, 1.1932e-05, 4.5188e-06, 4.8639e-06, 1.4389e-05, 1.1451e-05,\n",
       "        5.5890e-06, 4.4352e-06, 4.8694e-06, 5.0882e-06, 4.2243e-06, 4.2550e-06,\n",
       "        4.4754e-06, 4.3902e-06])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.abs(logits1 - logits2),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbb5ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
