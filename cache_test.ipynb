{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d55160d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-29 20:29:27 [__init__.py:239] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.4.3: Fast Gemma3 patching. Transformers: 4.51.3. vLLM: 0.8.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.642 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "# from transformers.models.gemma3.modeling_gemma3 import Gemma3TextScaledWordEmbedding\n",
    "from functions import *\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-pt-unsloth-bnb-4bit\",\n",
    "    max_seq_length = 8192, # Choose any for long context!\n",
    "    load_in_4bit = True,\n",
    "    resize_model_vocab=24,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a787dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.image_token_index = 16 # new image token\n",
    "model.language_model.lm_head.weight.requires_grad_(True);\n",
    "# # model.language_model.lm_head.load_state_dict(torch.load(\"/home/zhenlan/Desktop/Projects/ARC2/Model/gemma24.pth\"))\n",
    "model.language_model.lm_head.load_state_dict(torch.load(\"../../Model/lm_heads_weights_VLM2.pth\"))\n",
    "model = PeftModel.from_pretrained(model, \"../../Model/merged_model_VLM2\", is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8afbfc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.language_model.model.gradient_checkpointing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007f3632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "output_path = '/home/zhenlan/Desktop/Projects/ARC2/Data/ARC-AGI-2-main/combined_data.json'\n",
    "with open(output_path, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d630705",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_f, y_f = next(iter(data_gen_VLM(data, False, tokenizer, 3, False)))\n",
    "inputs_t, y_t = next(iter(data_gen_VLM(data, False, tokenizer, 3, True)))\n",
    "start_idx = len(inputs_t['input_ids'][0]) - 1\n",
    "end_idx = len(inputs_f['input_ids'][0]) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff67e79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3887, 4756)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33a45526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logit1 and logit2 give different results, suggesting something is sequence length dependent.\n",
    "# tested mannual casual mask, still see the difference. So not caused by mask.\n",
    "# model.eval();\n",
    "# with torch.no_grad():\n",
    "#     with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "#         outputs_full = model(**inputs_f)\n",
    "#         logits1 = outputs_full.logits[0, :start_idx+1]\n",
    "\n",
    "# # shorten inputs\n",
    "# inputs_f['input_ids'] = inputs_f['input_ids'][:, :start_idx+1]\n",
    "# # inputs_f['attention_mask'] = inputs_f['attention_mask'][:, :, :start_idx+1, :start_idx+1]\n",
    "# inputs_f['attention_mask'] = inputs_f['attention_mask'][:, :start_idx+1]\n",
    "# inputs_f['token_type_ids'] = inputs_f['token_type_ids'][:, :start_idx+1]\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "#         outputs_full = model(**inputs_f)\n",
    "#         logits2 = outputs_full.logits\n",
    "# torch.sum(logits1.argmax(-1) == logits2.argmax(-1))/logits1.shape[0]\n",
    "# torch.mean(torch.abs(logits1 - logits2)) # original mask, no peft\n",
    "# torch.mean(torch.abs(logits1 - logits2)) # casual mask\n",
    "# torch.mean(torch.abs(logits1 - logits2)) # original mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8c735e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "with torch.no_grad():\n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        outputs_full = model(**inputs_f)\n",
    "        logits1 = outputs_full.logits[0, start_idx: end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5123de7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits2 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Initial pass with the input prompt\n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        outputs_kv_init = model(**inputs_t, use_cache=True)\n",
    "        logits2.append(outputs_kv_init.logits[0, -1, :])\n",
    "        # Store the KV cache from initial pass\n",
    "        past_key_values = outputs_kv_init.past_key_values\n",
    "\n",
    "    # Iterate through the continuation tokens\n",
    "    for i in range(end_idx - start_idx - 1):\n",
    "        # Prepare input for the next iteration: only the current continuation token\n",
    "        current_token_id_tensor = inputs_f['input_ids'][:, start_idx + i + 1].unsqueeze(1)\n",
    "\n",
    "        # Model call with the single next token and past_key_values\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            outputs_kv_step = model(input_ids=current_token_id_tensor, \n",
    "                                   past_key_values=past_key_values, \n",
    "                                   use_cache=True)\n",
    "            # Store the logits from this step - CORRECTION HERE\n",
    "            logits2.append(outputs_kv_step.logits[0, -1, :])\n",
    "            # Update KV cache with new values\n",
    "            past_key_values = outputs_kv_step.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5673cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits2 = torch.stack(logits2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1191f393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1914, 1.7734, 1.4766, 1.5547, 1.8594, 2.0000, 1.8281, 2.0781, 2.0312,\n",
       "        2.1094, 2.1875, 2.2031, 2.3281, 2.2656, 2.1094, 2.0312, 1.8906, 1.9922,\n",
       "        2.0781, 2.2031, 2.0312, 1.7500, 1.7188, 1.9062, 2.0312, 2.0156, 2.0625,\n",
       "        1.9922, 2.0938, 2.2500, 1.1953, 1.6641, 1.9219, 1.6797, 1.9297, 2.4219,\n",
       "        2.5781, 2.3906, 3.2812, 2.8594, 1.9766, 2.3750, 2.5312, 2.5781, 1.9766,\n",
       "        2.5938, 2.0469, 2.0312, 2.5156, 2.4062, 2.3594, 2.0312, 2.2656, 1.8750,\n",
       "        2.0781, 2.3281, 2.2188, 2.1562, 2.3281, 3.3125, 2.4844, 1.8438, 2.1562,\n",
       "        2.2969, 2.5156, 2.6719, 1.9062, 2.2812, 2.2344, 2.4219, 2.5625, 2.5469,\n",
       "        2.7656, 2.0469, 2.5156, 2.0000, 1.9062, 2.0625, 1.8203, 1.8594, 2.1094,\n",
       "        2.1719, 1.5469, 1.6406, 1.8125, 2.3125, 2.4531, 1.4609, 2.2031, 3.9688,\n",
       "        2.7031, 2.3281, 1.9844, 1.8438, 2.3438, 2.0781, 2.1094, 2.4531, 2.2344,\n",
       "        2.2031, 3.0000, 2.5000, 2.9219, 2.6875, 2.8438, 2.2969, 1.5938, 1.5391,\n",
       "        1.8750, 2.1406, 1.4219, 2.2031, 1.6953, 1.6797, 1.9141, 2.1094, 2.0781,\n",
       "        1.7266, 1.5547, 4.4688, 2.5156, 2.1406, 2.2188, 1.5469, 2.3438, 2.3750,\n",
       "        2.4219, 2.5938, 2.4688, 2.3594, 2.1094, 2.9844, 2.0469, 1.6562, 2.7500,\n",
       "        2.1250, 2.0469, 2.2344, 2.3906, 1.5547, 1.4766, 2.3594, 1.9609, 2.0469,\n",
       "        1.4219, 2.0156, 2.1406, 1.5391, 2.4375, 5.1562, 2.6094, 2.2812, 2.5000,\n",
       "        2.0312, 2.9219, 3.0156, 2.0625, 2.9688, 2.9375, 2.7188, 3.1406, 2.9375,\n",
       "        3.3906, 1.9688, 2.9219, 1.7422, 1.3672, 1.5469, 1.9531, 2.4688, 1.6484,\n",
       "        2.7500, 2.0469, 2.1094, 1.7578, 2.2031, 2.1094, 1.9844, 2.6094, 5.6250,\n",
       "        2.5156, 2.0469, 1.6875, 1.6172, 2.7812, 2.7812, 2.7969, 2.8438, 2.5469,\n",
       "        2.2500, 2.4375, 2.8125, 2.0312, 2.3281, 2.5000, 2.0000, 1.4062, 1.2969,\n",
       "        1.4219, 2.2188, 2.9531, 2.4219, 2.7500, 2.1719, 1.6875, 2.2812, 2.5469,\n",
       "        2.2344, 2.9219, 6.0312, 2.5312, 2.5156, 2.0938, 1.5312, 2.5781, 2.8594,\n",
       "        2.7031, 2.8906, 2.8906, 2.6094, 2.4844, 3.0312, 2.4844, 1.9062, 2.8594,\n",
       "        2.3438, 1.3359, 1.6094, 1.7891, 2.5781, 2.0938, 2.7500, 2.2344, 1.9688,\n",
       "        1.4609, 2.0000, 2.0469, 1.7656, 2.6406, 6.1562, 2.5469, 2.5781, 2.1406,\n",
       "        1.7734, 2.9531, 3.2344, 2.5000, 2.9062, 3.0312, 1.9297, 2.4219, 2.4062,\n",
       "        2.3438, 2.8125, 3.2188, 2.8438, 1.4844, 1.6562, 2.0625, 2.7031, 2.8750,\n",
       "        2.8281, 2.3750, 2.2344, 1.7969, 2.8125, 2.8281, 2.1406, 2.8594, 6.2500,\n",
       "        2.6406, 2.0781, 1.7656, 1.7266, 3.0625, 2.8125, 2.3906, 2.8281, 2.3906,\n",
       "        2.6562, 2.6562, 2.7656, 2.7656, 2.7188, 3.2344, 2.3594, 1.5234, 1.6484,\n",
       "        2.0312, 2.9062, 2.2344, 2.8750, 2.2812, 2.1562, 2.0156, 2.2500, 2.7344,\n",
       "        2.3438, 2.8906, 6.5312, 2.7812, 2.1875, 2.4219, 1.9609, 3.4844, 3.2656,\n",
       "        2.4844, 2.6719, 2.2188, 2.2812, 2.2031, 2.9688, 2.7344, 2.2344, 2.7812,\n",
       "        2.0469, 1.7969, 1.7422, 2.2031, 2.9531, 2.7031, 2.6875, 2.1875, 2.0312,\n",
       "        1.7656, 2.5625, 3.2656, 2.2188, 2.7969, 6.5625, 2.7656, 2.3906, 2.2500,\n",
       "        1.7734, 2.9688, 3.5469, 2.1250, 2.7031, 2.4531, 2.4531, 2.7656, 2.5625,\n",
       "        3.3750, 2.0469, 2.8438, 2.7656, 2.2031, 2.3125, 2.7969, 3.3438, 2.1562,\n",
       "        2.7500, 2.4375, 2.5156, 2.0000, 2.0781, 2.5312, 2.8594, 3.1719, 6.2812,\n",
       "        2.6719, 2.6094, 2.1406, 1.5078, 2.7031, 2.5312, 2.6094, 2.7812, 2.2656,\n",
       "        1.6094, 1.4688, 1.8359, 1.8828, 2.0000, 2.4688, 2.2344, 1.7656, 1.8984,\n",
       "        2.5156, 2.1250, 2.5312, 3.0000, 2.6094, 2.3281, 1.6797, 1.7344, 2.6094,\n",
       "        2.3750, 2.9219, 6.5938, 2.6250, 2.4062, 2.3594, 2.0312, 3.4688, 3.1094,\n",
       "        2.5156, 2.6250, 2.2188, 1.5938, 2.1562, 2.3281, 2.3281, 2.6094, 3.0781,\n",
       "        2.5781, 1.6250, 1.7031, 1.7422, 2.7344, 2.0156, 2.5469, 2.4375, 2.3125,\n",
       "        1.9609, 1.9062, 2.2969, 2.2812, 2.7031, 6.5938, 2.7031, 2.4531, 2.0625,\n",
       "        1.5938, 3.0156, 3.4531, 2.7500, 3.2188, 3.1250, 2.0625, 2.7344, 2.3281,\n",
       "        2.4531, 2.6875, 3.4062, 2.4062, 1.4141, 1.8594, 1.9062, 2.4062, 2.4531,\n",
       "        2.9375, 2.3594, 2.2344, 1.7500, 2.6406, 3.3594, 2.5312, 3.2344, 6.7812,\n",
       "        2.5625, 2.2656, 1.7734, 1.2812, 2.8906, 2.8594, 3.4688, 3.2188, 2.7500,\n",
       "        2.2656, 2.0156, 2.7344, 2.8906, 2.4062, 2.6406, 2.3906, 1.7891, 2.2812,\n",
       "        2.8281, 3.2969, 2.1875, 2.8438, 2.6406, 2.8906, 2.6094, 2.8125, 2.9375,\n",
       "        2.5312, 2.8750, 7.0312, 2.4844, 2.4062, 2.4062, 1.5312, 2.8438, 2.9375,\n",
       "        3.2969, 3.1875, 2.8125, 2.8594, 2.4531, 2.6562, 2.6875, 2.6719, 2.7656,\n",
       "        2.1094, 1.4297, 1.7656, 2.4531, 2.3281, 2.4062, 3.0312, 2.4688, 2.5938,\n",
       "        1.9453, 2.3438, 2.7656, 2.4844, 3.0469, 7.0625, 2.6875, 2.6562, 2.5781,\n",
       "        1.9766, 3.2812, 3.5938, 3.0156, 3.2188, 2.9219, 2.1719, 2.5000, 2.8750,\n",
       "        2.7969, 1.9141, 2.5156, 2.6562, 2.3750, 2.9375, 3.6875, 3.9531, 2.2812,\n",
       "        2.8438, 2.6875, 2.6250, 1.9453, 1.9141, 2.7812, 2.5000, 2.8438, 6.9375,\n",
       "        2.5312, 2.6406, 2.4531, 2.8125, 3.3281, 3.3438, 2.8750, 2.9688, 2.6562,\n",
       "        2.7969, 2.5000, 2.6719, 3.1250, 2.4688, 2.6562, 2.3438, 2.0625, 1.6406,\n",
       "        2.0469, 2.9062, 2.7969, 2.8750, 2.4688, 2.3750, 2.0156, 3.0312, 3.5625,\n",
       "        2.3750, 3.0000, 7.1875, 2.5469, 2.3125, 2.4531, 2.0625, 3.5469, 3.2188,\n",
       "        2.5156, 2.7656, 2.3750, 2.5000, 3.2031, 3.3125, 2.8594, 2.7656, 2.9844,\n",
       "        2.6406, 2.3594, 2.5469, 3.0469, 3.5938, 2.3906, 3.0781, 2.8125, 2.9219,\n",
       "        2.6094, 2.5000, 3.2500, 2.4688, 2.8594, 7.1562, 2.5156, 2.6406, 2.6250,\n",
       "        1.6953, 3.0156, 3.2188, 2.3281, 2.9375, 2.3125, 1.6953, 2.7344, 2.3750,\n",
       "        2.4531, 2.4219, 2.8438, 2.5781, 1.5781, 1.8828, 2.4531, 1.8125, 2.3906,\n",
       "        2.9844, 2.5312, 2.4688, 1.5156, 2.0312, 2.9062, 2.5312, 3.0312, 6.8750,\n",
       "        2.5156, 2.2812, 2.4375, 1.5547, 2.9375, 2.8125, 2.4062, 2.6719, 2.4531,\n",
       "        2.0938, 1.7812, 2.4688, 2.5000, 2.4531, 2.6719, 2.3125, 1.8594, 2.3594,\n",
       "        2.8438, 3.2344, 2.2188, 2.8750, 2.5781, 2.4688, 1.8984, 2.3438, 2.7969,\n",
       "        2.6094, 2.7812, 7.0938, 2.2344, 2.2500, 2.2188, 1.4453, 2.9375, 2.8906,\n",
       "        2.5312, 2.7969, 2.4375, 2.3438, 2.0156, 2.4375, 2.9688, 2.5312, 2.6406,\n",
       "        2.2812, 1.7656, 1.7422, 1.8750, 2.4375, 2.3281, 2.5938, 2.3281, 2.2812,\n",
       "        1.7734, 2.7031, 3.2656, 2.1875, 3.0000, 7.0625, 2.1875, 2.6094, 2.1875,\n",
       "        1.8359, 2.9844, 3.2969, 2.2031, 2.8281, 2.4062, 2.1875, 2.6875, 2.7656,\n",
       "        2.4219, 1.6953, 2.7188, 2.4688, 1.7656, 2.0000, 2.5469, 3.1719, 2.0781,\n",
       "        2.8281, 2.5156, 2.5156, 1.6875, 2.3281, 3.2031, 2.7188, 3.0938, 7.1250,\n",
       "        2.2500, 2.4062, 2.0312, 1.1719, 2.7969, 2.7344, 2.4219, 2.8750, 2.5625,\n",
       "        1.8359, 1.7031, 2.0312, 2.7500, 2.3750, 2.6250, 2.4062, 1.7266, 1.8516,\n",
       "        2.4844, 1.5938, 2.5781, 3.2031, 2.6094, 2.5625, 1.4922, 2.1719, 3.1406,\n",
       "        2.7031, 3.0781, 7.2500, 2.5469, 2.7188, 2.7969, 1.5781, 2.7188, 2.9375,\n",
       "        2.5312, 2.8750, 2.4688, 2.1562, 1.6406, 2.8906, 2.7344, 2.6250, 3.1719,\n",
       "        2.6250, 1.9141, 2.6094, 3.1250, 3.4219, 2.4219, 3.0625, 2.6406, 2.6562,\n",
       "        1.9766, 2.5781, 3.1875, 2.7500, 3.0469, 7.3125, 2.2344, 2.6719, 2.5625,\n",
       "        1.7656, 2.9844, 3.5469, 2.6094, 3.5000, 2.8750, 1.8984, 2.8750, 2.5781,\n",
       "        2.7031, 2.7344, 3.2500, 2.8906, 2.1250, 2.2969, 2.2500, 2.5938, 2.4688,\n",
       "        2.8906, 2.4062, 2.3281, 1.8828, 2.8125, 3.5156, 2.4844, 3.5312, 7.6875,\n",
       "        2.2188, 2.5469, 1.7422, 2.6406, 2.9062, 2.9688, 2.8438, 2.3438, 2.5312,\n",
       "        2.0625, 2.5312, 3.2656, 3.1562, 3.1562, 3.0000, 2.4375, 2.1562, 2.4531,\n",
       "        2.6406, 2.8281, 2.9844, 2.8906, 2.5781, 1.9766, 2.5625, 3.2031, 3.2969,\n",
       "        3.0156, 3.0938, 7.6250, 1.7422, 2.2656, 2.6719, 3.1250, 3.0625, 2.9531,\n",
       "        3.8281, 3.6719, 3.0156, 2.8750, 2.8906, 2.8906, 3.3438, 3.5781, 3.3438,\n",
       "        2.9531, 2.6875, 2.7031, 2.9531, 3.2344, 3.1094, 2.9844, 2.8594, 2.9531,\n",
       "        2.9375, 3.1875, 3.2500, 2.9844, 3.2969], device='cuda:0',\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.abs(logits1 - logits2),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd94d21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
