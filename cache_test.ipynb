{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d55160d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 05-03 14:37:01 [__init__.py:239] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.4.3: Fast Gemma3 patching. Transformers: 4.51.3. vLLM: 0.8.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.642 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "# from transformers.models.gemma3.modeling_gemma3 import Gemma3TextScaledWordEmbedding\n",
    "from functions import *\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-pt-unsloth-bnb-4bit\",\n",
    "    max_seq_length = 8192, # Choose any for long context!\n",
    "    load_in_4bit = True,\n",
    "    resize_model_vocab=24,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a787dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.image_token_index = 16 # new image token\n",
    "model.language_model.lm_head.weight.requires_grad_(True);\n",
    "# # model.language_model.lm_head.load_state_dict(torch.load(\"/home/zhenlan/Desktop/Projects/ARC2/Model/gemma24.pth\"))\n",
    "model.language_model.lm_head.load_state_dict(torch.load(\"../../Model/lm_heads_weights_VLM2.pth\"))\n",
    "model = PeftModel.from_pretrained(model, \"../../Model/merged_model_VLM2\", is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8afbfc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.language_model.model.gradient_checkpointing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007f3632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "output_path = '/home/zhenlan/Desktop/Projects/ARC2/Data/ARC-AGI-2-main/combined_data.json'\n",
    "with open(output_path, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d630705",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_f, y_f = next(iter(data_gen_VLM(data, False, tokenizer, 3, False)))\n",
    "inputs_t, y_t = next(iter(data_gen_VLM(data, False, tokenizer, 3, True)))\n",
    "start_idx = len(inputs_t['input_ids'][0]) - 1\n",
    "end_idx = len(inputs_f['input_ids'][0]) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff67e79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3887, 4756)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33a45526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logit1 and logit2 give different results, suggesting something is sequence length dependent.\n",
    "# tested mannual casual mask, still see the difference. So not caused by mask.\n",
    "# model.eval();\n",
    "# with torch.no_grad():\n",
    "#     with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "#         outputs_full = model(**inputs_f)\n",
    "#         logits1 = outputs_full.logits[0, :start_idx+1]\n",
    "\n",
    "# # shorten inputs\n",
    "# inputs_f['input_ids'] = inputs_f['input_ids'][:, :start_idx+1]\n",
    "# # inputs_f['attention_mask'] = inputs_f['attention_mask'][:, :, :start_idx+1, :start_idx+1]\n",
    "# inputs_f['attention_mask'] = inputs_f['attention_mask'][:, :start_idx+1]\n",
    "# inputs_f['token_type_ids'] = inputs_f['token_type_ids'][:, :start_idx+1]\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "#         outputs_full = model(**inputs_f)\n",
    "#         logits2 = outputs_full.logits\n",
    "# torch.sum(logits1.argmax(-1) == logits2.argmax(-1))/logits1.shape[0]\n",
    "# torch.mean(torch.abs(logits1 - logits2)) # original mask, no peft\n",
    "# torch.mean(torch.abs(logits1 - logits2)) # casual mask\n",
    "# torch.mean(torch.abs(logits1 - logits2)) # original mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8c735e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "with torch.no_grad():\n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        outputs_full = model(**inputs_f)\n",
    "        logits1 = outputs_full.logits[0, start_idx: end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2375a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3888]), torch.Size([1, 1, 3888, 3888]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_t['input_ids'][0].shape, inputs_t['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5123de7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import HybridCache\n",
    "# past_key_values = HybridCache(model.config.text_config, 1, 5600)\n",
    "# logits2 = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     # Initial pass with the input prompt\n",
    "#     with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "#         outputs_kv_init = model(**inputs_t, use_cache=True, past_key_values=past_key_values)\n",
    "#         # outputs_kv_init = model(**inputs_t)\n",
    "#         logits2.append(outputs_kv_init.logits[0, -1, :])\n",
    "#         # Store the KV cache from initial pass\n",
    "#         past_key_values = outputs_kv_init.past_key_values\n",
    "\n",
    "#     # Iterate through the continuation tokens\n",
    "#     for i in range(end_idx - start_idx - 1):\n",
    "#         # Prepare input for the next iteration: only the current continuation token\n",
    "#         current_token_id_tensor = inputs_f['input_ids'][:, start_idx + i + 1].unsqueeze(1)\n",
    "\n",
    "#         # Model call with the single next token and past_key_values\n",
    "#         with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "#             outputs_kv_step = model(input_ids=current_token_id_tensor, \n",
    "#                                    past_key_values=past_key_values, \n",
    "#                                    use_cache=True,\n",
    "#                                 #    attention_mask=inputs_f['attention_mask'][:, :start_idx + i + 2], # .unsqueeze(0).unsqueeze(0)\n",
    "#                                 #    token_type_ids=inputs_f['token_type_ids'][:, start_idx + i + 1:start_idx + i + 2],\n",
    "#                                    ) # \n",
    "#             # Store the logits from this step - CORRECTION HERE\n",
    "#             logits2.append(outputs_kv_step.logits[0, -1, :])\n",
    "#             # Update KV cache with new values\n",
    "#             past_key_values = outputs_kv_step.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb4c6bf7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5600) must match the size of tensor b (3888) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# --- Initial pass with the input prompt ---\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16):\n\u001b[0;32m---> 17\u001b[0m         outputs_kv_init \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m     18\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs_t,\n\u001b[1;32m     19\u001b[0m             use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m             past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m     21\u001b[0m         )\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;66;03m# Store the logit for the *last* token of the prompt\u001b[39;00m\n\u001b[1;32m     23\u001b[0m         logits2\u001b[38;5;241m.\u001b[39mappend(outputs_kv_init\u001b[38;5;241m.\u001b[39mlogits[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/peft/peft_model.py:1719\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1718\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1719\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model(\n\u001b[1;32m   1720\u001b[0m             input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1721\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1722\u001b[0m             inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1723\u001b[0m             labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m   1724\u001b[0m             output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1725\u001b[0m             output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1726\u001b[0m             return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1727\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1728\u001b[0m         )\n\u001b[1;32m   1730\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1732\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/unsloth_zoo/temporary_patches.py:252\u001b[0m, in \u001b[0;36mpatch_Gemma3ForConditionalGeneration.<locals>.forward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, logits_to_keep, **lm_kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m     labels[attention_mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage_model(\n\u001b[1;32m    253\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m    254\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[1;32m    255\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    256\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    257\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m    258\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    259\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    260\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    261\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    262\u001b[0m     logits_to_keep\u001b[38;5;241m=\u001b[39mlogits_to_keep,\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlm_kwargs,\n\u001b[1;32m    264\u001b[0m )\n\u001b[1;32m    265\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    268\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/Projects/ARC2/Code/ARI2025/unsloth_compiled_cache/unsloth_compiled_module_gemma3.py:516\u001b[0m, in \u001b[0;36mGemma3ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    503\u001b[0m     input_ids: Optional[torch\u001b[38;5;241m.\u001b[39mLongTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs,\n\u001b[1;32m    515\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CausalLMOutputWithPast:\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Gemma3ForCausalLM_forward(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/Projects/ARC2/Code/ARI2025/unsloth_compiled_cache/unsloth_compiled_module_gemma3.py:337\u001b[0m, in \u001b[0;36mGemma3ForCausalLM_forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    334\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    335\u001b[0m )\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m    338\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    339\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    340\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    341\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    342\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m    343\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    344\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    345\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    346\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs,\n\u001b[1;32m    348\u001b[0m )\n\u001b[1;32m    350\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:726\u001b[0m, in \u001b[0;36mGemma3TextModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, last_cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    712\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    713\u001b[0m         partial(decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs),\n\u001b[1;32m    714\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    723\u001b[0m         last_cache_position,\n\u001b[1;32m    724\u001b[0m     )\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 726\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m    727\u001b[0m         hidden_states,\n\u001b[1;32m    728\u001b[0m         position_embeddings_global\u001b[38;5;241m=\u001b[39mposition_embeddings_global,\n\u001b[1;32m    729\u001b[0m         position_embeddings_local\u001b[38;5;241m=\u001b[39mposition_embeddings_local,\n\u001b[1;32m    730\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[1;32m    731\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    732\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    733\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    734\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    735\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    736\u001b[0m         last_cache_position\u001b[38;5;241m=\u001b[39mlast_cache_position,\n\u001b[1;32m    737\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[1;32m    738\u001b[0m     )\n\u001b[1;32m    740\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:420\u001b[0m, in \u001b[0;36mGemma3DecoderLayer.forward\u001b[0;34m(self, hidden_states, position_embeddings_global, position_embeddings_local, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, last_cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m position_embeddings_global\n\u001b[0;32m--> 420\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    421\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    422\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m    423\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    424\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    425\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m    426\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    427\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    428\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    430\u001b[0m )\n\u001b[1;32m    431\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m    432\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/unsloth_zoo/temporary_patches.py:546\u001b[0m, in \u001b[0;36mpatch_Gemma3Attention.<locals>.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m         key_states, value_states \u001b[38;5;241m=\u001b[39m key_states[:, :, :seq_len, :], value_states[:, :, :seq_len, :]\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# attention_interface: Callable = eager_attention_forward\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;66;03m# if self.config._attn_implementation != \"eager\":\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;66;03m#     if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;66;03m#     **kwargs,\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m--> 546\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m scaled_dot_product_attention(\n\u001b[1;32m    547\u001b[0m     query_states\u001b[38;5;241m.\u001b[39mto(downcast_dtype),\n\u001b[1;32m    548\u001b[0m     key_states\u001b[38;5;241m.\u001b[39mto(downcast_dtype),\n\u001b[1;32m    549\u001b[0m     value_states\u001b[38;5;241m.\u001b[39mto(downcast_dtype),\n\u001b[1;32m    550\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask\u001b[38;5;241m.\u001b[39mto(downcast_dtype),\n\u001b[1;32m    551\u001b[0m     dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_dropout \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m    552\u001b[0m     scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling,\n\u001b[1;32m    553\u001b[0m     enable_gqa\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_key_value_groups\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    554\u001b[0m )\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    556\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;66;03m#.contiguous()\u001b[39;00m\n\u001b[1;32m    557\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj(attn_output)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5600) must match the size of tensor b (3888) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import HybridCache\n",
    "past_key_values = HybridCache(model.config.text_config, 1, 5600)\n",
    "# Assume 'model', 'inputs_t', 'inputs_f', 'start_idx', 'end_idx', 'past_key_values'\n",
    "# are already defined and loaded appropriately.\n",
    "# 'model' is a Hugging Face transformer model supporting 'cache_position'.\n",
    "# 'inputs_t' contains the initial prompt tensors (e.g., {'input_ids': tensor, 'attention_mask': tensor})\n",
    "# 'inputs_f' contains the full sequence including continuation tokens.\n",
    "# 'past_key_values' might be None initially or contain a pre-filled cache.\n",
    "\n",
    "logits2 = []\n",
    "current_seq_len = 0 # Keep track of the sequence length processed so far\n",
    "\n",
    "with torch.no_grad():\n",
    "    # --- Initial pass with the input prompt ---\n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        outputs_kv_init = model(\n",
    "            **inputs_t,\n",
    "            use_cache=True,\n",
    "            past_key_values=past_key_values,\n",
    "        )\n",
    "        # Store the logit for the *last* token of the prompt\n",
    "        logits2.append(outputs_kv_init.logits[0, -1, :])\n",
    "        # Store the KV cache from initial pass\n",
    "        past_key_values = outputs_kv_init.past_key_values\n",
    "        # Update the sequence length tracker\n",
    "        current_seq_len = len(inputs_t['input_ids'][0]) # Assuming batch size of 1\n",
    "\n",
    "    # --- Iterate through the continuation tokens ---\n",
    "    num_continuation_tokens = end_idx - start_idx - 1\n",
    "    for i in range(num_continuation_tokens):\n",
    "        # Prepare input for the next iteration: only the current continuation token\n",
    "        # Note: Original code uses inputs_f. Ensure this aligns with your logic.\n",
    "        # Usually, in generation, the input is the *predicted* token from the previous step.\n",
    "        # Assuming inputs_f contains ground truth continuation here for analysis.\n",
    "        current_token_id_tensor = inputs_f['input_ids'][:, start_idx + i + 1].unsqueeze(1) # Shape: (batch_size, 1)\n",
    "\n",
    "        # The position for this single token is the current total sequence length\n",
    "        # Needs to have shape (batch_size, 1)\n",
    "        current_cache_position = torch.tensor([current_seq_len], device=current_token_id_tensor.device)\n",
    "\n",
    "        # Model call with the single next token and past_key_values\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            outputs_kv_step = model(\n",
    "                input_ids=current_token_id_tensor,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True,\n",
    "                cache_position=current_cache_position # ADDED\n",
    "            )\n",
    "            # Store the logits for the token we just processed\n",
    "            # Logits shape is (batch_size, 1, vocab_size), so index [0, 0, :] or [0, -1, :]\n",
    "            logits2.append(outputs_kv_step.logits[0, -1, :]) # Index 0 as seq_len is 1\n",
    "            # Update KV cache with new values\n",
    "            past_key_values = outputs_kv_step.past_key_values\n",
    "            # Increment the sequence length tracker\n",
    "            current_seq_len += 1\n",
    "\n",
    "# Now logits2 contains the logits for the last token of the prompt\n",
    "# and each of the subsequent continuation tokens processed one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5673cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits2 = torch.stack(logits2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1191f393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1006, 0.0381, 0.0693, 0.0620, 0.0530, 0.0500, 0.0598, 0.0864, 0.0889,\n",
       "        0.0864, 0.0437, 0.0623, 0.0737, 0.0732, 0.0864, 0.0806, 0.0786, 0.0552,\n",
       "        0.0598, 0.0623, 0.1104, 0.0933, 0.0669, 0.0869, 0.1016, 0.0771, 0.0757,\n",
       "        0.0583, 0.0273, 0.0334, 0.0547, 0.0376, 0.0371, 0.0442, 0.0422, 0.0354,\n",
       "        0.0361, 0.0403, 0.0574, 0.0254, 0.0201, 0.0378, 0.0542, 0.0305, 0.0278,\n",
       "        0.0869, 0.0444, 0.0574, 0.0432, 0.0574, 0.1025, 0.0840, 0.1011, 0.0879,\n",
       "        0.0275, 0.0630, 0.0854, 0.0718, 0.0835, 0.0430, 0.0391, 0.0269, 0.0250,\n",
       "        0.0226, 0.0625, 0.1055, 0.0439, 0.1719, 0.0693, 0.0908, 0.0454, 0.0278,\n",
       "        0.0732, 0.0952, 0.0559, 0.0583, 0.0398, 0.0718, 0.0625, 0.0649, 0.0854,\n",
       "        0.1143, 0.0515, 0.0625, 0.0281, 0.0530, 0.0508, 0.0825, 0.1108, 0.1455,\n",
       "        0.1553, 0.0864, 0.0280, 0.0317, 0.0527, 0.0613, 0.0547, 0.0381, 0.0530,\n",
       "        0.0601, 0.0737, 0.0498, 0.0430, 0.0352, 0.0281, 0.0244, 0.0359, 0.0284,\n",
       "        0.0435, 0.0391, 0.0703, 0.0801, 0.0684, 0.0364, 0.0286, 0.0327, 0.0422,\n",
       "        0.0405, 0.0967, 0.0520, 0.0447, 0.0237, 0.0388, 0.0234, 0.0613, 0.0728,\n",
       "        0.0752, 0.0403, 0.0640, 0.0850, 0.0369, 0.0265, 0.0603, 0.0366, 0.0261,\n",
       "        0.0791, 0.0854, 0.0549, 0.0383, 0.0464, 0.0337, 0.0471, 0.0535, 0.0435,\n",
       "        0.0221, 0.0413, 0.0437, 0.0203, 0.0781, 0.0454, 0.0369, 0.0265, 0.0491,\n",
       "        0.0378, 0.0796, 0.0635, 0.0498, 0.0359, 0.0452, 0.0493, 0.0393, 0.0732,\n",
       "        0.0535, 0.0427, 0.0864, 0.0908, 0.0269, 0.0236, 0.0464, 0.0330, 0.0586,\n",
       "        0.0752, 0.0422, 0.0540, 0.0503, 0.0474, 0.0645, 0.0618, 0.0659, 0.0840,\n",
       "        0.0378, 0.0518, 0.0413, 0.0437, 0.0544, 0.0537, 0.0398, 0.0649, 0.0266,\n",
       "        0.0688, 0.0405, 0.0332, 0.0593, 0.0413, 0.0737, 0.0349, 0.0444, 0.0562,\n",
       "        0.0454, 0.0767, 0.0806, 0.1084, 0.1147, 0.0718, 0.0771, 0.0635, 0.1069,\n",
       "        0.1055, 0.0781, 0.0275, 0.0879, 0.0371, 0.0728, 0.0544, 0.1182, 0.0840,\n",
       "        0.0422, 0.0776, 0.0378, 0.0525, 0.0334, 0.0549, 0.0221, 0.0459, 0.0713,\n",
       "        0.0356, 0.0284, 0.0259, 0.0466, 0.0405, 0.0532, 0.0781, 0.0430, 0.0635,\n",
       "        0.0269, 0.0654, 0.0432, 0.0825, 0.0510, 0.0415, 0.0713, 0.0334, 0.0535,\n",
       "        0.2676, 0.1309, 0.1123, 0.0444, 0.0386, 0.0620, 0.0674, 0.0820, 0.0864,\n",
       "        0.0273, 0.0615, 0.0386, 0.0542, 0.0386, 0.0378, 0.0518, 0.0474, 0.0898,\n",
       "        0.0820, 0.0447, 0.0684, 0.0327, 0.0371, 0.0337, 0.0518, 0.0947, 0.0513,\n",
       "        0.0479, 0.0908, 0.0552, 0.0366, 0.0454, 0.0388, 0.0635, 0.1055, 0.0530,\n",
       "        0.0498, 0.0564, 0.0535, 0.0889, 0.0840, 0.0698, 0.0913, 0.0342, 0.0294,\n",
       "        0.0422, 0.0552, 0.0337, 0.0549, 0.0762, 0.0791, 0.0500, 0.0635, 0.0591,\n",
       "        0.0525, 0.0830, 0.0464, 0.0405, 0.0493, 0.0310, 0.0488, 0.0654, 0.0347,\n",
       "        0.0378, 0.0630, 0.0579, 0.0405, 0.0542, 0.0620, 0.0535, 0.0767, 0.0737,\n",
       "        0.0284, 0.0640, 0.0287, 0.0371, 0.0635, 0.0601, 0.0425, 0.0410, 0.0520,\n",
       "        0.0284, 0.0308, 0.0444, 0.0674, 0.0405, 0.0498, 0.0294, 0.0405, 0.0625,\n",
       "        0.0540, 0.0703, 0.0532, 0.0781, 0.0972, 0.0312, 0.0952, 0.0845, 0.0562,\n",
       "        0.0461, 0.0425, 0.0503, 0.0454, 0.0405, 0.0425, 0.1025, 0.0454, 0.0525,\n",
       "        0.0381, 0.0493, 0.0659, 0.0327, 0.0447, 0.0527, 0.0903, 0.0952, 0.0952,\n",
       "        0.0508, 0.0322, 0.0537, 0.0496, 0.0481, 0.0923, 0.0525, 0.0811, 0.0369,\n",
       "        0.0518, 0.0312, 0.0322, 0.0452, 0.0601, 0.0962, 0.0830, 0.0471, 0.0271,\n",
       "        0.1016, 0.0206, 0.0737, 0.0664, 0.0491, 0.0579, 0.0562, 0.0422, 0.0801,\n",
       "        0.0962, 0.0991, 0.0996, 0.0835, 0.0522, 0.0723, 0.0747, 0.0581, 0.0508,\n",
       "        0.0442, 0.0383, 0.0554, 0.0566, 0.0284, 0.0452, 0.0845, 0.0669, 0.0610,\n",
       "        0.0457, 0.0718, 0.0459, 0.0903, 0.0586, 0.0703, 0.1377, 0.0762, 0.0869,\n",
       "        0.0603, 0.0649, 0.0669, 0.0854, 0.0850, 0.0557, 0.0786, 0.0444, 0.0640,\n",
       "        0.0369, 0.1406, 0.1250, 0.0991, 0.0918, 0.0317, 0.0388, 0.0781, 0.0596,\n",
       "        0.0850, 0.0515, 0.0688, 0.0603, 0.0791, 0.0723, 0.0747, 0.0586, 0.0669,\n",
       "        0.0981, 0.0649, 0.0752, 0.0260, 0.0408, 0.0281, 0.0688, 0.0645, 0.0593,\n",
       "        0.0623, 0.0630, 0.2148, 0.0586, 0.0535, 0.0723, 0.0459, 0.0747, 0.0437,\n",
       "        0.0347, 0.0452, 0.0289, 0.0713, 0.0415, 0.0605, 0.0425, 0.0172, 0.0574,\n",
       "        0.0503, 0.0364, 0.0649, 0.0684, 0.0608, 0.0537, 0.0542, 0.0664, 0.1396,\n",
       "        0.0645, 0.0398, 0.0574, 0.0566, 0.0356, 0.0364, 0.0442, 0.0625, 0.0500,\n",
       "        0.0596, 0.0767, 0.0586, 0.0510, 0.0496, 0.0542, 0.0525, 0.0479, 0.0742,\n",
       "        0.0349, 0.0564, 0.0288, 0.0771, 0.0598, 0.0452, 0.0542, 0.0610, 0.0623,\n",
       "        0.0178, 0.0552, 0.0879, 0.0830, 0.0625, 0.0408, 0.0342, 0.0459, 0.0718,\n",
       "        0.1377, 0.0581, 0.0386, 0.0427, 0.0801, 0.0952, 0.0427, 0.0723, 0.0859,\n",
       "        0.0728, 0.0221, 0.0630, 0.0457, 0.0393, 0.0542, 0.0186, 0.0459, 0.0420,\n",
       "        0.0664, 0.0608, 0.0825, 0.0500, 0.0366, 0.0410, 0.0403, 0.0430, 0.0659,\n",
       "        0.0698, 0.0474, 0.1104, 0.1074, 0.0549, 0.0649, 0.0270, 0.0713, 0.0767,\n",
       "        0.0459, 0.0708, 0.0498, 0.0559, 0.0486, 0.1011, 0.0427, 0.0417, 0.0503,\n",
       "        0.0859, 0.0796, 0.0498, 0.0732, 0.1216, 0.0679, 0.0640, 0.0535, 0.0767,\n",
       "        0.0581, 0.1260, 0.0583, 0.0654, 0.0557, 0.3066, 0.0913, 0.0747, 0.0718,\n",
       "        0.0347, 0.0664, 0.0294, 0.1113, 0.0315, 0.0476, 0.0781, 0.0415, 0.0591,\n",
       "        0.0757, 0.0483, 0.0405, 0.0591, 0.0669, 0.1211, 0.0840, 0.0693, 0.1543,\n",
       "        0.0850, 0.0535, 0.0820, 0.0540, 0.0742, 0.0542, 0.0209, 0.0933, 0.1011,\n",
       "        0.0635, 0.0874, 0.1543, 0.0386, 0.0771, 0.0732, 0.0294, 0.0503, 0.0464,\n",
       "        0.0388, 0.0544, 0.0918, 0.0588, 0.0520, 0.0217, 0.0674, 0.0669, 0.1357,\n",
       "        0.0771, 0.0256, 0.0825, 0.0400, 0.0354, 0.0505, 0.0698, 0.0693, 0.0625,\n",
       "        0.0420, 0.0608, 0.1357, 0.1216, 0.0532, 0.0698, 0.0403, 0.0991, 0.0347,\n",
       "        0.0498, 0.0576, 0.0723, 0.0981, 0.0796, 0.0767, 0.0349, 0.0325, 0.0491,\n",
       "        0.0747, 0.0674, 0.0437, 0.0562, 0.1006, 0.0825, 0.0610, 0.0312, 0.0796,\n",
       "        0.0413, 0.0723, 0.0830, 0.0608, 0.0427, 0.1895, 0.0245, 0.0649, 0.0442,\n",
       "        0.0249, 0.0427, 0.0366, 0.0767, 0.0430, 0.0854, 0.0598, 0.0547, 0.1079,\n",
       "        0.0518, 0.0679, 0.0393, 0.0693, 0.0581, 0.0918, 0.1260, 0.0742, 0.0752,\n",
       "        0.0620, 0.0327, 0.1094, 0.0698, 0.0723, 0.0610, 0.0757, 0.0435, 0.2070,\n",
       "        0.0659, 0.0391, 0.0815, 0.0454, 0.0884, 0.0530, 0.2080, 0.1064, 0.1069,\n",
       "        0.0991, 0.0598, 0.0928, 0.0640, 0.0471, 0.0408, 0.0522, 0.0503, 0.0464,\n",
       "        0.0869, 0.0684, 0.0889, 0.0571, 0.0349, 0.0444, 0.0381, 0.0625, 0.0840,\n",
       "        0.0894, 0.0605, 0.0767, 1.3281, 0.3438, 0.3008, 0.2031, 0.2754, 0.2148,\n",
       "        0.3164, 0.2070, 0.1943, 0.1514, 0.1914, 0.2305, 0.1836, 0.1367, 0.1338,\n",
       "        0.1572, 0.1143, 0.2246, 0.1543, 0.1128, 0.1670, 0.1455, 0.0913, 0.1592,\n",
       "        0.1855, 0.0776, 0.0583, 0.1006, 0.0918, 0.2139, 0.1299, 0.1167, 0.0718,\n",
       "        0.0796, 0.1621, 0.1069, 0.1963, 0.0596, 0.0928, 0.1235, 0.1104, 0.0742,\n",
       "        0.1377, 0.0781, 0.1123, 0.1133, 0.0728, 0.0767, 0.0947, 0.0703, 0.1094,\n",
       "        0.0566, 0.0542, 0.0581, 0.0908, 0.0815, 0.0737, 0.1040, 0.1260, 0.3457,\n",
       "        0.0928, 0.1318, 0.1641, 0.1182, 0.2090, 0.2383, 0.2656, 0.3164, 0.2500,\n",
       "        0.1992, 0.1660, 0.2197, 0.3555, 0.2812, 0.2275, 0.1650, 0.1709, 0.2012,\n",
       "        0.1885, 0.1924, 0.1963, 0.1543, 0.1963, 0.1445, 0.1680, 0.1689, 0.2393,\n",
       "        0.2021, 0.0815, 0.1338, 0.1245, 0.1367, 0.1216, 0.1128, 0.1260, 0.1914,\n",
       "        0.1465, 0.1455, 0.1089, 0.1035, 0.0942, 0.1611, 0.1455, 0.1494, 0.0933,\n",
       "        0.0957, 0.0796, 0.1084, 0.1719, 0.0830, 0.0801, 0.1016, 0.1152, 0.0894,\n",
       "        0.1250, 0.1631, 0.2520, 0.1836, 0.2031, 0.1934, 0.2041, 0.0889, 0.0845,\n",
       "        0.1299, 0.1309, 0.0854, 0.0791, 0.1108, 0.0820, 0.1182, 0.0815, 0.0771,\n",
       "        0.0762, 0.0801, 0.0830, 0.0776, 0.0884, 0.0850, 0.0957, 0.0825, 0.0669,\n",
       "        0.0737, 0.0640, 0.1089, 0.0830, 0.0879], device='cuda:0',\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.abs(logits1 - logits2),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd94d21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1f5f0d0",
   "metadata": {},
   "source": [
    "Text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52cbb74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma3ForConditionalGeneration(\n",
       "      (vision_tower): SiglipVisionModel(\n",
       "        (vision_model): SiglipVisionTransformer(\n",
       "          (embeddings): SiglipVisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "            (position_embedding): Embedding(4096, 1152)\n",
       "          )\n",
       "          (encoder): SiglipEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-26): 27 x SiglipEncoderLayer(\n",
       "                (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "                (self_attn): SiglipAttention(\n",
       "                  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): SiglipMLP(\n",
       "                  (activation_fn): PytorchGELUTanh()\n",
       "                  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (multi_modal_projector): Gemma3MultiModalProjector(\n",
       "        (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "      )\n",
       "      (language_model): Gemma3ForCausalLM(\n",
       "        (model): Gemma3TextModel(\n",
       "          (embed_tokens): Gemma3TextScaledWordEmbedding(24, 2560, padding_idx=0)\n",
       "          (layers): ModuleList(\n",
       "            (0): Gemma3DecoderLayer(\n",
       "              (self_attn): Gemma3Attention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2048, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              )\n",
       "              (mlp): Gemma3MLP(\n",
       "                (gate_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=10240, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            )\n",
       "            (1): Gemma3DecoderLayer(\n",
       "              (self_attn): Gemma3Attention(\n",
       "                (q_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              )\n",
       "              (mlp): Gemma3MLP(\n",
       "                (gate_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=10240, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            )\n",
       "            (2): Gemma3DecoderLayer(\n",
       "              (self_attn): Gemma3Attention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2048, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              )\n",
       "              (mlp): Gemma3MLP(\n",
       "                (gate_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=10240, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            )\n",
       "            (3): Gemma3DecoderLayer(\n",
       "              (self_attn): Gemma3Attention(\n",
       "                (q_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              )\n",
       "              (mlp): Gemma3MLP(\n",
       "                (gate_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=10240, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            )\n",
       "            (4): Gemma3DecoderLayer(\n",
       "              (self_attn): Gemma3Attention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2048, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              )\n",
       "              (mlp): Gemma3MLP(\n",
       "                (gate_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=10240, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            )\n",
       "            (5): Gemma3DecoderLayer(\n",
       "              (self_attn): Gemma3Attention(\n",
       "                (q_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              )\n",
       "              (mlp): Gemma3MLP(\n",
       "                (gate_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=10240, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            )\n",
       "            (6-33): 28 x Gemma3DecoderLayer(\n",
       "              (self_attn): Gemma3Attention(\n",
       "                (q_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              )\n",
       "              (mlp): Gemma3MLP(\n",
       "                (gate_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=10240, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            )\n",
       "          )\n",
       "          (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (rotary_emb): Gemma3RotaryEmbedding()\n",
       "          (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "        )\n",
       "        (lm_head): Linear(in_features=2560, out_features=24, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e05381d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.base_model.language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51b50b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "with torch.no_grad():\n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        outputs_full = model(inputs_f['input_ids'])\n",
    "        logits1 = outputs_full.logits[0, start_idx: end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf28be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits2 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Initial pass with the input prompt\n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        outputs_kv_init = model(inputs_t['input_ids'], use_cache=True)\n",
    "        logits2.append(outputs_kv_init.logits[0, -1, :])\n",
    "        # Store the KV cache from initial pass\n",
    "        past_key_values = outputs_kv_init.past_key_values\n",
    "\n",
    "    # Iterate through the continuation tokens\n",
    "    for i in range(end_idx - start_idx - 1):\n",
    "        # Prepare input for the next iteration: only the current continuation token\n",
    "        current_token_id_tensor = inputs_f['input_ids'][:, start_idx + i + 1].unsqueeze(1)\n",
    "\n",
    "        # Model call with the single next token and past_key_values\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            outputs_kv_step = model(input_ids=current_token_id_tensor, \n",
    "                                   past_key_values=past_key_values, \n",
    "                                   use_cache=True)\n",
    "            # Store the logits from this step - CORRECTION HERE\n",
    "            logits2.append(outputs_kv_step.logits[0, -1, :])\n",
    "            # Update KV cache with new values\n",
    "            past_key_values = outputs_kv_step.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d73ed24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits2 = torch.stack(logits2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "412df5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([869, 24]), torch.Size([869, 24]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits2.shape, logits1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75b79b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0625, 2.7812, 3.1406, 3.1094, 3.0781, 3.0625, 3.1250, 3.1719, 3.2812,\n",
       "        3.0625, 2.7969, 2.6406, 2.5625, 2.5312, 2.2812, 2.1250, 2.0625, 2.1875,\n",
       "        2.3906, 2.4062, 2.4062, 2.4062, 2.2969, 2.1406, 2.1094, 2.0625, 1.8359,\n",
       "        1.6562, 1.6641, 1.6641, 1.9219, 3.0625, 3.6094, 4.5312, 4.8438, 4.9375,\n",
       "        4.7188, 4.3438, 2.4688, 4.1562, 3.9062, 3.6406, 2.7656, 2.5312, 2.3125,\n",
       "        2.2344, 1.9766, 1.8984, 1.8594, 1.9219, 2.0938, 1.8828, 1.7422, 1.8906,\n",
       "        1.9766, 1.9531, 1.9688, 1.8828, 1.5859, 1.2109, 1.4844, 1.9062, 2.3906,\n",
       "        2.2812, 2.2656, 2.2656, 2.2812, 2.1562, 2.1094, 2.4531, 2.0000, 1.8438,\n",
       "        1.8594, 1.9922, 1.8984, 1.9531, 1.9531, 1.8047, 1.6797, 1.6562, 1.6016,\n",
       "        1.5547, 1.7578, 1.6641, 1.7500, 1.6094, 1.5156, 1.5625, 1.4844, 1.5078,\n",
       "        1.7031, 2.1250, 2.2656, 2.5625, 2.2188, 1.7969, 1.8125, 1.8750, 2.2031,\n",
       "        2.4062, 2.3125, 2.2031, 2.5781, 2.3125, 2.1094, 2.3438, 2.1094, 2.0781,\n",
       "        1.8438, 1.7109, 1.9453, 1.6953, 1.7891, 1.5859, 1.7656, 1.6406, 1.7031,\n",
       "        1.5391, 1.6484, 1.6406, 1.6094, 2.1250, 2.0000, 2.1250, 2.0781, 2.1094,\n",
       "        2.1562, 2.1562, 2.2812, 2.5469, 2.3125, 2.5000, 2.0000, 2.1250, 2.0625,\n",
       "        2.3438, 2.0156, 2.1875, 2.0469, 2.2812, 1.8750, 1.8906, 1.7188, 1.7734,\n",
       "        1.6172, 1.6719, 1.5391, 1.5078, 1.7578, 1.7812, 1.7656, 2.1719, 1.8438,\n",
       "        2.0625, 2.0625, 2.0469, 1.9688, 1.8516, 2.0938, 1.9766, 1.9219, 1.9062,\n",
       "        2.0156, 2.0000, 1.8906, 1.9531, 2.0000, 2.0781, 1.9141, 1.9219, 1.5781,\n",
       "        1.6016, 1.7578, 1.8984, 1.5391, 1.5859, 1.8984, 1.7734, 1.5000, 1.6172,\n",
       "        1.6641, 1.9531, 2.0781, 2.4688, 2.7031, 2.2344, 2.4062, 2.5156, 2.5781,\n",
       "        2.4844, 2.4219, 2.3750, 1.9766, 2.3594, 2.3125, 2.6250, 2.0938, 2.3281,\n",
       "        2.4062, 2.2500, 2.3438, 1.6875, 1.8672, 1.7500, 1.7734, 1.5938, 1.7812,\n",
       "        1.7266, 1.5703, 1.8281, 1.6406, 2.1719, 2.1719, 2.6250, 2.2812, 2.2344,\n",
       "        2.4531, 2.5312, 2.4375, 2.3594, 2.3906, 2.3594, 2.4531, 2.3750, 2.2500,\n",
       "        2.5625, 2.0000, 2.0312, 2.2969, 2.2656, 1.9219, 1.7422, 2.0312, 1.9141,\n",
       "        1.6797, 1.9844, 2.2969, 1.8359, 1.4922, 1.6328, 1.7500, 2.1562, 2.1406,\n",
       "        2.4062, 2.2969, 2.2969, 2.3438, 2.4219, 2.5781, 2.2188, 2.0938, 2.3750,\n",
       "        2.1875, 2.4375, 2.2188, 2.3594, 1.9766, 2.0781, 2.2031, 2.3438, 2.0781,\n",
       "        1.8438, 2.0312, 1.8750, 1.6250, 2.1094, 2.3125, 2.1406, 1.7188, 1.8984,\n",
       "        1.7266, 2.1562, 2.1250, 2.5781, 2.6719, 2.3281, 2.4531, 2.7031, 2.6250,\n",
       "        2.5156, 2.5938, 2.5312, 3.1094, 3.0469, 2.6875, 2.8594, 2.1875, 2.5000,\n",
       "        2.6094, 2.5938, 2.1875, 2.1094, 2.2031, 2.1719, 1.7734, 2.1562, 2.6719,\n",
       "        2.4375, 1.9219, 1.6797, 1.8438, 2.1406, 2.2188, 2.7344, 2.3281, 2.4688,\n",
       "        2.5000, 2.8594, 2.6562, 2.4844, 2.5312, 2.9688, 2.2656, 2.7812, 2.5625,\n",
       "        2.6875, 2.1406, 2.1562, 2.7500, 2.5312, 2.5000, 1.9375, 2.1094, 1.9609,\n",
       "        2.0469, 1.9297, 2.0625, 2.0156, 1.4922, 2.1875, 1.6094, 2.0469, 2.0938,\n",
       "        2.3594, 2.1562, 2.1250, 2.2188, 2.4219, 2.4844, 2.2969, 2.6250, 2.6406,\n",
       "        2.4688, 2.2188, 2.5781, 2.4844, 2.1406, 2.2344, 2.5781, 2.6719, 2.2031,\n",
       "        2.2031, 2.1406, 2.2188, 1.8984, 2.5156, 2.8594, 2.4844, 1.8047, 1.7344,\n",
       "        2.0781, 2.2812, 2.4375, 2.4844, 2.5625, 2.4531, 2.5938, 2.7812, 2.6875,\n",
       "        2.4062, 2.6094, 2.8438, 2.1875, 2.7344, 2.6094, 2.6875, 2.0469, 2.4688,\n",
       "        2.4219, 2.7344, 2.3438, 2.0625, 2.1406, 2.0781, 1.8125, 2.5781, 2.5312,\n",
       "        2.4531, 2.0312, 2.1719, 1.6875, 2.0781, 2.2656, 2.4844, 2.2500, 2.4062,\n",
       "        2.5156, 2.7344, 2.6719, 2.3281, 2.5781, 2.4062, 2.7969, 2.8906, 2.5000,\n",
       "        2.7344, 2.1250, 2.3125, 2.6562, 2.6562, 2.2031, 2.0156, 2.1094, 2.1719,\n",
       "        1.7344, 2.2188, 2.6406, 2.4375, 1.9375, 1.8516, 1.7266, 1.8828, 2.1250,\n",
       "        2.2344, 2.0469, 1.9766, 2.1875, 2.4219, 2.4219, 2.1719, 2.1875, 2.5625,\n",
       "        2.1875, 2.4062, 2.1562, 2.3438, 1.9141, 1.8516, 2.2812, 2.2812, 2.1250,\n",
       "        1.7656, 1.9375, 1.7812, 1.7969, 1.7578, 1.8984, 1.8281, 1.4609, 2.5156,\n",
       "        1.5391, 1.8438, 2.0781, 2.2188, 2.1875, 2.0781, 2.2656, 2.4375, 2.3594,\n",
       "        2.1562, 2.3438, 2.1562, 2.8281, 2.9844, 2.4219, 2.5000, 1.9453, 2.0312,\n",
       "        2.3906, 2.5156, 2.2344, 1.9609, 2.1406, 2.0938, 1.9062, 2.4062, 2.6719,\n",
       "        2.4062, 1.7656, 1.9375, 1.6719, 1.8672, 2.0625, 2.2969, 2.0469, 2.2188,\n",
       "        2.4219, 2.5938, 2.4531, 2.1406, 2.3281, 2.7344, 2.1250, 2.8594, 2.4531,\n",
       "        2.5156, 1.9531, 2.3438, 2.3438, 2.5938, 2.1250, 1.9375, 2.1562, 1.9219,\n",
       "        1.8203, 2.4062, 2.3750, 2.3594, 1.9219, 2.3906, 1.5391, 1.8359, 1.9609,\n",
       "        2.2344, 1.9688, 1.9062, 1.9922, 2.2656, 2.3750, 2.1719, 2.4531, 2.4688,\n",
       "        2.2812, 2.2188, 2.5000, 2.5000, 2.0469, 2.4844, 2.5312, 2.4531, 1.9453,\n",
       "        1.9531, 2.2188, 2.1875, 1.7812, 2.3125, 2.7188, 2.3906, 1.6484, 2.1719,\n",
       "        1.5625, 1.9453, 2.0625, 2.2344, 2.4062, 2.1719, 2.3906, 2.4375, 2.4688,\n",
       "        2.0938, 2.3594, 2.7656, 2.0781, 2.6875, 2.4219, 2.5000, 1.8672, 1.8906,\n",
       "        2.4375, 2.3594, 2.4531, 1.8672, 2.0938, 1.8906, 1.8906, 1.8828, 2.0312,\n",
       "        2.0781, 1.5625, 2.5156, 1.4375, 1.6641, 1.9766, 2.2969, 1.9531, 2.0312,\n",
       "        2.2344, 2.3906, 2.3750, 2.1875, 2.4219, 2.2969, 2.7188, 2.8594, 2.3750,\n",
       "        2.4219, 1.9062, 1.9453, 2.3750, 2.4219, 2.2500, 1.9062, 2.1094, 2.0469,\n",
       "        1.9375, 2.4375, 2.6406, 2.3906, 1.8594, 2.2188, 1.5156, 1.7656, 1.8750,\n",
       "        2.0156, 1.8516, 1.7812, 1.9062, 2.1250, 2.2812, 2.1094, 2.0469, 2.3281,\n",
       "        2.1562, 2.3125, 2.1562, 2.2656, 1.8672, 2.1250, 1.9766, 2.1875, 2.0469,\n",
       "        1.7891, 1.9766, 1.8359, 1.7266, 2.4375, 2.3906, 2.2500, 1.8516, 2.6875,\n",
       "        1.5625, 1.8125, 1.9922, 2.1250, 2.1406, 1.9453, 2.2344, 2.3906, 2.3750,\n",
       "        2.1250, 2.3125, 2.0938, 2.7500, 2.9844, 2.4688, 2.5781, 1.9375, 2.1719,\n",
       "        2.5000, 2.5469, 2.1719, 1.9609, 2.2812, 2.2031, 1.8750, 2.2812, 2.5625,\n",
       "        2.4531, 1.8828, 1.8438, 1.5312, 1.7812, 2.0312, 2.2031, 2.0625, 1.9844,\n",
       "        2.3438, 2.4844, 2.5156, 2.1719, 2.3750, 2.8594, 2.1562, 2.9375, 2.5312,\n",
       "        2.5312, 1.9219, 1.8281, 2.5625, 2.5312, 2.3750, 1.8594, 2.1875, 1.9141,\n",
       "        1.9688, 1.8984, 1.9609, 2.0469, 1.4219, 2.7656, 1.5078, 1.6562, 1.8516,\n",
       "        2.0469, 1.9062, 1.8281, 1.8672, 2.0625, 2.2812, 2.0781, 2.3906, 2.3906,\n",
       "        2.2031, 2.2031, 2.3906, 2.3906, 1.8672, 1.8359, 2.4844, 2.6094, 2.0781,\n",
       "        2.0312, 2.2656, 2.1719, 2.0469, 2.6094, 2.7188, 2.3906, 1.7266, 2.6094,\n",
       "        1.5469, 1.7422, 1.9375, 2.0469, 2.2500, 2.0938, 2.2812, 2.4844, 2.4844,\n",
       "        2.1250, 2.4062, 2.8125, 2.1094, 2.7969, 2.4688, 2.5000, 1.8750, 2.2344,\n",
       "        2.2188, 2.4375, 2.3281, 1.9062, 2.1562, 1.8984, 1.7891, 2.5625, 2.4062,\n",
       "        2.3906, 1.8125, 2.7969, 1.5391, 1.8203, 1.9844, 2.1406, 1.9453, 1.9844,\n",
       "        2.4688, 2.5312, 2.6406, 2.2500, 2.6094, 2.5469, 2.9375, 3.0312, 2.5625,\n",
       "        2.7656, 2.0625, 2.3125, 2.6406, 2.6719, 2.2344, 2.0000, 2.4062, 2.2031,\n",
       "        2.0312, 2.3750, 2.6875, 2.3906, 1.8281, 2.3438, 1.5312, 1.7500, 1.8828,\n",
       "        1.9375, 1.9141, 1.7734, 1.9531, 2.1875, 2.3906, 2.1406, 2.2031, 2.4219,\n",
       "        2.0625, 2.4062, 2.1094, 2.3438, 1.7578, 1.6719, 2.3750, 2.2500, 2.3125,\n",
       "        1.8047, 2.0000, 1.7891, 1.8125, 1.8203, 1.9062, 1.8047, 1.3281, 3.0469,\n",
       "        1.2031, 1.5312, 1.7031, 1.7422, 2.2188, 2.2500, 2.0781, 1.9766, 2.1875,\n",
       "        2.6250, 2.6406, 2.5625, 2.5781, 2.5781, 2.5156, 2.5156, 2.1094, 2.0000,\n",
       "        2.5000, 2.5156, 2.3750, 1.8359, 1.9766, 2.0625, 2.1875, 2.2500, 2.2656,\n",
       "        2.0469, 1.5938, 1.8047, 1.2031, 1.7266, 1.6406, 1.5234, 1.4844, 1.4375,\n",
       "        1.4688, 1.5547, 1.5312, 1.3203, 1.2109, 1.1875, 1.2578, 1.2266, 1.1172,\n",
       "        1.0469, 1.0781, 1.2031, 1.4219, 1.4375, 1.3047, 1.0859, 1.2422, 1.1172,\n",
       "        1.3125, 1.4844, 1.3281, 1.2422, 1.8984], device='cuda:0',\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.abs(logits1 - logits2),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5ad532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3451e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609d3c68c8084415b615279d7ddd0e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    \"google/gemma-3-4b-it\",\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    # device_map=\"auto\",\n",
    "    # attn_implementation=\"sdpa\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aebb28f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randint(0, 1000, (1, 256))#.to(\"cuda\")\n",
    "start_idx = 128\n",
    "end_idx = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35b69d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "with torch.no_grad():\n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        outputs_full = model(inputs)\n",
    "        logits1 = outputs_full.logits[0, start_idx: end_idx + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05f92f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import HybridCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9ab19ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_key_values = HybridCache(model.config.text_config, 1, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd8c695",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 256 is out of bounds for dimension 1 with size 256",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Iterate through the continuation tokens\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(end_idx \u001b[38;5;241m-\u001b[39m start_idx):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Prepare input for the next iteration: only the current continuation token\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     current_token_id_tensor \u001b[38;5;241m=\u001b[39m inputs[:, start_idx \u001b[38;5;241m+\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Model call with the single next token and past_key_values\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16):\n",
      "\u001b[0;31mIndexError\u001b[0m: index 256 is out of bounds for dimension 1 with size 256"
     ]
    }
   ],
   "source": [
    "logits2 = []\n",
    "with torch.no_grad():\n",
    "    # Initial pass with the input prompt\n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        outputs_kv_init = model(inputs[:,:start_idx + 1], use_cache=True, past_key_values=past_key_values)\n",
    "        logits2.append(outputs_kv_init.logits[0, -1, :])\n",
    "        # Store the KV cache from initial pass\n",
    "        past_key_values = outputs_kv_init.past_key_values\n",
    "\n",
    "    # Iterate through the continuation tokens\n",
    "    for i in range(end_idx - start_idx - 1):\n",
    "        # Prepare input for the next iteration: only the current continuation token\n",
    "        current_token_id_tensor = inputs[:, start_idx + i + 1].unsqueeze(1)\n",
    "\n",
    "        # Model call with the single next token and past_key_values\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            outputs_kv_step = model(input_ids=current_token_id_tensor, \n",
    "                                   past_key_values=past_key_values, \n",
    "                                   use_cache=True)\n",
    "            # Store the logits from this step - CORRECTION HERE\n",
    "            logits2.append(outputs_kv_step.logits[0, -1, :])\n",
    "            # Update KV cache with new values\n",
    "            past_key_values = outputs_kv_step.past_key_values\n",
    "logits2 = torch.stack(logits2, dim=0)\n",
    "logits2.shape, logits1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e0c2842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 262208]), torch.Size([128, 262208]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logits2 = torch.stack(logits2, dim=0)\n",
    "logits2.shape, logits1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ea6b216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.5761e-06, 7.2332e-06, 7.8308e-06, 8.9197e-06, 7.5235e-06, 7.1803e-06,\n",
       "        9.2514e-06, 1.0456e-05, 9.3818e-06, 1.2672e-05, 9.0390e-06, 8.1634e-06,\n",
       "        7.5472e-06, 1.1036e-05, 1.1434e-05, 1.0017e-05, 9.7087e-06, 1.2099e-05,\n",
       "        1.3566e-05, 1.0992e-05, 8.9328e-06, 8.0241e-06, 7.5371e-06, 8.4412e-06,\n",
       "        1.1368e-05, 7.4331e-06, 8.1296e-06, 8.6147e-06, 8.4934e-06, 1.0547e-05,\n",
       "        9.2572e-06, 7.8645e-06, 9.2196e-06, 1.1559e-05, 9.1299e-06, 1.1141e-05,\n",
       "        9.9925e-06, 1.1491e-05, 8.4543e-06, 9.1530e-06, 8.3192e-06, 8.1446e-06,\n",
       "        9.0655e-06, 1.0497e-05, 8.2049e-06, 1.0780e-05, 7.5738e-06, 8.0629e-06,\n",
       "        6.8507e-06, 1.0084e-05, 9.1416e-06, 8.0334e-06, 8.4851e-06, 1.0729e-05,\n",
       "        1.3607e-05, 7.8625e-06, 7.1217e-06, 7.0614e-06, 7.1365e-06, 9.5032e-06,\n",
       "        1.0092e-05, 7.0158e-06, 8.9976e-06, 9.3340e-06, 1.2156e-05, 8.5299e-06,\n",
       "        9.6471e-06, 1.3323e-05, 1.4118e-05, 1.1182e-05, 7.2470e-06, 9.4420e-06,\n",
       "        1.4857e-05, 1.0573e-05, 6.9856e-06, 6.7008e-06, 8.0400e-06, 6.8249e-06,\n",
       "        6.3196e-06, 6.5911e-06, 7.4116e-06, 6.7315e-06, 7.1983e-06, 6.5463e-06,\n",
       "        8.3482e-06, 8.0930e-06, 5.2618e-06, 9.3029e-06, 7.1126e-06, 8.6791e-06,\n",
       "        9.0481e-06, 8.4542e-06, 6.8355e-06, 8.4746e-06, 7.4499e-06, 8.2346e-06,\n",
       "        7.0119e-06, 6.8156e-06, 7.0467e-06, 8.4187e-06, 8.6727e-06, 9.1642e-06,\n",
       "        7.2884e-06, 7.7947e-06, 6.7549e-06, 8.4489e-06, 7.1558e-06, 7.1298e-06,\n",
       "        6.3564e-06, 4.9521e-06, 5.7325e-06, 6.7724e-06, 6.3755e-06, 5.5515e-06,\n",
       "        4.1408e-06, 1.1932e-05, 4.5188e-06, 4.8639e-06, 1.4389e-05, 1.1451e-05,\n",
       "        5.5890e-06, 4.4352e-06, 4.8694e-06, 5.0882e-06, 4.2243e-06, 4.2550e-06,\n",
       "        4.4754e-06, 4.3902e-06])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.abs(logits1 - logits2),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbb5ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
