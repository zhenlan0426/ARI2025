{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "784495eb",
   "metadata": {},
   "source": [
    "#### oneshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6088f9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-19 13:53:14 [__init__.py:239] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.3.19: Fast Gemma3 patching. Transformers: 4.50.0.dev0. vLLM: 0.8.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.642 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from functions import *\n",
    "model, tokenizer = FastModel.from_pretrained(model_name = \"unsloth/gemma-3-4b-pt-unsloth-bnb-4bit\",\n",
    "                                        max_seq_length = 8192,\n",
    "                                        load_in_4bit = True,\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "668cbf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_vocab_size = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66513064",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_tokenizer = tokenizer.tokenizer\n",
    "pretrained_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44e5b205",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding = pretrained_model.get_input_embeddings()\n",
    "pretrained_bos_id = pretrained_tokenizer.bos_token_id\n",
    "pretrained_eos_id = pretrained_tokenizer.eos_token_id\n",
    "\n",
    "# Initialize custom embedding with the same embedding dimension as pre-trained\n",
    "embedding_dim = pretrained_embedding.embedding_dim\n",
    "custom_embedding = nn.Embedding(custom_vocab_size, embedding_dim)\n",
    "with torch.no_grad():\n",
    "    custom_embedding.weight.normal_(mean=0.0, std=0.0192)\n",
    "# Set weights for special tokens\n",
    "custom_embedding.weight.data[10] = pretrained_embedding.weight.data[pretrained_bos_id]  # BOS_X\n",
    "custom_embedding.weight.data[12] = pretrained_embedding.weight.data[pretrained_bos_id]  # BOS_Y\n",
    "custom_embedding.weight.data[11] = pretrained_embedding.weight.data[pretrained_eos_id]  # EOS_X\n",
    "custom_embedding.weight.data[13] = pretrained_embedding.weight.data[pretrained_eos_id]  # EOS_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c66a6c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 30):\n",
    "    # Encode the integer directly\n",
    "    txt = str(i % 10) # Assuming single token output for each integer. only true for 1~9, hence %.\n",
    "    token_id = pretrained_tokenizer.encode(txt, add_special_tokens=False)\n",
    "    assert len(token_id) == 1, f\"Tokenization failed for {txt}: {token_id}\"\n",
    "    custom_embedding.weight.data[16 + i] = pretrained_embedding.weight.data[token_id[0]]\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "907e6751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0192, 0.0188, 0.0196, 0.0196, 0.0190, 0.0189, 0.0188, 0.0189, 0.0193,\n",
       "        0.0191, 0.0192, 0.0192, 0.0192, 0.0192, 0.0191, 0.0191, 0.0191, 0.0192,\n",
       "        0.0192, 0.0192, 0.0192, 0.0192, 0.0192, 0.0192, 0.0192, 0.0192, 0.0192,\n",
       "        0.0192, 0.0192, 0.0192, 0.0192, 0.0192, 0.0192, 0.0192, 0.0192, 0.0192,\n",
       "        0.0192, 0.0192, 0.0192, 0.0192, 0.0192, 0.0192, 0.0192, 0.0192, 0.0192,\n",
       "        0.0192, 0.0190, 0.0189], grad_fn=<StdBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_embedding.weight.std(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0aeae637",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(custom_embedding.state_dict(), '../../Model/gemma48.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d8b40f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cffae48",
   "metadata": {},
   "source": [
    "#### Causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76842e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.4.7: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.642 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c036192c090a412a84957c48a9b94906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from functions import *\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "                                        model_name = \"unsloth/gemma-3-4b-pt-unsloth-bnb-4bit\",\n",
    "                                        # model_name = \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "                                        max_seq_length = 8192,\n",
    "                                        load_in_4bit = True,\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af519957",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_vocab_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62819990",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_tokenizer = tokenizer.tokenizer\n",
    "# pretrained_tokenizer = tokenizer\n",
    "pretrained_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e3f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding = pretrained_model.get_input_embeddings()\n",
    "pretrained_bos_id = pretrained_tokenizer.bos_token_id\n",
    "pretrained_eos_id = pretrained_tokenizer.eos_token_id\n",
    "\n",
    "# Initialize custom embedding with the same embedding dimension as pre-trained\n",
    "embedding_dim = pretrained_embedding.embedding_dim\n",
    "custom_embedding = nn.Embedding(custom_vocab_size, embedding_dim)\n",
    "with torch.no_grad():\n",
    "    custom_embedding.weight.normal_(mean=0.0, std=0.0192)\n",
    "# Set weights for special tokens\n",
    "custom_embedding.weight.data[10] = pretrained_embedding.weight.data[pretrained_bos_id]  # BOS_X\n",
    "custom_embedding.weight.data[11] = pretrained_embedding.weight.data[pretrained_eos_id]  # EOS_X\n",
    "custom_embedding.weight.data[13] = pretrained_embedding.weight.data[pretrained_bos_id]  # BOS_Y\n",
    "custom_embedding.weight.data[14] = pretrained_embedding.weight.data[pretrained_eos_id]  # EOS_Y\n",
    "custom_embedding.weight.data[12] = pretrained_tokenizer.encode(\"\\n\", add_special_tokens=False)[0]  # Line break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "959aac20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0189, 0.0192, 0.0197, 0.0195, 0.0190, 0.0193, 0.0197, 0.0193, 0.0191,\n",
       "        0.0191, 0.0192, 0.0192, 0.0000, 0.0192, 0.0192, 0.0192],\n",
       "       grad_fn=<StdBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_embedding.weight.std(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaa4d8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(custom_embedding.state_dict(), '../../Model/gemma16.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f430e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343f0dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe71ff9f",
   "metadata": {},
   "source": [
    "#### VLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a634c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-24 08:44:26 [__init__.py:239] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.3.19: Fast Gemma3 patching. Transformers: 4.50.0.dev0. vLLM: 0.8.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.642 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from functions import *\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-pt-unsloth-bnb-4bit\",\n",
    "    max_seq_length = 8192, # Choose any for long context!\n",
    "    load_in_4bit = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e0bc7e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.language_model.lm_head.load_state_dict(torch.load(\"/home/zhenlan/Desktop/Projects/ARC2/Model/lm_heads_weights_VLM.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1088b216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID for ' red': 2604\n",
      "Token ID for ' blue': 3730\n",
      "Token ID for ' green': 3826\n",
      "Token ID for ' yellow': 7070\n",
      "Token ID for ' orange': 11167\n",
      "Token ID for ' purple': 16045\n",
      "Token ID for ' white': 2173\n",
      "Token ID for ' cyan': 49147\n",
      "Token ID for ' gray': 12819\n",
      "Token ID for ' brown': 8864\n",
      "Token ID for '<bos>': 2, new id: 10\n",
      "Token ID for ' input': 2744, new id: 11\n",
      "Token ID for ' output': 3938, new id: 12\n",
      "Token ID for '\\n': 107, new id: 13\n",
      "Token ID for '\\n\\n\\n': 109, new id: 14\n",
      "Token ID for '<start_of_image>': 255999, new id: 15\n",
      "Token ID for '<image_soft_token>': 262144, new id: 16\n",
      "Token ID for '<end_of_image>': 256000, new id: 17\n",
      "set up unused tokens\n"
     ]
    }
   ],
   "source": [
    "custom_vocab_size = 24\n",
    "pretrained_tokenizer = tokenizer.tokenizer\n",
    "pretrained_model = model\n",
    "pretrained_embedding = pretrained_model.get_input_embeddings()\n",
    "embedding_dim = pretrained_embedding.embedding_dim\n",
    "custom_embedding = nn.Embedding(custom_vocab_size, embedding_dim)\n",
    "colors = [\" red\", \" blue\", \" green\", \" yellow\", \" orange\", \" purple\", \" white\", \" cyan\", \" gray\", \" brown\"]\n",
    "for i, color in enumerate(colors):\n",
    "    # Encode the color directly\n",
    "    token_id = pretrained_tokenizer.encode(color, add_special_tokens=False)\n",
    "    assert len(token_id) == 1, f\"Tokenization failed for {color}: {token_id}\"\n",
    "    print(f\"Token ID for '{color}': {token_id[0]}\")\n",
    "    custom_embedding.weight.data[i] = pretrained_embedding.weight.data[token_id[0]]\n",
    "token_id = pretrained_tokenizer.encode('<bos>', add_special_tokens=False)\n",
    "custom_embedding.weight.data[10] = pretrained_embedding.weight.data[token_id[0]]\n",
    "print(f\"Token ID for '<bos>': {token_id[0]}, new id: {10}\")\n",
    "token_id = pretrained_tokenizer.encode(' input', add_special_tokens=False)\n",
    "custom_embedding.weight.data[11] = pretrained_embedding.weight.data[token_id[0]]\n",
    "print(f\"Token ID for ' input': {token_id[0]}, new id: {11}\")\n",
    "token_id = pretrained_tokenizer.encode(' output', add_special_tokens=False)\n",
    "custom_embedding.weight.data[12] = pretrained_embedding.weight.data[token_id[0]]\n",
    "print(f\"Token ID for ' output': {token_id[0]}, new id: {12}\")\n",
    "token_id = pretrained_tokenizer.encode('\\n', add_special_tokens=False)\n",
    "custom_embedding.weight.data[13] = pretrained_embedding.weight.data[token_id[0]]\n",
    "print(f\"Token ID for '\\\\n': {token_id[0]}, new id: {13}\")\n",
    "token_id = pretrained_tokenizer.encode('\\n\\n\\n', add_special_tokens=False)\n",
    "custom_embedding.weight.data[14] = pretrained_embedding.weight.data[token_id[0]]\n",
    "print(f\"Token ID for '\\\\n\\\\n\\\\n': {token_id[0]}, new id: {14}\")\n",
    "token_id = pretrained_tokenizer.encode('<start_of_image>', add_special_tokens=False)\n",
    "custom_embedding.weight.data[15] = pretrained_embedding.weight.data[token_id[0]]\n",
    "print(f\"Token ID for '<start_of_image>': {token_id[0]}, new id: {15}\")\n",
    "token_id = pretrained_tokenizer.encode('<image_soft_token>', add_special_tokens=False)\n",
    "custom_embedding.weight.data[16] = pretrained_embedding.weight.data[token_id[0]]\n",
    "print(f\"Token ID for '<image_soft_token>': {token_id[0]}, new id: {16}\")\n",
    "token_id = pretrained_tokenizer.encode('<end_of_image>', add_special_tokens=False)\n",
    "custom_embedding.weight.data[17] = pretrained_embedding.weight.data[token_id[0]]\n",
    "print(f\"Token ID for '<end_of_image>': {token_id[0]}, new id: {17}\")\n",
    "print(\"set up unused tokens\")\n",
    "custom_embedding.weight.data[18:24] = pretrained_embedding.weight.data[18:24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a976af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(custom_embedding.state_dict(), '../../Model/gemma24.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68df35d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430d75dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f4d91d2",
   "metadata": {},
   "source": [
    "#### VLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3936935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-27 15:19:54 [__init__.py:239] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.3.19: Fast Gemma3 patching. Transformers: 4.50.0.dev0. vLLM: 0.8.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.642 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from functions import *\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-pt-unsloth-bnb-4bit\",\n",
    "    max_seq_length = 8192, # Choose any for long context!\n",
    "    load_in_4bit = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d97a0e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.language_model.lm_head.load_state_dict(torch.load(\"/home/zhenlan/Desktop/Projects/ARC2/Model/lm_heads_weights_VLM.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2b45ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0173, device='cuda:0', dtype=torch.bfloat16),\n",
       " tensor(-2.6822e-05, device='cuda:0', dtype=torch.bfloat16))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.language_model.lm_head.weight.data[[2604, 3730, 3826, 7070, 11167, 16045, 2173, 49147, 12819, 8864, 2, 2744, 3938, 107, 109, 255999, 262144, 256000]].std(1).mean(), model.language_model.lm_head.weight.data[[2604, 3730, 3826, 7070, 11167, 16045, 2173, 49147, 12819, 8864, 2, 2744, 3938, 107, 109, 255999, 262144, 256000]].mean(1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cf0e53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID for ' red': 2604\n",
      "Token ID for ' blue': 3730\n",
      "Token ID for ' green': 3826\n",
      "Token ID for ' yellow': 7070\n",
      "Token ID for ' orange': 11167\n",
      "Token ID for ' purple': 16045\n",
      "Token ID for ' white': 2173\n",
      "Token ID for ' cyan': 49147\n",
      "Token ID for ' gray': 12819\n",
      "Token ID for ' brown': 8864\n",
      "Token ID for '<bos>': 2, new id: 10\n",
      "Token ID for ' input': 2744, new id: 11\n",
      "Token ID for ' output': 3938, new id: 12\n",
      "Token ID for '\\n': 107, new id: 13\n",
      "Token ID for '\\n\\n\\n': 109, new id: 14\n",
      "Token ID for '<start_of_image>': 255999, new id: 15\n",
      "Token ID for '<image_soft_token>': 262144, new id: 16\n",
      "Token ID for '<end_of_image>': 256000, new id: 17\n",
      "All 48 tokens are active, no need to find tokens to have negative effect\n"
     ]
    }
   ],
   "source": [
    "custom_vocab_size = 48\n",
    "pretrained_tokenizer = tokenizer.tokenizer\n",
    "pretrained_model = model\n",
    "pretrained_embedding = pretrained_model.get_input_embeddings()\n",
    "embedding_dim = pretrained_embedding.embedding_dim\n",
    "custom_embedding = nn.Embedding(custom_vocab_size, embedding_dim)\n",
    "custom_embedding.weight.data.normal_(mean=0.0, std=0.0173)\n",
    "colors = [\" red\", \" blue\", \" green\", \" yellow\", \" orange\", \" purple\", \" white\", \" cyan\", \" gray\", \" brown\"]\n",
    "for i, color in enumerate(colors):\n",
    "    # Encode the color directly\n",
    "    token_id = pretrained_tokenizer.encode(color, add_special_tokens=False)\n",
    "    assert len(token_id) == 1, f\"Tokenization failed for {color}: {token_id}\"\n",
    "    print(f\"Token ID for '{color}': {token_id[0]}\")\n",
    "    custom_embedding.weight.data[i] = pretrained_embedding.weight.data[token_id[0]]\n",
    "token_id = pretrained_tokenizer.encode('<bos>', add_special_tokens=False)\n",
    "custom_embedding.weight.data[10] = pretrained_embedding.weight.data[token_id[0]]\n",
    "print(f\"Token ID for '<bos>': {token_id[0]}, new id: {10}\")\n",
    "token_id = pretrained_tokenizer.encode(' input', add_special_tokens=False)\n",
    "custom_embedding.weight.data[11] = pretrained_embedding.weight.data[token_id[0]]\n",
    "print(f\"Token ID for ' input': {token_id[0]}, new id: {11}\")\n",
    "token_id = pretrained_tokenizer.encode(' output', add_special_tokens=False)\n",
    "custom_embedding.weight.data[12] = pretrained_embedding.weight.data[token_id[0]]\n",
    "print(f\"Token ID for ' output': {token_id[0]}, new id: {12}\")\n",
    "token_id = pretrained_tokenizer.encode('\\n', add_special_tokens=False)\n",
    "custom_embedding.weight.data[13] = pretrained_embedding.weight.data[token_id[0]]\n",
    "print(f\"Token ID for '\\\\n': {token_id[0]}, new id: {13}\")\n",
    "token_id = pretrained_tokenizer.encode('\\n\\n\\n', add_special_tokens=False)\n",
    "custom_embedding.weight.data[14] = pretrained_embedding.weight.data[token_id[0]]\n",
    "print(f\"Token ID for '\\\\n\\\\n\\\\n': {token_id[0]}, new id: {14}\")\n",
    "token_id = pretrained_tokenizer.encode('<start_of_image>', add_special_tokens=False)\n",
    "custom_embedding.weight.data[15] = pretrained_embedding.weight.data[token_id[0]]\n",
    "print(f\"Token ID for '<start_of_image>': {token_id[0]}, new id: {15}\")\n",
    "token_id = pretrained_tokenizer.encode('<image_soft_token>', add_special_tokens=False)\n",
    "custom_embedding.weight.data[16] = pretrained_embedding.weight.data[token_id[0]]\n",
    "print(f\"Token ID for '<image_soft_token>': {token_id[0]}, new id: {16}\")\n",
    "token_id = pretrained_tokenizer.encode('<end_of_image>', add_special_tokens=False)\n",
    "custom_embedding.weight.data[17] = pretrained_embedding.weight.data[token_id[0]]\n",
    "print(f\"Token ID for '<end_of_image>': {token_id[0]}, new id: {17}\")\n",
    "print(\"All 48 tokens are active, no need to find tokens to have negative effect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccb0d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(custom_embedding.state_dict(), '../../Model/gemma48.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a15f29f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12915995",
   "metadata": {},
   "source": [
    "#### Qwen 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ab946d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.4.7: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.642 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7cda6e3eb414b879c20872a320cf092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from functions import *\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "                                        # model_name = \"unsloth/gemma-3-4b-pt-unsloth-bnb-4bit\",\n",
    "                                        model_name = \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "                                        max_seq_length = 8192,\n",
    "                                        load_in_4bit = True,\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb531bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_vocab_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "940bc61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_tokenizer = tokenizer.tokenizer\n",
    "pretrained_tokenizer = tokenizer\n",
    "pretrained_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99265ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID for ' red': 2518\n",
      "Token ID for ' blue': 6303\n",
      "Token ID for ' green': 6176\n",
      "Token ID for ' yellow': 13753\n",
      "Token ID for ' orange': 18575\n",
      "Token ID for ' purple': 24932\n",
      "Token ID for ' white': 4158\n",
      "Token ID for ' cyan': 57888\n",
      "Token ID for ' gray': 17545\n",
      "Token ID for ' brown': 13876\n"
     ]
    }
   ],
   "source": [
    "pretrained_embedding = pretrained_model.get_input_embeddings()\n",
    "pretrained_bos_id = pretrained_tokenizer.encode(\"<|box_start|>\", add_special_tokens=False)[0]\n",
    "pretrained_eos_id = pretrained_tokenizer.encode(\"<|box_end|>\", add_special_tokens=False)[0]\n",
    "pretrained_line_break_id = pretrained_tokenizer.encode(\"\\n\", add_special_tokens=False)[0]\n",
    "\n",
    "# Initialize custom embedding with the same embedding dimension as pre-trained\n",
    "embedding_dim = pretrained_embedding.embedding_dim\n",
    "custom_embedding = nn.Embedding(custom_vocab_size, embedding_dim)\n",
    "\n",
    "# Set weights for special tokens\n",
    "custom_embedding.weight.data[10] = pretrained_embedding.weight.data[pretrained_bos_id]  # BOS_X\n",
    "custom_embedding.weight.data[11] = pretrained_embedding.weight.data[pretrained_eos_id]  # EOS_X\n",
    "custom_embedding.weight.data[13] = pretrained_embedding.weight.data[pretrained_bos_id]  # BOS_Y\n",
    "custom_embedding.weight.data[14] = pretrained_embedding.weight.data[pretrained_eos_id]  # EOS_Y\n",
    "custom_embedding.weight.data[12] = pretrained_embedding.weight.data[pretrained_line_break_id]  # Line break\n",
    "\n",
    "colors = [\" red\", \" blue\", \" green\", \" yellow\", \" orange\", \" purple\", \" white\", \" cyan\", \" gray\", \" brown\"]\n",
    "for i, color in enumerate(colors):\n",
    "    # Encode the color directly\n",
    "    token_id = pretrained_tokenizer.encode(color, add_special_tokens=False)\n",
    "    assert len(token_id) == 1, f\"Tokenization failed for {color}: {token_id}\"\n",
    "    print(f\"Token ID for '{color}': {token_id[0]}\")\n",
    "    custom_embedding.weight.data[i] = pretrained_embedding.weight.data[token_id[0]]\n",
    "\n",
    "custom_embedding.weight.data[15] = pretrained_embedding.weight.data[pretrained_tokenizer.vocab_size - 1] # random non-special token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "692c1499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0279, 0.0270, 0.0270, 0.0262, 0.0259, 0.0251, 0.0273, 0.0241, 0.0256,\n",
       "        0.0259, 0.0089, 0.0088, 0.0236, 0.0089, 0.0088, 0.0095],\n",
       "       grad_fn=<StdBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_embedding.weight.std(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0794e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(custom_embedding.state_dict(), '../../Model/qwen16.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec886f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c2bbd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3bab4b1",
   "metadata": {},
   "source": [
    "#### Qwen 3 - 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d12b8a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.4.7: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.642 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a276be38074e038f2306451ce47506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from functions import *\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "                                        # model_name = \"unsloth/gemma-3-4b-pt-unsloth-bnb-4bit\",\n",
    "                                        model_name = \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "                                        max_seq_length = 8192,\n",
    "                                        load_in_4bit = True,\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca7bcd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_vocab_size = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88baa38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_tokenizer = tokenizer.tokenizer\n",
    "pretrained_tokenizer = tokenizer\n",
    "pretrained_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81ff8757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID for ' red': 2518\n",
      "Token ID for ' blue': 6303\n",
      "Token ID for ' green': 6176\n",
      "Token ID for ' yellow': 13753\n",
      "Token ID for ' orange': 18575\n",
      "Token ID for ' purple': 24932\n",
      "Token ID for ' white': 4158\n",
      "Token ID for ' cyan': 57888\n",
      "Token ID for ' gray': 17545\n",
      "Token ID for ' brown': 13876\n"
     ]
    }
   ],
   "source": [
    "pretrained_embedding = pretrained_model.get_input_embeddings()\n",
    "pretrained_bos_id = pretrained_tokenizer.encode(\"<|box_start|>\", add_special_tokens=False)[0]\n",
    "pretrained_eos_id = pretrained_tokenizer.encode(\"<|box_end|>\", add_special_tokens=False)[0]\n",
    "pretrained_line_break_id = pretrained_tokenizer.encode(\"\\n\", add_special_tokens=False)[0]\n",
    "\n",
    "# Initialize custom embedding with the same embedding dimension as pre-trained\n",
    "embedding_dim = pretrained_embedding.embedding_dim\n",
    "custom_embedding = nn.Embedding(custom_vocab_size, embedding_dim)\n",
    "\n",
    "# Set weights for special tokens\n",
    "custom_embedding.weight.data[10] = pretrained_embedding.weight.data[pretrained_bos_id]  # BOS_X\n",
    "custom_embedding.weight.data[11] = pretrained_embedding.weight.data[pretrained_eos_id]  # EOS_X\n",
    "custom_embedding.weight.data[13] = pretrained_embedding.weight.data[pretrained_bos_id]  # BOS_Y\n",
    "custom_embedding.weight.data[14] = pretrained_embedding.weight.data[pretrained_eos_id]  # EOS_Y\n",
    "custom_embedding.weight.data[12] = pretrained_embedding.weight.data[pretrained_line_break_id]  # Line break\n",
    "\n",
    "colors = [\" red\", \" blue\", \" green\", \" yellow\", \" orange\", \" purple\", \" white\", \" cyan\", \" gray\", \" brown\"]\n",
    "for i, color in enumerate(colors):\n",
    "    # Encode the color directly\n",
    "    token_id = pretrained_tokenizer.encode(color, add_special_tokens=False)\n",
    "    assert len(token_id) == 1, f\"Tokenization failed for {color}: {token_id}\"\n",
    "    print(f\"Token ID for '{color}': {token_id[0]}\")\n",
    "    custom_embedding.weight.data[i] = pretrained_embedding.weight.data[token_id[0]]\n",
    "\n",
    "custom_embedding.weight.data[15] = pretrained_embedding.weight.data[pretrained_tokenizer.vocab_size - 1] # random non-special token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39ebb814",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 31):\n",
    "    # Encode the integer directly\n",
    "    txt = str(i % 10) # Assuming single token output for each integer. only true for 1~9, hence %.\n",
    "    token_id = pretrained_tokenizer.encode(txt, add_special_tokens=False)\n",
    "    assert len(token_id) == 1, f\"Tokenization failed for {txt}: {token_id}\"\n",
    "    custom_embedding.weight.data[17 + i] = pretrained_embedding.weight.data[token_id[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49a116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_embedding.weight.data[16] = custom_embedding.weight.data[18:].mean(0)\n",
    "custom_embedding.weight.data[17] = custom_embedding.weight.data[18:].median(0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "124720db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0426,  0.0204, -0.0201,  ...,  0.0363, -0.0052, -0.0250],\n",
       "        [-0.0018,  0.0293, -0.0065,  ..., -0.0048,  0.0051,  0.0103]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# custom_embedding.weight.data[16:18].div_(1/0.0258);\n",
    "# custom_embedding.weight.data[18:].median(0).values;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2527ef98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0279, 0.0270, 0.0270, 0.0262, 0.0259, 0.0251, 0.0273, 0.0241, 0.0256,\n",
       "        0.0259, 0.0089, 0.0088, 0.0236, 0.0089, 0.0088, 0.0095, 0.0176, 0.0172,\n",
       "        0.0245, 0.0241, 0.0247, 0.0248, 0.0246, 0.0245, 0.0258, 0.0256, 0.0252,\n",
       "        0.0230, 0.0245, 0.0241, 0.0247, 0.0248, 0.0246, 0.0245, 0.0258, 0.0256,\n",
       "        0.0252, 0.0230, 0.0245, 0.0241, 0.0247, 0.0248, 0.0246, 0.0245, 0.0258,\n",
       "        0.0256, 0.0252, 0.0230], grad_fn=<StdBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_embedding.weight.std(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8eddd89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(custom_embedding.state_dict(), '../../Model/qwen48.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cf573c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
