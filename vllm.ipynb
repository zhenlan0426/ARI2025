{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-05 11:56:23 [__init__.py:239] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.3.19: Fast Gemma3 patching. Transformers: 4.50.0.dev0. vLLM: 0.8.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.642 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel,FastLanguageModel\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "import torch.nn as nn\n",
    "from peft import PeftModel\n",
    "import gc\n",
    "# from transformers.models.gemma3.modeling_gemma3 import Gemma3TextScaledWordEmbedding\n",
    "from functions import *\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    # model_name=\"unsloth/gemma-3-12b-pt\",\n",
    "    # model_name=\"unsloth/gemma-3-4b-pt\",\n",
    "    max_seq_length = 8192, # Choose any for long context!\n",
    "    load_in_4bit = True,\n",
    "    resize_model_vocab=16,\n",
    ")\n",
    "base_model = model.base_model\n",
    "base_model.lm_head.weight.requires_grad_(True);\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.language_model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "base_model.lm_head.load_state_dict(torch.load('../Model/lm_heads_weights_pt.pth'))\n",
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
    "\n",
    "    r = 64,           # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 64,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")\n",
    "# model = FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained_merged\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msave_directory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msave_method\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'merged_16bit'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpush_to_hub\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mis_main_process\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstate_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msave_function\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0msave\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x7c4050483b00\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_shard_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'5GB'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msafe_serialization\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvariant\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msave_peft_format\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtemporary_location\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'_unsloth_temporary_saved_buffers'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmaximum_memory_usage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "\u001b[0;32mdef\u001b[0m \u001b[0munsloth_generic_save_pretrained_merged\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msave_directory\u001b[0m       \u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtokenizer\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msave_method\u001b[0m          \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"merged_16bit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# [\"lora\", \"merged_16bit\", \"merged_4bit\"]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpush_to_hub\u001b[0m          \u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtoken\u001b[0m                \u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mis_main_process\u001b[0m      \u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstate_dict\u001b[0m           \u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msave_function\u001b[0m        \u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_shard_size\u001b[0m       \u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"5GB\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msafe_serialization\u001b[0m   \u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvariant\u001b[0m              \u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msave_peft_format\u001b[0m     \u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtags\u001b[0m                 \u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtemporary_location\u001b[0m   \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"_unsloth_temporary_saved_buffers\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmaximum_memory_usage\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Same as .push_to_hub(...) except 4bit weights are auto\u001b[0m\n",
      "\u001b[0;34m        converted to float16 with as few overhead as possible.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Choose for `save_method` to be either:\u001b[0m\n",
      "\u001b[0;34m        1. `16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\u001b[0m\n",
      "\u001b[0;34m        2.  `4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\u001b[0m\n",
      "\u001b[0;34m        3.  `lora`: Save LoRA adapters with no merging. Useful for HF inference.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"Unsloth: You're not saving a tokenizer as well?\\n\"\u001b[0m\\\n",
      "            \u001b[0;34m\"You can do it separately via `tokenizer.save_pretrained(...)`\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0marguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdel\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0munsloth_generic_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/lib/python3.12/site-packages/unsloth/save.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: You're not saving a tokenizer as well?\n",
      "You can do it separately via `tokenizer.save_pretrained(...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading safetensors index for unsloth/gemma-3-4b-it...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f290311751d46528302d988db661b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [02:46<02:46, 166.56s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6deccba9d49545e6ba2522d092944f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [04:56<00:00, 148.17s/it]\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\"../Model/merged_16b\", save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-05 11:55:31 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 04-05 11:55:35 [config.py:585] This model supports multiple tasks: {'classify', 'reward', 'score', 'generate', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 04-05 11:55:36 [config.py:664] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "WARNING 04-05 11:55:36 [arg_utils.py:1854] --quantization bitsandbytes is not supported by the V1 Engine. Falling back to V0. \n",
      "WARNING 04-05 11:55:36 [arg_utils.py:1741] The model has a long context length (131072). This may causeOOM during the initial memory profiling phase, or result in low performance due to small KV cache size. Consider setting --max-model-len to a smaller value.\n",
      "INFO 04-05 11:55:36 [llm_engine.py:241] Initializing a V0 LLM engine (v0.8.2) with config: model='unsloth/gemma-3-4b-it-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/gemma-3-4b-it-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=unsloth/gemma-3-4b-it-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 04-05 11:55:37 [cuda.py:291] Using Flash Attention backend.\n",
      "INFO 04-05 11:55:37 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 04-05 11:55:37 [model_runner.py:1110] Starting to load model unsloth/gemma-3-4b-it-unsloth-bnb-4bit...\n",
      "INFO 04-05 11:55:37 [cuda.py:265] Cannot use FlashAttention-2 backend for head size 72.\n",
      "INFO 04-05 11:55:37 [cuda.py:288] Using XFormers backend.\n",
      "INFO 04-05 11:55:37 [config.py:3243] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256] is overridden by config [256, 128, 2, 1, 4, 136, 8, 144, 16, 152, 24, 160, 32, 168, 40, 176, 48, 184, 56, 192, 64, 200, 72, 208, 80, 216, 88, 120, 224, 96, 232, 104, 240, 112, 248]\n",
      "INFO 04-05 11:55:38 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 04-05 11:55:38 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5650ea55d349c09ec697eb1dc1d4c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLM, SamplingParams\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# llm = LLM(model=\"../Model/merged_16b\")\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m llm \u001b[38;5;241m=\u001b[39m LLM(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsloth/gemma-3-4b-it-unsloth-bnb-4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/vllm/utils.py:1037\u001b[0m, in \u001b[0;36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1030\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1032\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1033\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1034\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m         )\n\u001b[0;32m-> 1037\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/vllm/entrypoints/llm.py:243\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m    215\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    216\u001b[0m     task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    240\u001b[0m )\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m LLMEngine\u001b[38;5;241m.\u001b[39mfrom_engine_args(\n\u001b[1;32m    244\u001b[0m     engine_args\u001b[38;5;241m=\u001b[39mengine_args, usage_context\u001b[38;5;241m=\u001b[39mUsageContext\u001b[38;5;241m.\u001b[39mLLM_CLASS)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/vllm/engine/llm_engine.py:520\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMEngine \u001b[38;5;28;01mas\u001b[39;00m V1LLMEngine\n\u001b[1;32m    518\u001b[0m     engine_cls \u001b[38;5;241m=\u001b[39m V1LLMEngine\n\u001b[0;32m--> 520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine_cls\u001b[38;5;241m.\u001b[39mfrom_vllm_config(\n\u001b[1;32m    521\u001b[0m     vllm_config\u001b[38;5;241m=\u001b[39mvllm_config,\n\u001b[1;32m    522\u001b[0m     usage_context\u001b[38;5;241m=\u001b[39musage_context,\n\u001b[1;32m    523\u001b[0m     stat_loggers\u001b[38;5;241m=\u001b[39mstat_loggers,\n\u001b[1;32m    524\u001b[0m     disable_log_stats\u001b[38;5;241m=\u001b[39mengine_args\u001b[38;5;241m.\u001b[39mdisable_log_stats,\n\u001b[1;32m    525\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/vllm/engine/llm_engine.py:496\u001b[0m, in \u001b[0;36mLLMEngine.from_vllm_config\u001b[0;34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_vllm_config\u001b[39m(\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m     disable_log_stats: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLMEngine\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    497\u001b[0m         vllm_config\u001b[38;5;241m=\u001b[39mvllm_config,\n\u001b[1;32m    498\u001b[0m         executor_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_executor_cls(vllm_config),\n\u001b[1;32m    499\u001b[0m         log_stats\u001b[38;5;241m=\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m disable_log_stats),\n\u001b[1;32m    500\u001b[0m         usage_context\u001b[38;5;241m=\u001b[39musage_context,\n\u001b[1;32m    501\u001b[0m         stat_loggers\u001b[38;5;241m=\u001b[39mstat_loggers,\n\u001b[1;32m    502\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/vllm/engine/llm_engine.py:280\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, mm_registry, use_cached_outputs)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_registry \u001b[38;5;241m=\u001b[39m input_registry\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_processor \u001b[38;5;241m=\u001b[39m input_registry\u001b[38;5;241m.\u001b[39mcreate_input_processor(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config)\n\u001b[0;32m--> 280\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m executor_class(vllm_config\u001b[38;5;241m=\u001b[39mvllm_config, )\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mrunner_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpooling\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_kv_caches()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/vllm/executor/executor_base.py:52\u001b[0m, in \u001b[0;36mExecutorBase.__init__\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_adapter_config \u001b[38;5;241m=\u001b[39m vllm_config\u001b[38;5;241m.\u001b[39mprompt_adapter_config\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;241m=\u001b[39m vllm_config\u001b[38;5;241m.\u001b[39mobservability_config\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_executor()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_sleeping \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py:47\u001b[0m, in \u001b[0;36mUniProcExecutor._init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollective_rpc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_worker\u001b[39m\u001b[38;5;124m\"\u001b[39m, args\u001b[38;5;241m=\u001b[39m([kwargs], ))\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollective_rpc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_device\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollective_rpc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py:56\u001b[0m, in \u001b[0;36mUniProcExecutor.collective_rpc\u001b[0;34m(self, method, timeout, args, kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 56\u001b[0m answer \u001b[38;5;241m=\u001b[39m run_method(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker, method, args, kwargs)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [answer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/vllm/utils.py:2255\u001b[0m, in \u001b[0;36mrun_method\u001b[0;34m(obj, method, args, kwargs)\u001b[0m\n\u001b[1;32m   2253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2254\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(method, obj)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m-> 2255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/vllm/worker/worker.py:183\u001b[0m, in \u001b[0;36mWorker.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m     context \u001b[38;5;241m=\u001b[39m nullcontext()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context:\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_runner\u001b[38;5;241m.\u001b[39mload_model()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/vllm/worker/model_runner.py:1113\u001b[0m, in \u001b[0;36mGPUModelRunnerBase.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m DeviceMemoryProfiler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mas\u001b[39;00m m:\n\u001b[1;32m   1112\u001b[0m     time_before_load \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m-> 1113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m get_model(vllm_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_config)\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_config:\n\u001b[1;32m   1115\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m supports_lora(\n\u001b[1;32m   1116\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1117\u001b[0m         ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not support LoRA yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py:14\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(vllm_config)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model\u001b[39m(\u001b[38;5;241m*\u001b[39m, vllm_config: VllmConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m nn\u001b[38;5;241m.\u001b[39mModule:\n\u001b[1;32m     13\u001b[0m     loader \u001b[38;5;241m=\u001b[39m get_model_loader(vllm_config\u001b[38;5;241m.\u001b[39mload_config)\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loader\u001b[38;5;241m.\u001b[39mload_model(vllm_config\u001b[38;5;241m=\u001b[39mvllm_config)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py:444\u001b[0m, in \u001b[0;36mDefaultModelLoader.load_model\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m    441\u001b[0m     model \u001b[38;5;241m=\u001b[39m _initialize_model(vllm_config\u001b[38;5;241m=\u001b[39mvllm_config)\n\u001b[1;32m    443\u001b[0m weights_to_load \u001b[38;5;241m=\u001b[39m {name \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters()}\n\u001b[0;32m--> 444\u001b[0m loaded_weights \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mload_weights(\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_all_weights(model_config, model))\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter_after_loading_weights \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m    447\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading weights took \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter_after_loading_weights \u001b[38;5;241m-\u001b[39m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter_before_loading_weights)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/gemma3_mm.py:793\u001b[0m, in \u001b[0;36mGemma3ForConditionalGeneration.load_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, weights: Iterable[Tuple[\u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    791\u001b[0m                                                torch\u001b[38;5;241m.\u001b[39mTensor]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Set[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    792\u001b[0m     loader \u001b[38;5;241m=\u001b[39m AutoWeightsLoader(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loader\u001b[38;5;241m.\u001b[39mload_weights(weights)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:235\u001b[0m, in \u001b[0;36mAutoWeightsLoader.load_weights\u001b[0;34m(self, weights, mapper)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mapper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     weights \u001b[38;5;241m=\u001b[39m mapper\u001b[38;5;241m.\u001b[39mapply(weights)\n\u001b[0;32m--> 235\u001b[0m autoloaded_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, weights))\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m autoloaded_weights\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:196\u001b[0m, in \u001b[0;36mAutoWeightsLoader._load_module\u001b[0;34m(self, base_prefix, module, weights)\u001b[0m\n\u001b[1;32m    192\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping module \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, prefix)\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_module(prefix,\n\u001b[1;32m    197\u001b[0m                                  child_modules[child_prefix],\n\u001b[1;32m    198\u001b[0m                                  child_weights)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m child_prefix \u001b[38;5;129;01min\u001b[39;00m child_params:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_skip(prefix):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:173\u001b[0m, in \u001b[0;36mAutoWeightsLoader._load_module\u001b[0;34m(self, base_prefix, module, weights)\u001b[0m\n\u001b[1;32m    171\u001b[0m module_load_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(module_load_weights):\n\u001b[0;32m--> 173\u001b[0m     loaded_params \u001b[38;5;241m=\u001b[39m module_load_weights(weights)\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loaded_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    176\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to collect loaded parameters \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor module \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, module)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/gemma3.py:528\u001b[0m, in \u001b[0;36mGemma3ForCausalLM.load_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, weights: Iterable[Tuple[\u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    522\u001b[0m                                                torch\u001b[38;5;241m.\u001b[39mTensor]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Set[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    523\u001b[0m     loader \u001b[38;5;241m=\u001b[39m AutoWeightsLoader(\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    525\u001b[0m         skip_prefixes\u001b[38;5;241m=\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlm_head.\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    526\u001b[0m                        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtie_word_embeddings \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    527\u001b[0m     )\n\u001b[0;32m--> 528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loader\u001b[38;5;241m.\u001b[39mload_weights(weights)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:235\u001b[0m, in \u001b[0;36mAutoWeightsLoader.load_weights\u001b[0;34m(self, weights, mapper)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mapper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     weights \u001b[38;5;241m=\u001b[39m mapper\u001b[38;5;241m.\u001b[39mapply(weights)\n\u001b[0;32m--> 235\u001b[0m autoloaded_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, weights))\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m autoloaded_weights\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:196\u001b[0m, in \u001b[0;36mAutoWeightsLoader._load_module\u001b[0;34m(self, base_prefix, module, weights)\u001b[0m\n\u001b[1;32m    192\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping module \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, prefix)\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_module(prefix,\n\u001b[1;32m    197\u001b[0m                                  child_modules[child_prefix],\n\u001b[1;32m    198\u001b[0m                                  child_weights)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m child_prefix \u001b[38;5;129;01min\u001b[39;00m child_params:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_skip(prefix):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:173\u001b[0m, in \u001b[0;36mAutoWeightsLoader._load_module\u001b[0;34m(self, base_prefix, module, weights)\u001b[0m\n\u001b[1;32m    171\u001b[0m module_load_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(module_load_weights):\n\u001b[0;32m--> 173\u001b[0m     loaded_params \u001b[38;5;241m=\u001b[39m module_load_weights(weights)\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loaded_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    176\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to collect loaded parameters \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor module \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, module)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/gemma3.py:452\u001b[0m, in \u001b[0;36mGemma3Model.load_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    449\u001b[0m         param \u001b[38;5;241m=\u001b[39m params_dict[name]\n\u001b[1;32m    450\u001b[0m         weight_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(param, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_loader\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    451\u001b[0m                                 default_weight_loader)\n\u001b[0;32m--> 452\u001b[0m         weight_loader(param, loaded_weight)\n\u001b[1;32m    453\u001b[0m     loaded_params\u001b[38;5;241m.\u001b[39madd(name)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_params\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py:1228\u001b[0m, in \u001b[0;36mRowParallelLinear.weight_loader\u001b[0;34m(self, param, loaded_weight)\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(loaded_weight\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1226\u001b[0m     loaded_weight \u001b[38;5;241m=\u001b[39m loaded_weight\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1228\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m param_data\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m loaded_weight\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   1229\u001b[0m param_data\u001b[38;5;241m.\u001b[39mcopy_(loaded_weight)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "# llm = LLM(model=\"../Model/merged_16b\")\n",
    "llm = LLM(model=\"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\")\n",
    "\n",
    "\n",
    "# # Step 2: Prepare pre-tokenized input\n",
    "# input_text = \"Hello, world!\"\n",
    "# input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\").tolist()[0]  # Get token IDs as a list\n",
    "# print(f\"Input tokens: {input_tokens}\")\n",
    "\n",
    "# # Step 3: Configure sampling parameters\n",
    "# sampling_params = SamplingParams(\n",
    "#     temperature=0.7,\n",
    "#     max_tokens=20,\n",
    "#     # Ensure we don't skip tokenization checks unnecessarily\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    n = 5,\n",
    "    temperature=0.7,\n",
    "    max_tokens=20,\n",
    "    # Ensure we don't skip tokenization checks unnecessarily\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 31414, 6, 232, 328]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tokens: [50118, 50118, 100, 216, 1085, 59, 5, 232, 6, 53, 38, 348, 57, 1207, 11, 1261, 13, 5, 94, 891]\n",
      "Decoded output: \n",
      "\n",
      "I know nothing about the world, but I've been living in Florida for the last couple\n",
      "Output tokens: [50118, 2678, 6, 20280, 4, 50118, 100, 437, 888, 269, 6661, 7, 28, 259, 4, 50118, 100, 2035, 95, 11]\n",
      "Decoded output: \n",
      "oh, hi.\n",
      "I'm actually really sorry to be here.\n",
      "I arrived just in\n",
      "Output tokens: [38, 437, 6661, 6, 38, 348, 57, 447, 15, 5, 754, 14, 38, 524, 41, 2370, 5385, 4, 38, 437]\n",
      "Decoded output:  I'm sorry, I've been working on the fact that I am an English speaker. I'm\n",
      "Output tokens: [50118, 50118, 31414, 328, 38, 524, 5, 29085, 328, 50118, 50118, 2264, 16, 5, 766, 9, 127, 5059, 116, 50118]\n",
      "Decoded output: \n",
      "\n",
      "Hello! I am the Witch!\n",
      "\n",
      "What is the name of my blog?\n",
      "\n",
      "Output tokens: [50118, 30086, 89, 4, 50118, 31414, 4, 50118, 100, 437, 6661, 4, 50118, 100, 21, 10, 410, 628, 4, 50118]\n",
      "Decoded output: \n",
      "Hi there.\n",
      "Hello.\n",
      "I'm sorry.\n",
      "I was a little late.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Pass token IDs directly to vLLM\n",
    "# Note: vLLM's high-level `generate` API expects text, so we use a list of token IDs with a custom approach\n",
    "# We'll wrap it in a way that mimics the expected input structure\n",
    "request_input = [input_tokens]  # vLLM expects a list of inputs (batch)\n",
    "outputs = llm.generate(prompt_token_ids=request_input,  # Use prompt_token_ids instead of text prompts\n",
    "                        sampling_params=sampling_params,\n",
    "                        use_tqdm=False  # Disable progress bar for clarity\n",
    "                    )\n",
    "\n",
    "# Step 5: Extract output token IDs\n",
    "for output in outputs:\n",
    "    for i in output.outputs:\n",
    "        output_tokens = i.token_ids  # Raw token IDs\n",
    "        output_text = tokenizer.decode(output_tokens)  # Decode for human-readable output\n",
    "        print(f\"Output tokens: {output_tokens}\")\n",
    "        print(f\"Decoded output: {output_text}\")\n",
    "\n",
    "# Optional: If you only want tokens and no decoding, just use output_tokens directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50118, 50118, 1106, 47, 32, 2509, 11, 3736, 84, 3221, 6, 2540, 27754, 50, 5124, 4, 370, 40, 28, 441]\n",
      "[50118, 30086, 6, 232, 328, 50118, 31414, 6, 232, 328, 50118, 31414, 6, 232, 328, 50118, 31414, 6, 232, 328]\n",
      "[50118, 50118, 250, 621, 54, 34, 57, 1207, 11, 5, 382, 13, 262, 107, 122, 6, 3026, 162, 14, 51]\n",
      "[38, 437, 10, 664, 621, 11, 5, 382, 54, 6138, 7, 1504, 36, 100, 437, 4959, 11, 1777, 4170, 322]\n",
      "[38, 437, 95, 667, 7, 6136, 10, 936, 6, 8, 38, 437, 45, 686, 99, 7, 109, 4, 38, 33]\n"
     ]
    }
   ],
   "source": [
    "for i in output.outputs:\n",
    "    print(i.token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
