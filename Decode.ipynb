{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-04 07:57:07 [__init__.py:239] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.3.19: Fast Gemma3 patching. Transformers: 4.50.0.dev0. vLLM: 0.8.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.642 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel,FastLanguageModel\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "import torch.nn as nn\n",
    "from peft import PeftModel\n",
    "import gc\n",
    "# from transformers.models.gemma3.modeling_gemma3 import Gemma3TextScaledWordEmbedding\n",
    "from functions import *\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    # model_name=\"unsloth/gemma-3-12b-pt\",\n",
    "    # model_name=\"unsloth/gemma-3-4b-pt\",\n",
    "    max_seq_length = 8192, # Choose any for long context!\n",
    "    load_in_4bit = True,\n",
    "    resize_model_vocab=16,\n",
    ")\n",
    "model = model.base_model\n",
    "model.lm_head.weight.requires_grad_(True);\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head.load_state_dict(torch.load('../Model/lm_heads_weights_pt.pth'))\n",
    "model = PeftModel.from_pretrained(model, \"../Model/merged_model_pt/\",is_trainable=False)\n",
    "model = FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "output_path = '/home/zhenlan/Desktop/Projects/ARC2/Data/ARC-AGI-2-main/combined_data.json'\n",
    "with open(output_path, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = iter(data_gen(data,False,8600))\n",
    "x, y = next(gen)\n",
    "x, y = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.where(x[0] == 13)[0][-1].item() # last index of BOS\n",
    "x, y = x[:,:idx+1], x[0][idx+1:] # x ends with BOS_Y (13), y starts from the next token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0, 12,  7,  0,  0,  7,  7,  0,  0,  7,  7,  0,  0,  7,  7,  0,  0,\n",
       "         7,  7,  0,  0,  7, 12,  0,  7,  7,  0,  0,  7,  7,  0,  0,  7,  7,  0,\n",
       "         0,  7,  7,  0,  0,  7,  7,  0, 12,  0,  7,  7,  0,  0,  7,  7,  0,  0,\n",
       "         7,  7,  0,  0,  7,  7,  0,  0,  7,  7,  0, 12,  7,  0,  0,  7,  7,  0,\n",
       "         0,  7,  7,  0,  0,  7,  7,  0,  0,  7,  7,  0,  0,  7, 12,  7,  0,  0,\n",
       "         7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  0,  0,  7, 12,\n",
       "         0,  7,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,\n",
       "         7,  0, 12,  0,  7,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  7,  7,  0, 12,  7,  0,  0,  7,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  7,  0,  0,  7, 12,  7,  0,  0,  7,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  7,  0,  0,  7, 12,  0,  7,  7,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  7,  0, 12,  0,  7,  7,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  7,  0, 12,\n",
       "         7,  0,  0,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  0,\n",
       "         0,  7, 12,  7,  0,  0,  7,  0,  0,  0,  0,  0,  0,  0,  0,  7,  0,  0,\n",
       "         7,  7,  0,  0,  7, 12,  0,  7,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  7,  7,  0,  0,  7,  7,  0, 12,  0,  7,  7,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  7,  7,  0,  0,  7,  7,  0, 12,  7,  0,  0,  7,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  7,  0,  0,  7,  7,  0,  0,  7, 12,  7,  0,  0,\n",
       "         7,  7,  0,  0,  7,  7,  0,  0,  7,  7,  0,  0,  7,  7,  0,  0,  7, 12,\n",
       "         0,  7,  7,  0,  0,  7,  7,  0,  0,  7,  7,  0,  0,  7,  7,  0,  0,  7,\n",
       "         7,  0, 12,  0,  7,  7,  0,  0,  7,  7,  0,  0,  7,  7,  0,  0,  7,  7,\n",
       "         0,  0,  7,  7,  0, 12, 14], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found EOS. Path NLL: 0.1276 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1291 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1288 | Path Len: 930\n",
      "Found EOS. Path NLL: 0.1289 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1308 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1290 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1306 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1314 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1292 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1289 | Path Len: 930\n",
      "Found EOS. Path NLL: 0.1317 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1307 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1307 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1276 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1276 | Path Len: 930\n",
      "Found EOS. Path NLL: 0.1312 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1301 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1295 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1311 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1324 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1298 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1295 | Path Len: 930\n",
      "Found EOS. Path NLL: 0.1320 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1265 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1264 | Path Len: 930\n",
      "Found EOS. Path NLL: 0.1305 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1326 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1325 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1333 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1269 | Path Len: 929\n",
      "Found EOS. Path NLL: 0.1334 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1269 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1265 | Path Len: 930\n",
      "Found EOS. Path NLL: 0.1260 | Path Len: 929\n",
      "Found EOS. Path NLL: 0.1316 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1297 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1317 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1314 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1298 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1297 | Path Len: 930\n",
      "Found EOS. Path NLL: 0.1318 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1318 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1317 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1331 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1346 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1316 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1315 | Path Len: 930\n",
      "Found EOS. Path NLL: 0.1355 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1339 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1316 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1333 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1346 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1320 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1329 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1291 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1308 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1323 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1292 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1291 | Path Len: 930\n",
      "Found EOS. Path NLL: 0.1318 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1312 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1329 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1342 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1346 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1328 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1327 | Path Len: 930\n",
      "Found EOS. Path NLL: 0.1367 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1350 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1351 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1350 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1344 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1332 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1329 | Path Len: 930\n",
      "Found EOS. Path NLL: 0.1322 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1339 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1342 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1322 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1322 | Path Len: 930\n",
      "Found EOS. Path NLL: 0.1343 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1299 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1298 | Path Len: 930\n",
      "Found EOS. Path NLL: 0.1352 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1351 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1297 | Path Len: 929\n",
      "Found EOS. Path NLL: 0.1328 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1346 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1361 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1330 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1329 | Path Len: 930\n",
      "Found EOS. Path NLL: 0.1352 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1329 | Path Len: 931\n",
      "Found EOS. Path NLL: 0.1345 | Path Len: 931\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# %debug\u001b[39;00m\n\u001b[1;32m      2\u001b[0m search \u001b[38;5;241m=\u001b[39m MinNLLSearcher(model)\n\u001b[0;32m----> 3\u001b[0m search\u001b[38;5;241m.\u001b[39mdfs_generate(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[9], line 105\u001b[0m, in \u001b[0;36mMinNLLSearcher.dfs_generate\u001b[0;34m(self, current_ids, current_nll, past_key_values, current_depth)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m \u001b[38;5;66;03m# Don't recurse further down this path\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# --- Recursive Step ---\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Pass the `new_past_key_values` which contains the cache state *after*\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdfs_generate(current_ids\u001b[38;5;241m=\u001b[39mnext_ids,\n\u001b[1;32m    106\u001b[0m                   current_nll\u001b[38;5;241m=\u001b[39mpotential_total_nll,\n\u001b[1;32m    107\u001b[0m                   past_key_values\u001b[38;5;241m=\u001b[39mnew_past_key_values,\n\u001b[1;32m    108\u001b[0m                   current_depth\u001b[38;5;241m=\u001b[39mcurrent_depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    109\u001b[0m                  )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[9], line 105\u001b[0m, in \u001b[0;36mMinNLLSearcher.dfs_generate\u001b[0;34m(self, current_ids, current_nll, past_key_values, current_depth)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m \u001b[38;5;66;03m# Don't recurse further down this path\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# --- Recursive Step ---\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Pass the `new_past_key_values` which contains the cache state *after*\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdfs_generate(current_ids\u001b[38;5;241m=\u001b[39mnext_ids,\n\u001b[1;32m    106\u001b[0m                   current_nll\u001b[38;5;241m=\u001b[39mpotential_total_nll,\n\u001b[1;32m    107\u001b[0m                   past_key_values\u001b[38;5;241m=\u001b[39mnew_past_key_values,\n\u001b[1;32m    108\u001b[0m                   current_depth\u001b[38;5;241m=\u001b[39mcurrent_depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    109\u001b[0m                  )\n",
      "    \u001b[0;31m[... skipping similar frames: context_decorator.<locals>.decorate_context at line 116 (927 times), MinNLLSearcher.dfs_generate at line 105 (926 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[9], line 105\u001b[0m, in \u001b[0;36mMinNLLSearcher.dfs_generate\u001b[0;34m(self, current_ids, current_nll, past_key_values, current_depth)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m \u001b[38;5;66;03m# Don't recurse further down this path\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# --- Recursive Step ---\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Pass the `new_past_key_values` which contains the cache state *after*\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdfs_generate(current_ids\u001b[38;5;241m=\u001b[39mnext_ids,\n\u001b[1;32m    106\u001b[0m                   current_nll\u001b[38;5;241m=\u001b[39mpotential_total_nll,\n\u001b[1;32m    107\u001b[0m                   past_key_values\u001b[38;5;241m=\u001b[39mnew_past_key_values,\n\u001b[1;32m    108\u001b[0m                   current_depth\u001b[38;5;241m=\u001b[39mcurrent_depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    109\u001b[0m                  )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[9], line 45\u001b[0m, in \u001b[0;36mMinNLLSearcher.dfs_generate\u001b[0;34m(self, current_ids, current_nll, past_key_values, current_depth)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Get logits and new KV cache\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16):\n\u001b[0;32m---> 45\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m     46\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids_step,\n\u001b[1;32m     47\u001b[0m         past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m     48\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# Crucial for efficiency\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     )\n\u001b[1;32m     51\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;66;03m# Shape: (1, input_seq_len, vocab_size)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m new_past_key_values \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpast_key_values \u001b[38;5;66;03m# Updated KV cache\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/peft/peft_model.py:1719\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1718\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1719\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model(\n\u001b[1;32m   1720\u001b[0m             input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1721\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1722\u001b[0m             inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1723\u001b[0m             labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m   1724\u001b[0m             output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1725\u001b[0m             output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1726\u001b[0m             return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1727\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1728\u001b[0m         )\n\u001b[1;32m   1730\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1732\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/Projects/ARC2/Code/unsloth_compiled_cache/unsloth_compiled_module_gemma3.py:722\u001b[0m, in \u001b[0;36mGemma3ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    708\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs,\n\u001b[1;32m    721\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, CausalLMOutputWithPast]:\n\u001b[0;32m--> 722\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Gemma3ForCausalLM_forward(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/Projects/ARC2/Code/unsloth_compiled_cache/unsloth_compiled_module_gemma3.py:537\u001b[0m, in \u001b[0;36mGemma3ForCausalLM_forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 537\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m    538\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    539\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    540\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    541\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    542\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m    543\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    544\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    545\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    546\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    547\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs,\n\u001b[1;32m    549\u001b[0m )\n\u001b[1;32m    551\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:754\u001b[0m, in \u001b[0;36mGemma3TextModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, last_cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    740\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    741\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    742\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    751\u001b[0m         last_cache_position,\n\u001b[1;32m    752\u001b[0m     )\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 754\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m    755\u001b[0m         hidden_states,\n\u001b[1;32m    756\u001b[0m         position_embeddings_global\u001b[38;5;241m=\u001b[39mposition_embeddings_global,\n\u001b[1;32m    757\u001b[0m         position_embeddings_local\u001b[38;5;241m=\u001b[39mposition_embeddings_local,\n\u001b[1;32m    758\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[1;32m    759\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    760\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    761\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    762\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    763\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    764\u001b[0m         last_cache_position\u001b[38;5;241m=\u001b[39mlast_cache_position,\n\u001b[1;32m    765\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[1;32m    766\u001b[0m     )\n\u001b[1;32m    768\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:443\u001b[0m, in \u001b[0;36mGemma3DecoderLayer.forward\u001b[0;34m(self, hidden_states, position_embeddings_global, position_embeddings_local, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, last_cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m position_embeddings_global\n\u001b[0;32m--> 443\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    444\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    445\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m    446\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    447\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    448\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m    449\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    450\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    451\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    453\u001b[0m )\n\u001b[1;32m    454\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m    455\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/unsloth_zoo/temporary_patches.py:550\u001b[0m, in \u001b[0;36mpatch_Gemma3Attention.<locals>.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m         key_states, value_states \u001b[38;5;241m=\u001b[39m key_states[:, :, :seq_len, :], value_states[:, :, :seq_len, :]\n\u001b[1;32m    528\u001b[0m \u001b[38;5;66;03m# attention_interface: Callable = eager_attention_forward\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;66;03m# if self.config._attn_implementation != \"eager\":\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m#     if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;66;03m#     **kwargs,\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m--> 550\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m scaled_dot_product_attention(\n\u001b[1;32m    551\u001b[0m     query_states\u001b[38;5;241m.\u001b[39mto(downcast_dtype),\n\u001b[1;32m    552\u001b[0m     key_states\u001b[38;5;241m.\u001b[39mto(downcast_dtype),\n\u001b[1;32m    553\u001b[0m     value_states\u001b[38;5;241m.\u001b[39mto(downcast_dtype),\n\u001b[1;32m    554\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask\u001b[38;5;241m.\u001b[39mto(downcast_dtype),\n\u001b[1;32m    555\u001b[0m     dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_dropout \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m    556\u001b[0m     scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling,\n\u001b[1;32m    557\u001b[0m     enable_gqa\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_key_value_groups\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    558\u001b[0m )\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    560\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;66;03m#.contiguous()\u001b[39;00m\n\u001b[1;32m    561\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj(attn_output)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %debug\n",
    "search = MinNLLSearcher(model)\n",
    "search.dfs_generate(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cumulative NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinNLLSearcher(object):\n",
    "    def __init__(self, model, max_depth: int = 31 * 30 + 1, brunch_factor: int = 3):\n",
    "        \"\"\"Initialize the searcher with a pre-trained model.\"\"\"\n",
    "\n",
    "        self.max_depth = max_depth\n",
    "        self.model = model\n",
    "        self.best_paths = []\n",
    "        self.nlls = []\n",
    "        self.min_nll = float('inf')\n",
    "        self.brunch_factor = brunch_factor\n",
    "\n",
    "    @staticmethod\n",
    "    def check_equal_line_lengths(tensor):\n",
    "        \"\"\" Check if all lines in a torch tensor have the same length when called at a line break position.\"\"\"\n",
    "        tensor = tensor[0]\n",
    "        if tensor[-1].item() != 12: # only check if the last token is a line break\n",
    "            return True\n",
    "        idx = (tensor == 12).nonzero(as_tuple=True)[0]\n",
    "        if len(idx) <= 1:\n",
    "            return True\n",
    "        return len(torch.unique(idx[1:] - idx[:-1])) == 1\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def dfs_generate(self, current_ids, current_nll = 0, past_key_values = None, current_depth = 0):\n",
    "        \"\"\"Performs Depth-First Search to find the lowest NLL completion.\"\"\"\n",
    "        # current_ids is torch.Tensor of Shape: (1, seq_len)\n",
    "        model = self.model\n",
    "        max_depth = self.max_depth\n",
    "        device = model.device\n",
    "        # Safety check for recursion depth\n",
    "        if current_depth > max_depth:\n",
    "            return\n",
    "\n",
    "        # Prepare inputs for the model\n",
    "        if past_key_values is None:\n",
    "            # First call, process the whole sequence\n",
    "            input_ids_step = current_ids\n",
    "        else:\n",
    "            # Subsequent calls, only process the last token\n",
    "            input_ids_step = current_ids[:, -1:]\n",
    "\n",
    "        # Get logits and new KV cache\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            outputs = model(\n",
    "                input_ids=input_ids_step,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True, # Crucial for efficiency\n",
    "            )\n",
    "\n",
    "        logits = outputs.logits.float() # Shape: (1, input_seq_len, vocab_size)\n",
    "        new_past_key_values = outputs.past_key_values # Updated KV cache\n",
    "\n",
    "        # Get logits for the *next* token prediction\n",
    "        # Logits shape is (batch_size, sequence_length, vocab_size)\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "\n",
    "        # Calculate log probabilities and negative log likelihoods\n",
    "        log_probs = torch.log_softmax(next_token_logits, dim=-1)\n",
    "        nlls = -log_probs # Shape: (1, vocab_size)\n",
    "\n",
    "        # Sort potential next tokens by NLL (ascending)\n",
    "        # We only need to explore the top `branching_factor` candidates\n",
    "        sorted_nlls, sorted_indices = torch.sort(nlls.squeeze(), descending=False)\n",
    "        sorted_nlls = sorted_nlls[:self.brunch_factor]\n",
    "        sorted_indices = sorted_indices[:self.brunch_factor]\n",
    "        \n",
    "        # Iterate through the most promising next tokens\n",
    "        for next_token_id, next_token_nll in zip(sorted_indices, sorted_nlls):\n",
    "            next_token_id = next_token_id.item()\n",
    "            next_token_nll = next_token_nll.item()\n",
    "\n",
    "            potential_total_nll = current_nll + next_token_nll\n",
    "\n",
    "            # --- Pruning ---\n",
    "            if potential_total_nll >= self.min_nll:\n",
    "                # If the current path's NLL is already worse than the best complete\n",
    "                # sequence found so far, prune this branch.\n",
    "                break\n",
    "\n",
    "            \n",
    "            if past_key_values is None: # first call\n",
    "                next_ids = torch.tensor([[next_token_id]], dtype=torch.long, device=device)\n",
    "            else: # Append the chosen token\n",
    "                next_ids = torch.cat(\n",
    "                    [current_ids, torch.tensor([[next_token_id]], dtype=torch.long, device=device)],\n",
    "                    dim=1\n",
    "                )\n",
    "            \n",
    "            if not self.check_equal_line_lengths(next_ids):\n",
    "                # If the line lengths are not equal, prune this branch\n",
    "                continue\n",
    "            \n",
    "            # --- Base Case: EOS token ---\n",
    "            if next_token_id == 14 and next_ids[0][-2].item() == 12: # Line break followed by EOS\n",
    "                print(f\"Found EOS. Path NLL: {potential_total_nll:.4f} | Path Len: {current_depth}\")\n",
    "                self.best_paths.append(next_ids[0].tolist())\n",
    "                self.nlls.append(potential_total_nll)\n",
    "                self.min_nll = min(self.min_nll, potential_total_nll)\n",
    "                # Continue searching other branches even after finding an EOS\n",
    "                continue # Don't recurse further down this path\n",
    "\n",
    "            # --- Recursive Step ---\n",
    "            # Pass the `new_past_key_values` which contains the cache state *after*\n",
    "            self.dfs_generate(current_ids=next_ids,\n",
    "                              current_nll=potential_total_nll,\n",
    "                              past_key_values=new_past_key_values,\n",
    "                              current_depth=current_depth + 1,\n",
    "                             )\n",
    "        # del new_past_key_values, past_key_values, logits, next_token_logits, log_probs, nlls\n",
    "        # gc.collect()\n",
    "        # torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per step NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinNLLSearcher(object):\n",
    "    def __init__(self, model, max_depth: int = 31 * 30 + 1, brunch_factor: int = 4, multiple: float = 1.25):\n",
    "        \"\"\"Initialize the searcher with a pre-trained model.\"\"\"\n",
    "\n",
    "        self.max_depth = max_depth\n",
    "        self.model = model\n",
    "        self.best_paths = []\n",
    "        self.nlls = []\n",
    "        self.min_nll = float('inf')\n",
    "        self.brunch_factor = brunch_factor\n",
    "        self.multiple = multiple\n",
    "\n",
    "    @staticmethod\n",
    "    def check_equal_line_lengths(tensor):\n",
    "        \"\"\" Check if all lines in a torch tensor have the same length when called at a line break position.\"\"\"\n",
    "        tensor = tensor[0]\n",
    "        if tensor[-1].item() != 12: # only check if the last token is a line break\n",
    "            return True\n",
    "        idx = (tensor == 12).nonzero(as_tuple=True)[0]\n",
    "        if len(idx) <= 1:\n",
    "            return True\n",
    "        return len(torch.unique(idx[1:] - idx[:-1])) == 1\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def dfs_generate(self, current_ids, current_nll = 0, past_key_values = None, current_depth = 1):\n",
    "        \"\"\"Performs Depth-First Search to find the lowest NLL completion.\"\"\"\n",
    "        # current_ids is torch.Tensor of Shape: (1, seq_len)\n",
    "        model = self.model\n",
    "        max_depth = self.max_depth\n",
    "        device = model.device\n",
    "        # Safety check for recursion depth\n",
    "        if current_depth > max_depth:\n",
    "            return\n",
    "\n",
    "        # Prepare inputs for the model\n",
    "        if past_key_values is None:\n",
    "            # First call, process the whole sequence\n",
    "            input_ids_step = current_ids\n",
    "        else:\n",
    "            # Subsequent calls, only process the last token\n",
    "            input_ids_step = current_ids[:, -1:]\n",
    "\n",
    "        # Get logits and new KV cache\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            outputs = model(\n",
    "                input_ids=input_ids_step,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True, # Crucial for efficiency\n",
    "            )\n",
    "\n",
    "        logits = outputs.logits.float() # Shape: (1, input_seq_len, vocab_size)\n",
    "        new_past_key_values = outputs.past_key_values # Updated KV cache\n",
    "\n",
    "        # Get logits for the *next* token prediction\n",
    "        # Logits shape is (batch_size, sequence_length, vocab_size)\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "\n",
    "        # Calculate log probabilities and negative log likelihoods\n",
    "        log_probs = torch.log_softmax(next_token_logits, dim=-1)\n",
    "        nlls = -log_probs # Shape: (1, vocab_size)\n",
    "\n",
    "        # Sort potential next tokens by NLL (ascending)\n",
    "        # We only need to explore the top `branching_factor` candidates\n",
    "        sorted_nlls, sorted_indices = torch.sort(nlls.squeeze(), descending=False)\n",
    "        sorted_nlls = sorted_nlls[:self.brunch_factor]\n",
    "        sorted_indices = sorted_indices[:self.brunch_factor]\n",
    "        \n",
    "        # Iterate through the most promising next tokens\n",
    "        for next_token_id, next_token_nll in zip(sorted_indices, sorted_nlls):\n",
    "            next_token_id = next_token_id.item()\n",
    "            next_token_nll = next_token_nll.item()\n",
    "\n",
    "            potential_total_nll = current_nll + next_token_nll\n",
    "\n",
    "            # --- Pruning ---\n",
    "            if potential_total_nll/current_depth >= self.min_nll * self.multiple:\n",
    "                # If the current path's NLL is already worse than the best complete\n",
    "                # sequence found so far, prune this branch.\n",
    "                break\n",
    "\n",
    "            \n",
    "            if past_key_values is None: # first call\n",
    "                next_ids = torch.tensor([[next_token_id]], dtype=torch.long, device=device)\n",
    "            else: # Append the chosen token\n",
    "                next_ids = torch.cat(\n",
    "                    [current_ids, torch.tensor([[next_token_id]], dtype=torch.long, device=device)],\n",
    "                    dim=1\n",
    "                )\n",
    "            \n",
    "            if not self.check_equal_line_lengths(next_ids):\n",
    "                # If the line lengths are not equal, prune this branch\n",
    "                continue\n",
    "            \n",
    "            # --- Base Case: EOS token ---\n",
    "            if next_token_id == 14 and next_ids[0][-2].item() == 12: # Line break followed by EOS\n",
    "                print(f\"Found EOS. Path NLL: {potential_total_nll/current_depth:.4f} | Path Len: {current_depth}\")\n",
    "                self.best_paths.append(next_ids[0].tolist())\n",
    "                self.nlls.append(potential_total_nll/current_depth)\n",
    "                self.min_nll = min(self.min_nll, potential_total_nll/current_depth)\n",
    "                # Continue searching other branches even after finding an EOS\n",
    "                continue # Don't recurse further down this path\n",
    "\n",
    "            # --- Recursive Step ---\n",
    "            # Pass the `new_past_key_values` which contains the cache state *after*\n",
    "            self.dfs_generate(current_ids=next_ids,\n",
    "                              current_nll=potential_total_nll,\n",
    "                              past_key_values=new_past_key_values,\n",
    "                              current_depth=current_depth + 1,\n",
    "                             )\n",
    "        # del new_past_key_values, past_key_values, logits, next_token_logits, log_probs, nlls\n",
    "        # gc.collect()\n",
    "        # torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
