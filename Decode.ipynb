{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-02 14:18:05 [__init__.py:239] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.3.19: Fast Gemma3 patching. Transformers: 4.50.0.dev0. vLLM: 0.8.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.642 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel,FastLanguageModel\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "import torch.nn as nn\n",
    "from peft import PeftModel\n",
    "import gc\n",
    "# from transformers.models.gemma3.modeling_gemma3 import Gemma3TextScaledWordEmbedding\n",
    "from functions import *\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    # model_name=\"unsloth/gemma-3-12b-pt\",\n",
    "    # model_name=\"unsloth/gemma-3-4b-pt\",\n",
    "    max_seq_length = 8192, # Choose any for long context!\n",
    "    load_in_4bit = True,\n",
    "    resize_model_vocab=16,\n",
    ")\n",
    "model = model.base_model\n",
    "model.lm_head.weight.requires_grad_(True);\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head.load_state_dict(torch.load('../Model/lm_heads_weights.pth'))\n",
    "model = PeftModel.from_pretrained(model, \"../Model/merged_model/\",is_trainable=True)\n",
    "FastLanguageModel.for_inference(model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "output_path = '/home/zhenlan/Desktop/Projects/ARC2/Data/ARC-AGI-2-main/combined_data.json'\n",
    "with open(output_path, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = iter(data_gen(data,False,8600))\n",
    "x, y = next(gen)\n",
    "x, y = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.where(x[0] == 13)[0][-1].item() # last index of BOS\n",
    "x, y = x[:,:idx+1], x[0][idx+1:] # x ends with BOS_Y (13), y starts from the next token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0, 12,  7,  0,  0,  7,  7,  0,  0,  7,  7,  0,  0,  7,  7,  0,  0,\n",
       "         7,  7,  0,  0,  7, 12,  0,  7,  7,  0,  0,  7,  7,  0,  0,  7,  7,  0,\n",
       "         0,  7,  7,  0,  0,  7,  7,  0, 12,  0,  7,  7,  0,  0,  7,  7,  0,  0,\n",
       "         7,  7,  0,  0,  7,  7,  0,  0,  7,  7,  0, 12,  7,  0,  0,  7,  7,  0,\n",
       "         0,  7,  7,  0,  0,  7,  7,  0,  0,  7,  7,  0,  0,  7, 12,  7,  0,  0,\n",
       "         7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  0,  0,  7, 12,\n",
       "         0,  7,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,\n",
       "         7,  0, 12,  0,  7,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  7,  7,  0, 12,  7,  0,  0,  7,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  7,  0,  0,  7, 12,  7,  0,  0,  7,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  7,  0,  0,  7, 12,  0,  7,  7,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  7,  0, 12,  0,  7,  7,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  7,  0, 12,\n",
       "         7,  0,  0,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  0,\n",
       "         0,  7, 12,  7,  0,  0,  7,  0,  0,  0,  0,  0,  0,  0,  0,  7,  0,  0,\n",
       "         7,  7,  0,  0,  7, 12,  0,  7,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  7,  7,  0,  0,  7,  7,  0, 12,  0,  7,  7,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  7,  7,  0,  0,  7,  7,  0, 12,  7,  0,  0,  7,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  7,  0,  0,  7,  7,  0,  0,  7, 12,  7,  0,  0,\n",
       "         7,  7,  0,  0,  7,  7,  0,  0,  7,  7,  0,  0,  7,  7,  0,  0,  7, 12,\n",
       "         0,  7,  7,  0,  0,  7,  7,  0,  0,  7,  7,  0,  0,  7,  7,  0,  0,  7,\n",
       "         7,  0, 12,  0,  7,  7,  0,  0,  7,  7,  0,  0,  7,  7,  0,  0,  7,  7,\n",
       "         0,  0,  7,  7,  0, 12, 14], device='cuda:0')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found EOS. Path NLL: 50.0909 | Path Len: 931\n",
      "Found EOS. Path NLL: 48.8021 | Path Len: 931\n",
      "Found EOS. Path NLL: 48.7532 | Path Len: 918\n",
      "Found EOS. Path NLL: 48.7419 | Path Len: 918\n",
      "Found EOS. Path NLL: 48.6569 | Path Len: 917\n",
      "Found EOS. Path NLL: 48.6299 | Path Len: 916\n",
      "Found EOS. Path NLL: 48.5776 | Path Len: 915\n",
      "Found EOS. Path NLL: 48.3667 | Path Len: 914\n",
      "Found EOS. Path NLL: 48.0158 | Path Len: 913\n",
      "Found EOS. Path NLL: 47.8201 | Path Len: 911\n",
      "Found EOS. Path NLL: 47.6044 | Path Len: 910\n",
      "Found EOS. Path NLL: 47.1818 | Path Len: 909\n",
      "Found EOS. Path NLL: 46.6319 | Path Len: 908\n",
      "Found EOS. Path NLL: 46.6211 | Path Len: 887\n",
      "Found EOS. Path NLL: 46.3040 | Path Len: 886\n",
      "Found EOS. Path NLL: 46.1992 | Path Len: 885\n",
      "Found EOS. Path NLL: 45.3965 | Path Len: 880\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# %debug\u001b[39;00m\n\u001b[1;32m      2\u001b[0m search \u001b[38;5;241m=\u001b[39m MinNLLSearcher(model)\n\u001b[0;32m----> 3\u001b[0m search\u001b[38;5;241m.\u001b[39mdfs_generate(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[105], line 104\u001b[0m, in \u001b[0;36mMinNLLSearcher.dfs_generate\u001b[0;34m(self, current_ids, current_nll, past_key_values, current_depth)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m \u001b[38;5;66;03m# Don't recurse further down this path\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# --- Recursive Step ---\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Pass the `new_past_key_values` which contains the cache state *after*\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdfs_generate(current_ids\u001b[38;5;241m=\u001b[39mnext_ids,\n\u001b[1;32m    105\u001b[0m                   current_nll\u001b[38;5;241m=\u001b[39mpotential_total_nll,\n\u001b[1;32m    106\u001b[0m                   past_key_values\u001b[38;5;241m=\u001b[39mnew_past_key_values,\n\u001b[1;32m    107\u001b[0m                   current_depth\u001b[38;5;241m=\u001b[39mcurrent_depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    108\u001b[0m                  )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[105], line 104\u001b[0m, in \u001b[0;36mMinNLLSearcher.dfs_generate\u001b[0;34m(self, current_ids, current_nll, past_key_values, current_depth)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m \u001b[38;5;66;03m# Don't recurse further down this path\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# --- Recursive Step ---\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Pass the `new_past_key_values` which contains the cache state *after*\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdfs_generate(current_ids\u001b[38;5;241m=\u001b[39mnext_ids,\n\u001b[1;32m    105\u001b[0m                   current_nll\u001b[38;5;241m=\u001b[39mpotential_total_nll,\n\u001b[1;32m    106\u001b[0m                   past_key_values\u001b[38;5;241m=\u001b[39mnew_past_key_values,\n\u001b[1;32m    107\u001b[0m                   current_depth\u001b[38;5;241m=\u001b[39mcurrent_depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    108\u001b[0m                  )\n",
      "    \u001b[0;31m[... skipping similar frames: context_decorator.<locals>.decorate_context at line 116 (868 times), MinNLLSearcher.dfs_generate at line 104 (867 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[105], line 104\u001b[0m, in \u001b[0;36mMinNLLSearcher.dfs_generate\u001b[0;34m(self, current_ids, current_nll, past_key_values, current_depth)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m \u001b[38;5;66;03m# Don't recurse further down this path\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# --- Recursive Step ---\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Pass the `new_past_key_values` which contains the cache state *after*\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdfs_generate(current_ids\u001b[38;5;241m=\u001b[39mnext_ids,\n\u001b[1;32m    105\u001b[0m                   current_nll\u001b[38;5;241m=\u001b[39mpotential_total_nll,\n\u001b[1;32m    106\u001b[0m                   past_key_values\u001b[38;5;241m=\u001b[39mnew_past_key_values,\n\u001b[1;32m    107\u001b[0m                   current_depth\u001b[38;5;241m=\u001b[39mcurrent_depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    108\u001b[0m                  )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[105], line 44\u001b[0m, in \u001b[0;36mMinNLLSearcher.dfs_generate\u001b[0;34m(self, current_ids, current_nll, past_key_values, current_depth)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Get logits and new KV cache\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16):\n\u001b[0;32m---> 44\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m     45\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids_step,\n\u001b[1;32m     46\u001b[0m         past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m     47\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# Crucial for efficiency\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     )\n\u001b[1;32m     50\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;66;03m# Shape: (1, input_seq_len, vocab_size)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m new_past_key_values \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpast_key_values \u001b[38;5;66;03m# Updated KV cache\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/peft/peft_model.py:1719\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1718\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1719\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model(\n\u001b[1;32m   1720\u001b[0m             input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1721\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1722\u001b[0m             inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1723\u001b[0m             labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m   1724\u001b[0m             output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1725\u001b[0m             output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1726\u001b[0m             return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1727\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1728\u001b[0m         )\n\u001b[1;32m   1730\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1732\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/Projects/ARC2/Code/unsloth_compiled_cache/unsloth_compiled_module_gemma3.py:722\u001b[0m, in \u001b[0;36mGemma3ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    708\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs,\n\u001b[1;32m    721\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, CausalLMOutputWithPast]:\n\u001b[0;32m--> 722\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Gemma3ForCausalLM_forward(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/Projects/ARC2/Code/unsloth_compiled_cache/unsloth_compiled_module_gemma3.py:537\u001b[0m, in \u001b[0;36mGemma3ForCausalLM_forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 537\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m    538\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    539\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    540\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    541\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    542\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m    543\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    544\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    545\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    546\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    547\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs,\n\u001b[1;32m    549\u001b[0m )\n\u001b[1;32m    551\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:754\u001b[0m, in \u001b[0;36mGemma3TextModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, last_cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    740\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    741\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    742\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    751\u001b[0m         last_cache_position,\n\u001b[1;32m    752\u001b[0m     )\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 754\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m    755\u001b[0m         hidden_states,\n\u001b[1;32m    756\u001b[0m         position_embeddings_global\u001b[38;5;241m=\u001b[39mposition_embeddings_global,\n\u001b[1;32m    757\u001b[0m         position_embeddings_local\u001b[38;5;241m=\u001b[39mposition_embeddings_local,\n\u001b[1;32m    758\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[1;32m    759\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    760\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    761\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    762\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    763\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    764\u001b[0m         last_cache_position\u001b[38;5;241m=\u001b[39mlast_cache_position,\n\u001b[1;32m    765\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[1;32m    766\u001b[0m     )\n\u001b[1;32m    768\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:459\u001b[0m, in \u001b[0;36mGemma3DecoderLayer.forward\u001b[0;34m(self, hidden_states, position_embeddings_global, position_embeddings_local, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, last_cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    458\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_feedforward_layernorm(hidden_states)\n\u001b[0;32m--> 459\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    460\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_feedforward_layernorm(hidden_states)\n\u001b[1;32m    461\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/Projects/ARC2/Code/unsloth_compiled_cache/unsloth_compiled_module_gemma3.py:124\u001b[0m, in \u001b[0;36mGemma3MLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Gemma3MLP_forward(\u001b[38;5;28mself\u001b[39m, x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:574\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    570\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[1;32m    571\u001b[0m )\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    577\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[1;32m    578\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[1;32m    579\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Projects/ARC2/Code/unsloth_compiled_cache/unsloth_compiled_module_gemma3.py:107\u001b[0m, in \u001b[0;36mGemma3MLP_forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgemma3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_gemma3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (copy, Callable, List, Optional, Tuple, Union, torch, nn, ACT2FN, Cache, HybridCache, StaticCache, GenerationMixin, FlashAttentionKwargs, CausalLMOutputWithPast, ROPE_INIT_FUNCTIONS, ALL_ATTENTION_FUNCTIONS, PreTrainedModel, Unpack, add_start_docstrings, add_start_docstrings_to_model_forward, is_torchdynamo_compiling, replace_return_docstrings, deprecate_kwarg, AutoModel, AutoModelForCausalLM, Gemma3Config, Gemma3TextConfig, logger, \u001b[38;5;18m__name__\u001b[39m, _CONFIG_FOR_DOC, Gemma3CausalLMOutputWithPast, GEMMA3_START_DOCSTRING, Gemma3PreTrainedModel, GEMMA3_INPUTS_DOCSTRING, Gemma3TextModel, Gemma3ForCausalLM, Gemma3ForConditionalGeneration)\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mcompile(fullgraph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, dynamic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, options \u001b[38;5;241m=\u001b[39m torch_compile_options)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mGemma3MLP_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    109\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/Desktop/Projects/ARC2/Code/unsloth_compiled_cache/unsloth_compiled_module_gemma3.py:109\u001b[0m, in \u001b[0;36mtorch_dynamo_resume_in_Gemma3MLP_forward_at_109\u001b[0;34m(___stack0, ___stack1, ___stack2, self, x)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mcompile(fullgraph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, dynamic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, options \u001b[38;5;241m=\u001b[39m torch_compile_options)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mGemma3MLP_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 109\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m prior_skip_guard_eval_unsafe \u001b[38;5;241m=\u001b[39m set_skip_guard_eval_unsafe(\n\u001b[1;32m    742\u001b[0m     _is_skip_guard_eval_unsafe_stance()\n\u001b[1;32m    743\u001b[0m )\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py:1184\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.forward\u001b[0;34m(*runtime_args)\u001b[0m\n\u001b[1;32m   1182\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(params_flat)\n\u001b[1;32m   1183\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(runtime_args)\n\u001b[0;32m-> 1184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn(full_args)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:426\u001b[0m, in \u001b[0;36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    423\u001b[0m     ret_outs \u001b[38;5;241m=\u001b[39m fw_outs\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m runtime_metadata\u001b[38;5;241m.\u001b[39mdynamic_outputs:\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t, o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ret_outs, runtime_metadata\u001b[38;5;241m.\u001b[39moutput_info):\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m o\u001b[38;5;241m.\u001b[39mdynamic_dims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:52\u001b[0m, in \u001b[0;36mstrict_zip\u001b[0;34m(strict, *iterables, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m!=\u001b[39m length:\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     49\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe iterables have different lengths and strict mode is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m         )\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m original_zip(\u001b[38;5;241m*\u001b[39miterables, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %debug\n",
    "search = MinNLLSearcher(model)\n",
    "search.dfs_generate(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinNLLSearcher(object):\n",
    "    def __init__(self, model, max_depth: int = 31 * 30 + 1, brunch_factor: int = 7):\n",
    "        \"\"\"Initialize the searcher with a pre-trained model.\"\"\"\n",
    "\n",
    "        self.max_depth = max_depth\n",
    "        self.model = model\n",
    "        self.best_paths = []\n",
    "        self.nlls = []\n",
    "        self.min_nll = float('inf')\n",
    "        self.brunch_factor = brunch_factor\n",
    "\n",
    "    @staticmethod\n",
    "    def check_equal_line_lengths(tensor):\n",
    "        \"\"\" Check if all lines in a torch tensor have the same length when called at a line break position.\"\"\"\n",
    "        tensor = tensor[0]\n",
    "        if tensor[-1].item() != 12: # only check if the last token is a line break\n",
    "            return True\n",
    "        idx = (tensor == 12).nonzero(as_tuple=True)[0]\n",
    "        if len(idx) <= 1:\n",
    "            return True\n",
    "        return len(torch.unique(idx[1:] - idx[:-1])) == 1\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def dfs_generate(self, current_ids, current_nll = 0, past_key_values = None, current_depth = 0):\n",
    "        \"\"\"Performs Depth-First Search to find the lowest NLL completion.\"\"\"\n",
    "        # current_ids is torch.Tensor of Shape: (1, seq_len)\n",
    "        model = self.model\n",
    "        max_depth = self.max_depth\n",
    "        device = model.device\n",
    "        # Safety check for recursion depth\n",
    "        if current_depth > max_depth:\n",
    "            return\n",
    "\n",
    "        # Prepare inputs for the model\n",
    "        if past_key_values is None:\n",
    "            # First call, process the whole sequence\n",
    "            input_ids_step = current_ids\n",
    "        else:\n",
    "            # Subsequent calls, only process the last token\n",
    "            input_ids_step = current_ids[:, -1:]\n",
    "\n",
    "        # Get logits and new KV cache\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            outputs = model(\n",
    "                input_ids=input_ids_step,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True, # Crucial for efficiency\n",
    "            )\n",
    "\n",
    "        logits = outputs.logits.float() # Shape: (1, input_seq_len, vocab_size)\n",
    "        new_past_key_values = outputs.past_key_values # Updated KV cache\n",
    "\n",
    "        # Get logits for the *next* token prediction\n",
    "        # Logits shape is (batch_size, sequence_length, vocab_size)\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "\n",
    "        # Calculate log probabilities and negative log likelihoods\n",
    "        log_probs = torch.log_softmax(next_token_logits, dim=-1)\n",
    "        nlls = -log_probs # Shape: (1, vocab_size)\n",
    "\n",
    "        # Sort potential next tokens by NLL (ascending)\n",
    "        # We only need to explore the top `branching_factor` candidates\n",
    "        sorted_nlls, sorted_indices = torch.sort(nlls.squeeze(), descending=False)\n",
    "        sorted_nlls = sorted_nlls[:self.brunch_factor]\n",
    "        sorted_indices = sorted_indices[:self.brunch_factor]\n",
    "        \n",
    "        # Iterate through the most promising next tokens\n",
    "        for next_token_id, next_token_nll in zip(sorted_indices, sorted_nlls):\n",
    "            next_token_id = next_token_id.item()\n",
    "            next_token_nll = next_token_nll.item()\n",
    "\n",
    "            potential_total_nll = current_nll + next_token_nll\n",
    "\n",
    "            # --- Pruning ---\n",
    "            if potential_total_nll >= self.min_nll:\n",
    "                # If the current path's NLL is already worse than the best complete\n",
    "                # sequence found so far, prune this branch.\n",
    "                break\n",
    "\n",
    "            \n",
    "            if past_key_values is None: # first call\n",
    "                next_ids = torch.tensor([[next_token_id]], dtype=torch.long, device=device)\n",
    "            else: # Append the chosen token\n",
    "                next_ids = torch.cat(\n",
    "                    [current_ids, torch.tensor([[next_token_id]], dtype=torch.long, device=device)],\n",
    "                    dim=1\n",
    "                )\n",
    "            \n",
    "            if not self.check_equal_line_lengths(next_ids):\n",
    "                # If the line lengths are not equal, prune this branch\n",
    "                continue\n",
    "            \n",
    "            # --- Base Case: EOS token ---\n",
    "            if next_token_id == 14 and next_ids[0][-1].item() == 12: # Line break followed by EOS\n",
    "                print(f\"Found EOS. Path NLL: {potential_total_nll:.4f} | Path Len: {current_depth}\")\n",
    "                self.best_paths.append(next_ids[0].tolist())\n",
    "                self.nlls.append(potential_total_nll)\n",
    "                self.min_nll = min(self.min_nll, potential_total_nll)\n",
    "                # Continue searching other branches even after finding an EOS\n",
    "                continue # Don't recurse further down this path\n",
    "\n",
    "            # --- Recursive Step ---\n",
    "            # Pass the `new_past_key_values` which contains the cache state *after*\n",
    "            self.dfs_generate(current_ids=next_ids,\n",
    "                              current_nll=potential_total_nll,\n",
    "                              past_key_values=new_past_key_values,\n",
    "                              current_depth=current_depth + 1,\n",
    "                             )\n",
    "        # del new_past_key_values, past_key_values, logits, next_token_logits, log_probs, nlls\n",
    "        # gc.collect()\n",
    "        # torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
